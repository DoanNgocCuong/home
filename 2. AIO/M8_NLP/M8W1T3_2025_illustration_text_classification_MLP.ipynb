{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification - MLP"
      ],
      "metadata": {
        "id": "b_DSNYXLGn3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchtext==0.17.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEPrpzVVH4rQ",
        "outputId": "2e1b5554-05d7-4a17-8ad0-9486e5baf517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.17.0\n",
            "  Downloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (2.32.3)\n",
            "Collecting torch==2.2.0 (from torchtext==0.17.0)\n",
            "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (1.26.4)\n",
            "Collecting torchdata==0.7.1 (from torchtext==0.17.0)\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0->torchtext==0.17.0)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchtext==0.17.0) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.0) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0->torchtext==0.17.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0->torchtext==0.17.0) (1.3.0)\n",
            "Downloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setup"
      ],
      "metadata": {
        "id": "vSTqG7X8O82N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "corpus = [\n",
        "    \"Deepseek sell off stock market\",\n",
        "    \"Deepseek release all their technical documents\"\n",
        "]\n",
        "data_size = len(corpus)\n",
        "\n",
        "# 0: negative - 1: positive\n",
        "labels = [0, 1]\n",
        "\n",
        "# Define the max vocabulary size and sequence length\n",
        "vocab_size = 8\n",
        "sequence_length = 5"
      ],
      "metadata": {
        "id": "HjFoMLulF4jS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "c0620746-6641-406a-85c7-de3ffaf12d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b12a82441c4d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "# version 0.17.0\n",
        "\n",
        "sample1 = corpus[0]\n",
        "sample2 = corpus[1]\n",
        "\n",
        "#Define tokenizer function\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "sample1_tokens = tokenizer(sample1)\n",
        "sample2_tokens = tokenizer(sample2)\n",
        "\n",
        "print(sample1_tokens)\n",
        "print(sample2_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvYUgYBZoymB",
        "outputId": "e320af8d-3e96-413f-b1a9-f99d64e2a0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['deepseek', 'sell', 'off', 'stock', 'market']\n",
            "['deepseek', 'release', 'all', 'their', 'technical', 'documents']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample1_ids = [vocab[token] for token in sample1_tokens]\n",
        "sample2_ids = [vocab[token] for token in sample2_tokens]\n",
        "\n",
        "print(sample1_ids)\n",
        "print(sample2_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6gmk_rDM11u",
        "outputId": "93dbc9c2-1047-41d4-f7d4-9c9941ca13a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 0, 6, 0, 5]\n",
            "[2, 7, 3, 0, 0, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Define tokenizer function\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Create a function to yield list of tokens\n",
        "def yield_tokens(examples):\n",
        "    for text in examples:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(corpus),\n",
        "                                  max_tokens=vocab_size,\n",
        "                                  specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "vocab.get_stoi()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPEte3mnF4c0",
        "outputId": "718a35e7-692d-41c7-f433-7a12fd8bbe62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'release': 7,\n",
              " 'off': 6,\n",
              " 'market': 5,\n",
              " 'documents': 4,\n",
              " 'deepseek': 2,\n",
              " '<pad>': 1,\n",
              " 'all': 3,\n",
              " '<unk>': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and numericalize your samples\n",
        "def vectorize(text, vocab, sequence_length):\n",
        "    tokens = tokenizer(text)\n",
        "    token_ids = [vocab[token] for token in tokens][:sequence_length]\n",
        "    token_ids = token_ids + [vocab[\"<pad>\"]] * (sequence_length - len(tokens))\n",
        "    return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "# Vectorize the samples\n",
        "corpus_ids = []\n",
        "for sentence in corpus:\n",
        "    corpus_ids.append(vectorize(sentence, vocab, sequence_length))"
      ],
      "metadata": {
        "id": "6Qcwb_J9F4aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfoTpnTxML6i",
        "outputId": "5bd0e16e-28f5-47b5-ff64-2388d12ab3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([2, 0, 6, 0, 5]), tensor([2, 7, 3, 0, 0])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for v in corpus_ids:\n",
        "    print(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYTUNNQfF4Xh",
        "outputId": "180a2054-b3fe-48b5-b255-a13b0b03d010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 0, 6, 0, 5])\n",
            "tensor([2, 7, 3, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 8\n",
        "embedding_dim = 2\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "custom_weights = torch.tensor( [[-0.1882,  0.5530],\n",
        "                                [ 1.7840, -0.8278],\n",
        "                                [ 1.0281, -1.9094],\n",
        "                                [-1.3083, -0.0987],\n",
        "                                [ 0.2293,  1.3255],\n",
        "                                [ 0.4058, -0.6624],\n",
        "                                [ 0.5582,  0.0786],\n",
        "                                [ 0.4309, -1.3067]]).float()\n",
        "embedding.weight = nn.Parameter(custom_weights)\n",
        "\n",
        "print(embedding.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwudWVGDTDd-",
        "outputId": "432b9951-df2f-4949-d3ab-9c4346801593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1882,  0.5530],\n",
            "        [ 1.7840, -0.8278],\n",
            "        [ 1.0281, -1.9094],\n",
            "        [-1.3083, -0.0987],\n",
            "        [ 0.2293,  1.3255],\n",
            "        [ 0.4058, -0.6624],\n",
            "        [ 0.5582,  0.0786],\n",
            "        [ 0.4309, -1.3067]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(vocab_size, 2)\n",
        "custom_weights = torch.tensor( [[-0.1882,  0.5530],\n",
        "                                [ 1.7840, -0.8278],\n",
        "                                [ 1.0281, -1.9094],\n",
        "                                [-1.3083, -0.0987],\n",
        "                                [ 0.2293,  1.3255],\n",
        "                                [ 0.4058, -0.6624],\n",
        "                                [ 0.5582,  0.0786],\n",
        "                                [ 0.4309, -1.3067]]).float()\n",
        "embedding.weight = nn.Parameter(custom_weights)\n",
        "\n",
        "\n",
        "fc = nn.Linear(10, 2)\n",
        "fc_weights = torch.tensor( [[0.2108, -0.0074,  0.2760,  0.2325, -0.0518, -0.1876,  0.0194, 0.0378, 0.0210, 0.2982],\n",
        "                            [0.0284,  0.2968, -0.0260,  0.1251, -0.0282,  0.0175, -0.1817, 0.2483, 0.2338, 0.2985]]).float()\n",
        "fc.weight = nn.Parameter(fc_weights)\n",
        "\n",
        "fc_bias = torch.tensor([-0.3049,  0.1028]).float()\n",
        "fc.bias = nn.Parameter(fc_bias)\n",
        "\n",
        "flatten = nn.Flatten()\n",
        "model = nn.Sequential(embedding, flatten, fc)"
      ],
      "metadata": {
        "id": "zk11cVlLF4Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqJS2A5TZOmk",
        "outputId": "c148e465-af2d-439f-ef5b-220e07a4049e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(8, 2)\n",
            "  (1): Flatten(start_dim=1, end_dim=-1)\n",
            "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "                             lr=0.1)"
      ],
      "metadata": {
        "id": "TfAf-_07JKLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hand on"
      ],
      "metadata": {
        "id": "OD0znSHQbRgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_1 = torch.tensor([[2, 0, 6, 0, 5]], dtype=torch.long)\n",
        "label_1 = torch.tensor([0], dtype=torch.long)"
      ],
      "metadata": {
        "id": "vvIwyszTa0A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_output = embedding(input_1)\n",
        "print(\"Embedded Output:\\n\", embedded_output, embedded_output.shape)\n",
        "flattened_output = flatten(embedded_output)\n",
        "print(\"Flattened Output:\\n\", flattened_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7xYmI0xa0D8",
        "outputId": "61ededdf-273b-4184-ace1-905dfe61e801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Output:\n",
            " tensor([[[ 1.0281, -1.9094],\n",
            "         [-0.1882,  0.5530],\n",
            "         [ 0.5582,  0.0786],\n",
            "         [-0.1882,  0.5530],\n",
            "         [ 0.4058, -0.6624]]], grad_fn=<EmbeddingBackward0>) torch.Size([1, 5, 2])\n",
            "Flattened Output:\n",
            " torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = fc_weights[0]\n",
        "w2 = fc_weights[1]\n",
        "w2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_GYyox_d4eR",
        "outputId": "63e9ddd8-332c-4337-e538-6c1798fa1911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0284,  0.2968, -0.0260,  0.1251, -0.0282,  0.0175, -0.1817,  0.2483,\n",
              "         0.2338,  0.2985])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for i, x in enumerate(flattened_output[0]):\n",
        "    result.append(x * w1[i])\n",
        "\n",
        "result, sum(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16y82dEFdfDn",
        "outputId": "efa4cda4-fb58-4db2-9391-612acc8f482b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor(0.2167, grad_fn=<MulBackward0>),\n",
              "  tensor(0.0141, grad_fn=<MulBackward0>),\n",
              "  tensor(-0.0519, grad_fn=<MulBackward0>),\n",
              "  tensor(0.1286, grad_fn=<MulBackward0>),\n",
              "  tensor(-0.0289, grad_fn=<MulBackward0>),\n",
              "  tensor(-0.0147, grad_fn=<MulBackward0>),\n",
              "  tensor(-0.0037, grad_fn=<MulBackward0>),\n",
              "  tensor(0.0209, grad_fn=<MulBackward0>),\n",
              "  tensor(0.0085, grad_fn=<MulBackward0>),\n",
              "  tensor(-0.1975, grad_fn=<MulBackward0>)],\n",
              " tensor(0.0921, grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for i, x in enumerate(flattened_output[0]):\n",
        "    result.append(x * w2[i])\n",
        "\n",
        "result, sum(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WHPgC8kdfGu",
        "outputId": "7f97d42b-b14e-4a81-85a5-85c955a1b8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor(0.0292, grad_fn=<MulBackward0>),\n",
              "  tensor(-0.5667, grad_fn=<MulBackward0>),\n",
              "  tensor(0.0049, grad_fn=<MulBackward0>),\n",
              "  tensor(0.0692, grad_fn=<MulBackward0>),\n",
              "  tensor(-0.0157, grad_fn=<MulBackward0>),\n",
              "  tensor(0.0014, grad_fn=<MulBackward0>),\n",
              "  tensor(0.0342, grad_fn=<MulBackward0>),\n",
              "  tensor(0.1373, grad_fn=<MulBackward0>),\n",
              "  tensor(0.0949, grad_fn=<MulBackward0>),\n",
              "  tensor(-0.1977, grad_fn=<MulBackward0>)],\n",
              " tensor(-0.4091, grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc_output = fc(flattened_output)\n",
        "print(\"FC Output:\\n\", fc_output, fc_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xueofw8beOk",
        "outputId": "75420b9f-428b-4c3c-8236-e384e0517f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FC Output:\n",
            " tensor([[-0.2128, -0.3063]], grad_fn=<AddmmBackward0>) torch.Size([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "math.e**(-0.2128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNgXEqTKwFAl",
        "outputId": "d72710de-906c-4b8b-dcd6-0afe06de9c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8083177846081321"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.e**(-0.3063)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJQ1BpB3wK5E",
        "outputId": "09adb4bc-c72e-48cb-8eba-f8d01f919905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7361657366043477"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.7361/1.5445"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhLnNv3txJ51",
        "outputId": "c4d40e48-2e20-477d-c9ff-3e9a4ad60dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4765943671090968"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train sample-1 once, and check gradient and weight"
      ],
      "metadata": {
        "id": "4ZU6ltI4O_2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first sample\n",
        "\n",
        "input_1 = torch.tensor([[2, 0, 6, 0, 5]], dtype=torch.long)\n",
        "label_1 = torch.tensor([0], dtype=torch.long)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "outputs = model(input_1)\n",
        "print(outputs)\n",
        "print(torch.softmax(outputs, axis=-1))\n",
        "\n",
        "loss = criterion(outputs, label_1)\n",
        "print(loss)\n",
        "\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_wTfOnzJOQ_",
        "outputId": "e2b51579-1f7e-418d-fcc8-7821814f2f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2128, -0.3063]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[0.5234, 0.4766]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor(0.6475, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n embedding.weight \\n\")\n",
        "print(embedding.weight)\n",
        "\n",
        "print(\"\\n embedding.weight.grad \\n\")\n",
        "print(embedding.weight.grad)\n",
        "\n",
        "print(\"\\n fc.weight \\n\")\n",
        "print(fc.weight)\n",
        "\n",
        "print(\"\\n fc.bias \\n\")\n",
        "print(fc.bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ1taeNKJP_D",
        "outputId": "4a21e15b-fc63-46f0-827a-af57e110695f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " embedding.weight \n",
            "\n",
            "Parameter containing:\n",
            "tensor([[-0.1642,  0.5481],\n",
            "        [ 1.7840, -0.8278],\n",
            "        [ 1.0368, -1.9239],\n",
            "        [-1.3083, -0.0987],\n",
            "        [ 0.2293,  1.3255],\n",
            "        [ 0.3957, -0.6624],\n",
            "        [ 0.5571,  0.0688],\n",
            "        [ 0.4309, -1.3067]], requires_grad=True)\n",
            "\n",
            " embedding.weight.grad \n",
            "\n",
            "tensor([[-2.3980e-01,  4.9141e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00],\n",
            "        [-8.6939e-02,  1.4499e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00],\n",
            "        [ 1.0143e-01,  1.4298e-04],\n",
            "        [ 1.1249e-02,  9.7758e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00]])\n",
            "\n",
            " fc.weight \n",
            "\n",
            "Parameter containing:\n",
            "tensor([[ 0.2598, -0.0984,  0.2670,  0.2589, -0.0252, -0.1839,  0.0104,  0.0642,\n",
            "          0.0403,  0.2666],\n",
            "        [-0.0206,  0.3878, -0.0170,  0.0987, -0.0548,  0.0138, -0.1727,  0.2219,\n",
            "          0.2145,  0.3301]], requires_grad=True)\n",
            "\n",
            " fc.bias \n",
            "\n",
            "Parameter containing:\n",
            "tensor([-0.2572,  0.0551], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check if the loss reduces"
      ],
      "metadata": {
        "id": "Cp-XS4GPPJ1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()\n",
        "outputs = model(input_1)\n",
        "print(outputs)\n",
        "print(torch.softmax(outputs, axis=-1))\n",
        "\n",
        "loss = criterion(outputs, label_1)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g1p9TCwJSo2",
        "outputId": "f2a91adb-ff26-46a6-a905-dd5d617dcdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1456, -0.6688]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[0.6930, 0.3070]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor(0.3667, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4wYNc6ZxPecv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train several epochs (ignore this part)"
      ],
      "metadata": {
        "id": "BWqBVTHMPP7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([[4, 3, 0, 6, 3],\n",
        "                       [2, 5, 7, 2, 0]], dtype=torch.long)\n",
        "labels = torch.tensor([0, 1], dtype=torch.long)\n",
        "\n",
        "for _ in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftjA9ZnzJZFR",
        "outputId": "3cf3c854-fa0e-49f9-8d62-2fa9e6214c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.6064, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9049, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5502, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3779, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2824, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if the model has adready learned sucessfuly"
      ],
      "metadata": {
        "id": "JmwD9JyVQK-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(inputs)\n",
        "print(outputs)\n",
        "print(torch.softmax(outputs, axis=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRkcGw8OQFwX",
        "outputId": "5869d3b7-b105-44d0-a302-f8ab29153ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2058, -0.8800],\n",
            "        [-1.3359,  0.4520]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[0.7476, 0.2524],\n",
            "        [0.1433, 0.8567]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "npVHuf9gQYKc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}