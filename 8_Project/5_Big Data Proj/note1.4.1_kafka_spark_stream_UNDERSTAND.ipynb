{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dags/kafka-stream.py\n",
    "spark-stream.py\n",
    "------\n",
    "1. 2 File này là gì, \n",
    "2. Vai trò là gì trong luồng \n",
    "\n",
    "Nói về luồng tổng quan trước\n",
    "3. Bỏ đi thì saoa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tổng quan về luồng dữ liệu\n",
    "\n",
    "Luồng dữ liệu trong hai file `kafka-stream.py` và `spark-stream.py` là một kiến trúc **real-time streaming** kết hợp giữa Apache Kafka, Apache Spark, và Cassandra. Dưới đây là mô tả tổng quan:\n",
    "\n",
    "1. **Luồng dữ liệu tổng quan:**\n",
    "   - **Dữ liệu đầu vào:** Được lấy từ API `https://randomuser.me/api/` (cung cấp dữ liệu người dùng ngẫu nhiên).\n",
    "   - **Apache Kafka:** Là hệ thống message queue dùng để truyền tải dữ liệu thời gian thực từ `kafka-stream.py` đến `spark-stream.py`.\n",
    "   - **Apache Spark:** Đọc dữ liệu từ Kafka, xử lý dữ liệu, và lưu vào cơ sở dữ liệu NoSQL (Cassandra).\n",
    "   - **Cassandra DB:** Lưu trữ dữ liệu đã được xử lý để phục vụ các ứng dụng downstream (phân tích, báo cáo, etc.).\n",
    "\n",
    "2. **Dòng chảy dữ liệu chính:**\n",
    "   - `kafka-stream.py` (Producer): \n",
    "     1. Lấy dữ liệu từ API.\n",
    "     2. Định dạng dữ liệu (format).\n",
    "     3. Gửi dữ liệu đến **Kafka topic `users_created`**.\n",
    "   - `spark-stream.py` (Consumer):\n",
    "     1. Kết nối đến Kafka để nhận dữ liệu từ topic `users_created`.\n",
    "     2. Xử lý dữ liệu (cấu trúc lại schema).\n",
    "     3. Lưu dữ liệu đã xử lý vào bảng `created_users` trong Cassandra.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Mô tả từng file và vai trò trong luồng\n",
    "\n",
    "| **File**              | **Vai trò**                                                                                                            |\n",
    "|-----------------------|------------------------------------------------------------------------------------------------------------------------|\n",
    "| **`kafka-stream.py`** | Là **producer** trong Kafka. File này lấy dữ liệu từ API, xử lý và đẩy dữ liệu lên Kafka topic (`users_created`).       |\n",
    "| **`spark-stream.py`** | Là **consumer** trong Kafka. File này đọc dữ liệu từ Kafka, xử lý dữ liệu bằng Apache Spark và lưu kết quả vào Cassandra.|\n",
    "\n",
    "---\n",
    "\n",
    "#### **`kafka-stream.py` (Producer)**\n",
    "- **Vai trò:**\n",
    "  - Kết nối đến API (`https://randomuser.me/api/`) để lấy dữ liệu người dùng.\n",
    "  - Xử lý (format) dữ liệu đầu vào thành JSON với các thông tin cụ thể như `first_name`, `address`, `email`, etc.\n",
    "  - Đẩy dữ liệu lên Kafka topic `users_created`.\n",
    "\n",
    "- **Các bước chính:**\n",
    "  1. **Gửi request đến API:** Lấy dữ liệu người dùng ngẫu nhiên.\n",
    "  2. **Xử lý dữ liệu:** Biến đổi dữ liệu thành định dạng JSON dễ sử dụng.\n",
    "  3. **Gửi dữ liệu vào Kafka:** Đẩy dữ liệu lên topic `users_created`.\n",
    "\n",
    "#### **`spark-stream.py` (Consumer)**\n",
    "- **Vai trò:**\n",
    "  - Kết nối đến Kafka để nhận dữ liệu từ topic `users_created`.\n",
    "  - Xử lý dữ liệu bằng Apache Spark (chuyển đổi schema).\n",
    "  - Lưu dữ liệu đã xử lý vào Cassandra trong bảng `created_users`.\n",
    "\n",
    "- **Các bước chính:**\n",
    "  1. **Kết nối Spark với Kafka:** Nhận dữ liệu từ topic.\n",
    "  2. **Chuyển đổi dữ liệu:** Xác định schema, chuyển đổi JSON thành dataframe.\n",
    "  3. **Lưu vào Cassandra:** Lưu dữ liệu vào bảng `created_users`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Vai trò trong luồng tổng quan\n",
    "\n",
    "| **Thành Phần**           | **Vai Trò**                                                                                                  |\n",
    "|--------------------------|------------------------------------------------------------------------------------------------------------|\n",
    "| **API (Random User)**    | Nguồn dữ liệu ban đầu cung cấp thông tin người dùng.                                                       |\n",
    "| **`kafka-stream.py`**    | Tạo dữ liệu (producer) và gửi lên Kafka để truyền tải tới các thành phần khác trong thời gian thực.          |\n",
    "| **Kafka (Message Queue)**| Trung gian truyền tải dữ liệu thời gian thực giữa producer (`kafka-stream.py`) và consumer (`spark-stream.py`).|\n",
    "| **`spark-stream.py`**    | Xử lý dữ liệu từ Kafka, định dạng lại, và lưu trữ vào Cassandra.                                           |\n",
    "| **Cassandra**            | Cơ sở dữ liệu lưu trữ kết quả cuối cùng của luồng dữ liệu.                                                |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Nếu bỏ đi một trong các thành phần\n",
    "\n",
    "| **Thành Phần Bị Loại Bỏ** | **Hậu Quả**                                                                                                                                                   |\n",
    "|---------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **`kafka-stream.py`**     | - Dữ liệu sẽ không được đẩy lên Kafka, dẫn đến luồng dữ liệu không khởi động. <br>- Consumer (`spark-stream.py`) sẽ không nhận được dữ liệu để xử lý.          |\n",
    "| **`spark-stream.py`**     | - Dữ liệu từ Kafka không được xử lý và lưu vào Cassandra. <br>- Cassandra sẽ không chứa dữ liệu để phục vụ phân tích hoặc sử dụng downstream.                  |\n",
    "| **Kafka (Message Queue)** | - Mất kênh trung gian truyền tải dữ liệu giữa producer và consumer. <br>- `spark-stream.py` sẽ không nhận được dữ liệu từ `kafka-stream.py`.                  |\n",
    "| **Cassandra**             | - Không có nơi lưu trữ kết quả cuối cùng của luồng. <br>- Dữ liệu sẽ chỉ tồn tại trong Spark hoặc Kafka và không được lưu trữ lâu dài để sử dụng sau này.      |\n",
    "\n",
    "---\n",
    "\n",
    "### Kết luận\n",
    "Hai file này đóng vai trò cốt lõi trong kiến trúc **real-time streaming**:\n",
    "- `kafka-stream.py`: Thu thập và gửi dữ liệu vào Kafka (producer).\n",
    "- `spark-stream.py`: Nhận và xử lý dữ liệu từ Kafka, sau đó lưu trữ vào Cassandra (consumer).\n",
    "\n",
    "Nếu bất kỳ thành phần nào bị bỏ đi, luồng dữ liệu sẽ không thể hoạt động hoàn chỉnh. Để đảm bảo luồng hoạt động tốt, cần kiểm tra kỹ các kết nối (Kafka, Cassandra) và định dạng dữ liệu trong cả hai file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiểu đơn giản về Kafka và cách hoạt động của 2 file `kafka-stream.py` và `spark-stream.py`\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka là gì?**\n",
    "Hãy tưởng tượng **Kafka** giống như **hệ thống băng chuyền thông minh** trong một nhà máy.  \n",
    "- **Nhiệm vụ của Kafka:** Vận chuyển dữ liệu từ nơi này đến nơi khác một cách nhanh chóng và liên tục.\n",
    "- **Producer (Người gửi):** Gửi dữ liệu lên băng chuyền (Kafka).  \n",
    "- **Consumer (Người nhận):** Lấy dữ liệu từ băng chuyền để xử lý hoặc lưu trữ.\n",
    "\n",
    "---\n",
    "\n",
    "### **File 1: `kafka-stream.py`**\n",
    "\n",
    "#### **Vai trò:**\n",
    "- File này là **Producer**: Tạo dữ liệu và gửi nó lên **Kafka**.\n",
    "- Tưởng tượng: Đây là **người gửi hàng** trong nhà máy. Họ lấy thông tin từ nguồn (API) và đặt nó lên băng chuyền (Kafka).\n",
    "\n",
    "#### **Cách hoạt động:**\n",
    "1. **Lấy dữ liệu từ API `randomuser.me`:**\n",
    "   - API này cung cấp thông tin về một người dùng ngẫu nhiên (họ tên, địa chỉ, email, ...).\n",
    "   - Dữ liệu này giống như \"nguyên liệu thô\" mà nhà máy cần xử lý.\n",
    "\n",
    "2. **Định dạng dữ liệu:**\n",
    "   - File này \"đóng gói\" thông tin người dùng sao cho dễ hiểu và sẵn sàng để gửi.\n",
    "\n",
    "3. **Gửi dữ liệu lên Kafka:**\n",
    "   - Dữ liệu được gửi lên một **topic** tên là `users_created` trong Kafka.\n",
    "   - Topic này giống như một \"khu vực riêng\" trên băng chuyền, nơi người gửi và người nhận giao tiếp.\n",
    "\n",
    "#### **Nếu không có file này:**\n",
    "- Không ai tạo hoặc gửi dữ liệu lên Kafka. Băng chuyền sẽ trống và hệ thống không hoạt động được.\n",
    "\n",
    "---\n",
    "\n",
    "### **File 2: `spark-stream.py`**\n",
    "\n",
    "#### **Vai trò:**\n",
    "- File này là **Consumer**: Lấy dữ liệu từ Kafka và xử lý nó.\n",
    "- Tưởng tượng: Đây là **công nhân nhà máy**, nhận dữ liệu từ băng chuyền và biến nó thành sản phẩm hoàn chỉnh.\n",
    "\n",
    "#### **Cách hoạt động:**\n",
    "1. **Kết nối đến Kafka:**\n",
    "   - File này kết nối tới Kafka và lắng nghe dữ liệu từ topic `users_created`.\n",
    "\n",
    "2. **Xử lý dữ liệu với Spark:**\n",
    "   - Dữ liệu được \"chuyển đổi\" và chuẩn bị cho việc lưu trữ. Spark làm nhiệm vụ này giống như công nhân nhà máy chế biến nguyên liệu.\n",
    "\n",
    "3. **Lưu dữ liệu vào Cassandra:**\n",
    "   - Sau khi xử lý xong, dữ liệu được lưu trữ trong cơ sở dữ liệu Cassandra (giống như đưa sản phẩm vào kho).\n",
    "\n",
    "#### **Nếu không có file này:**\n",
    "- Không ai nhận và xử lý dữ liệu từ Kafka. Dữ liệu trên băng chuyền sẽ không được sử dụng.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm lại luồng hoạt động:**\n",
    "\n",
    "1. **File `kafka-stream.py`:**\n",
    "   - **Lấy dữ liệu từ API** → **Định dạng dữ liệu** → **Gửi lên Kafka** (băng chuyền).\n",
    "   - Vai trò: **Producer** (Người gửi).\n",
    "\n",
    "2. **Kafka:**\n",
    "   - Lưu trữ và vận chuyển dữ liệu từ **Producer (kafka-stream.py)** đến **Consumer (spark-stream.py)**.\n",
    "\n",
    "3. **File `spark-stream.py`:**\n",
    "   - **Nhận dữ liệu từ Kafka** → **Xử lý bằng Spark** → **Lưu vào Cassandra**.\n",
    "   - Vai trò: **Consumer** (Người nhận).\n",
    "\n",
    "---\n",
    "\n",
    "### **Nếu không có Kafka thì sao?**\n",
    "- Kafka chính là \"băng chuyền\". Nếu không có Kafka:\n",
    "  - **Producer** (file `kafka-stream.py`) không có cách nào gửi dữ liệu đi.\n",
    "  - **Consumer** (file `spark-stream.py`) không nhận được dữ liệu để xử lý.\n",
    "  - Hệ thống sẽ không hoạt động.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm tắt dễ hiểu:**\n",
    "- **Kafka**: Là \"băng chuyền\" vận chuyển dữ liệu.\n",
    "- **`kafka-stream.py`**: Là \"người gửi hàng\", tạo dữ liệu và đặt lên băng chuyền (Kafka).\n",
    "- **`spark-stream.py`**: Là \"công nhân\", lấy dữ liệu từ băng chuyền (Kafka), xử lý và lưu lại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên thông tin bạn cung cấp và hình ảnh, vấn đề có thể xảy ra do một số nguyên nhân sau:\n",
    "\n",
    "---\n",
    "\n",
    "### **Nguyên nhân 1: Control Center không hiển thị dữ liệu cũ**\n",
    "- **Control Center chỉ hiển thị dữ liệu được gửi vào sau khi trang được mở**. Dữ liệu đã có sẵn trong topic (xác minh bằng `kafka-console-consumer`) sẽ không được hiển thị nếu chúng đã được gửi trước đó.\n",
    "\n",
    "#### **Giải pháp:**\n",
    "1. **Gửi thêm dữ liệu mới vào topic:**\n",
    "   - Chạy lại `kafka-stream.py` để gửi thêm dữ liệu vào Kafka.\n",
    "2. **Refresh Control Center:**\n",
    "   - Mở lại tab `Messages` trong Control Center sau khi dữ liệu mới đã được gửi.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nguyên nhân 2: Không có Schema được xác định cho topic**\n",
    "- **Control Center có thể không hiển thị dữ liệu nếu không có schema (Avro, JSON, etc.) được cấu hình trong Schema Registry**. Dữ liệu được gửi dưới dạng raw binary sẽ không hiển thị trên giao diện.\n",
    "\n",
    "#### **Giải pháp:**\n",
    "1. **Kiểm tra định dạng dữ liệu:**\n",
    "   - Trong script `kafka-stream.py`, kiểm tra định dạng dữ liệu gửi lên Kafka. Đảm bảo rằng dữ liệu được gửi dưới dạng JSON.\n",
    "   - Dòng gửi dữ liệu phải giống như:\n",
    "     ```python\n",
    "     producer.send('users_created', json.dumps(data).encode('utf-8'))\n",
    "     ```\n",
    "\n",
    "2. **Xác minh Schema Registry:**\n",
    "   - Kiểm tra Schema Registry bằng cách truy cập:\n",
    "     ```\n",
    "     http://localhost:8081\n",
    "     ```\n",
    "   - Đảm bảo rằng topic `users_created` có schema tương ứng được đăng ký.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nguyên nhân 3: Consumer đang tiêu thụ dữ liệu**\n",
    "- **Một Consumer khác có thể đang tiêu thụ hết dữ liệu từ topic `users_created`**, khiến Control Center không thể hiển thị dữ liệu nào trong tab `Messages`.\n",
    "\n",
    "#### **Giải pháp:**\n",
    "1. **Kiểm tra consumer:**\n",
    "   - Chạy lệnh kiểm tra các consumer group đang sử dụng topic `users_created`:\n",
    "     ```bash\n",
    "     docker exec -it broker kafka-consumer-groups --bootstrap-server broker:29092 --describe --all-groups\n",
    "     ```\n",
    "   - Xem các thông tin như `OFFSET` và `LAG` để kiểm tra dữ liệu đã được tiêu thụ hay chưa.\n",
    "\n",
    "2. **Tạm dừng consumer khác:**\n",
    "   - Nếu có consumer đang tiêu thụ dữ liệu, tạm dừng hoặc dừng consumer đó để xác minh.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nguyên nhân 4: Offset không được đặt đúng**\n",
    "- **Control Center có thể không tự động đọc từ offset `earliest`.** Nếu offset mặc định được đặt là `latest`, nó sẽ không hiển thị dữ liệu đã được gửi trước đó.\n",
    "\n",
    "#### **Giải pháp:**\n",
    "1. **Đặt lại offset về `earliest`:**\n",
    "   - Trong Control Center, sử dụng tùy chọn `Jump to offset` và chọn offset `0`.\n",
    "   - Hoặc chạy lệnh sau để kiểm tra:\n",
    "     ```bash\n",
    "     docker exec -it broker kafka-console-consumer \\\n",
    "       --bootstrap-server broker:29092 \\\n",
    "       --topic users_created \\\n",
    "       --from-beginning\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Nguyên nhân 5: Vấn đề hiển thị trong Control Center**\n",
    "- **Control Center có thể bị lỗi caching hoặc refresh chậm.**\n",
    "\n",
    "#### **Giải pháp:**\n",
    "1. **Xóa cache trình duyệt:**\n",
    "   - Refresh trang hoặc thử mở lại Control Center trong trình duyệt ẩn danh.\n",
    "2. **Kiểm tra log Control Center:**\n",
    "   - Chạy lệnh kiểm tra log:\n",
    "     ```bash\n",
    "     docker logs control-center\n",
    "     ```\n",
    "   - Xem các lỗi liên quan đến kết nối hoặc hiển thị.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tóm tắt các bước cần thực hiện**\n",
    "1. **Gửi thêm dữ liệu mới vào Kafka và refresh Control Center.**\n",
    "2. **Đảm bảo dữ liệu được gửi dưới dạng JSON hoặc Avro.**\n",
    "3. **Kiểm tra các consumer khác đang tiêu thụ topic.**\n",
    "4. **Đặt offset về `earliest` để xem toàn bộ dữ liệu.**\n",
    "5. **Kiểm tra log của Control Center để phát hiện lỗi.**\n",
    "\n",
    "Nếu vẫn không khắc phục được, bạn có thể chia sẻ thêm thông tin log từ Control Center hoặc Kafka để phân tích chi tiết hơn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check thử \n",
    "```\n",
    "2. **Xác minh Schema Registry:**\n",
    "   - Kiểm tra Schema Registry bằng cách truy cập:\n",
    "     ```\n",
    "     http://localhost:8081\n",
    "     ```\n",
    "   - Đảm bảo rằng topic `users_created` có schema tương ứng được đăng ký.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nguyên nhân 3: Consumer đang tiêu thụ dữ liệu**\n",
    "- **Một Consumer khác có thể đang tiêu thụ hết dữ liệu từ topic `users_created`**, khiến Control Center không thể hiển thị dữ liệu nào trong tab `Messages`.\n",
    "\n",
    "#### **Giải pháp:**\n",
    "1. **Kiểm tra consumer:**\n",
    "   - Chạy lệnh kiểm tra các consumer group đang sử dụng topic `users_created`:\n",
    "     ```bash\n",
    "     docker exec -it broker kafka-consumer-groups --bootstrap-server broker:29092 --describe --all-groups\n",
    "     ```\n",
    "   - Xem các thông tin như `OFFSET` và `LAG` để kiểm tra dữ liệu đã được tiêu thụ hay chưa.\n",
    "\n",
    "2. **Tạm dừng consumer khác:**\n",
    "   - Nếu có consumer đang tiêu thụ dữ liệu, tạm dừng hoặc dừng consumer đó để xác minh.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phân tích vấn đề từ dữ liệu bạn cung cấp\n",
    "\n",
    "\n",
    "#### 2. **Không thấy Schema tại `localhost:8081`:**\n",
    "- Giao diện Schema Registry tại `http://localhost:8081` trả về `{}`.\n",
    "- Điều này cho thấy không có schema nào được đăng ký cho topic `users_created`.\n",
    "\n",
    "**Hệ quả:**\n",
    "- Nếu topic được ghi dữ liệu dưới dạng `JSON` hoặc `Avro`, nhưng không đăng ký schema, Control Center sẽ không biết cách diễn giải dữ liệu, và do đó không hiển thị dữ liệu trong tab **Messages**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Consumer group `_confluent-controlcenter-7-4-0-lastProduceTimeConsumer` không có thành viên hoạt động:**\n",
    "- **Thông báo:** `Consumer group '_confluent-controlcenter-7-4-0-lastProduceTimeConsumer' has no active members.`\n",
    "- Điều này cho thấy consumer group không có ứng dụng nào đang hoạt động để tiêu thụ dữ liệu từ topic `users_created`.\n",
    "\n",
    "---\n",
    "\n",
    "### Giải pháp để khắc phục\n",
    "\n",
    "\n",
    "\n",
    "#### 2. **Đăng ký Schema cho topic `users_created`:**\n",
    "- Nếu dữ liệu được gửi vào topic dưới dạng JSON hoặc Avro, cần đăng ký schema.\n",
    "- Đăng ký schema qua lệnh `curl` hoặc REST API của Schema Registry:\n",
    "  ```bash\n",
    "  curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" \\\n",
    "       --data '{\n",
    "         \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"User\\\",\\\"fields\\\":[{\\\"name\\\":\\\"first_name\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"last_name\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"address\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"post_code\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"email\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"username\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"dob\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"registered_date\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"phone\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"picture\\\",\\\"type\\\":\\\"string\\\"}]}\"\n",
    "       }' http://localhost:8081/subjects/users_created-value/versions\n",
    "  ```\n",
    "\n",
    "**Lưu ý:**\n",
    "- Thay schema trong JSON trên bằng định dạng thực tế của dữ liệu bạn đang gửi.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Khởi động lại consumer group cho topic `users_created`:**\n",
    "- Đảm bảo consumer group `_confluent-controlcenter-7-4-0-lastProduceTimeConsumer` có thành viên hoạt động.\n",
    "- Nếu cần, restart `Control Center`:\n",
    "  ```bash\n",
    "  docker-compose restart control-center\n",
    "  ```\n",
    "- Check lại Control Center\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix 1 hồi thì là do cần phải **Đăng ký Schema cho topic `users_created`:**\n",
    "```\n",
    "curl -X POST -H \"Content-Type: application/vnd.schemaregistry.v1+json\" \\\n",
    "     --data '{\n",
    "       \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"User\\\",\\\"fields\\\":[{\\\"name\\\":\\\"first_name\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"last_name\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"gender\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"address\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"post_code\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"email\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"username\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"dob\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"registered_date\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"phone\\\",\\\"type\\\":\\\"string\\\"},{\\\"name\\\":\\\"picture\\\",\\\"type\\\":\\\"string\\\"}]}\"\n",
    "     }' http://localhost:8081/subjects/users_created-value/versions\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
