{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Vọc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. With Cursor\n",
    "```\n",
    "@project-flashrag \n",
    "Kiểm tra xem project rag này: \n",
    "1. Pipeline như nào \n",
    "2. Các model đang được sử dụng chính? \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên thông tin từ các file được cung cấp, tôi có thể phân tích pipeline và các model chính được sử dụng trong project RAG này như sau:\n",
    "\n",
    "1. Pipeline:\n",
    "\n",
    "Pipeline chính của project này bao gồm các bước sau:\n",
    "\n",
    "a) Retriever: Sử dụng một mô hình retriever để tìm kiếm các đoạn văn bản liên quan từ corpus.\n",
    "\n",
    "b) (Tùy chọn) Reranker: Nếu được bật, sẽ sử dụng một mô hình reranker để sắp xếp lại các kết quả từ retriever.\n",
    "\n",
    "c) Generator: Sử dụng một mô hình ngôn ngữ lớn (LLM) để tạo câu trả lời dựa trên câu hỏi và các đoạn văn bản được truy xuất.\n",
    "\n",
    "Quy trình này được thể hiện trong file `main.py`:\n",
    "\n",
    "\n",
    "```183:193:project-flashrag/main.py\n",
    "    golden_answers = dataset.golden_answers\n",
    "    inference_rag = InferenceRAG(config)\n",
    "    pred_answer_list = inference_rag.process(dataset)\n",
    "    questions = dataset.question\n",
    "    retrieval_results = dataset.retrieval_result\n",
    "    data_writer = []\n",
    "    for idx, question in enumerate(questions):\n",
    "        reference = \"\"\n",
    "        for i, element in enumerate(retrieval_results[idx]):\n",
    "            reference += f\"Doc({i}):\\n\" + element.get(\"contents\") + \"\\n\"\n",
    "        data_writer.append([question, golden_answers[idx][0], pred_answer_list[idx], reference])\n",
    "```\n",
    "\n",
    "\n",
    "2. Các model đang được sử dụng chính:\n",
    "\n",
    "a) Retriever:\n",
    "- Mặc định: \"intfloat/e5-base-v2\"\n",
    "- Có thể được cấu hình trong file config:\n",
    "\n",
    "\n",
    "```3:7:project-flashrag/flashrag/config/basic_config.yaml\n",
    "model2path:\n",
    "  e5: \"intfloat/e5-base-v2\"\n",
    "  bge: \"intfloat/e5-base-v2\"\n",
    "  contriever: \"facebook/contriever\"\n",
    "  llama2-7B-chat: \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "```\n",
    "\n",
    "\n",
    "b) Reranker (tùy chọn):\n",
    "- Có thể được cấu hình trong file config:\n",
    "\n",
    "\n",
    "```63:70:project-flashrag/examples/methods/my_config.yaml\n",
    "use_reranker: False # whether to use reranker\n",
    "rerank_model_name: ~ # same as retrieval_method\n",
    "rerank_model_path: ~ # path to reranker model, path will be automatically find in `retriever_model2path`\n",
    "rerank_pooling_method: ~\n",
    "rerank_topk: 5  # number of remain documents after reranking\n",
    "rerank_max_length: 512\n",
    "rerank_batch_size: 256 # batch size for reranker\n",
    "rerank_use_fp16: True\n",
    "```\n",
    "\n",
    "\n",
    "c) Generator (LLM):\n",
    "- Mặc định: \"PartAI/Dorna-Llama3-8B-Instruct\"\n",
    "- Có thể được cấu hình trong file config:\n",
    "\n",
    "\n",
    "```73:80:project-flashrag/examples/methods/my_config.yaml\n",
    "framework: vllm # inference frame work of LLM, supporting: 'hf','vllm','fschat'\n",
    "generator_model: \"llama3-8B-instruct\" # name or path of the generator model\n",
    "generator_max_input_len: 2048  # max length of the input\n",
    "generator_batch_size: 2 # batch size for generation, invalid for vllm\n",
    "generation_params:\n",
    "  do_sample: False\n",
    "  max_tokens: 32\n",
    "use_fid: False # whether to use FID, only valid in encoder-decoder model\n",
    "```\n",
    "\n",
    "\n",
    "Ngoài ra, project cũng hỗ trợ sử dụng các mô hình GPT thông qua API OpenAI hoặc Azure, như được thể hiện trong file:\n",
    "\n",
    "\n",
    "```77:154:project-flashrag/deepeval/models/gpt_model.py\n",
    "class GPTModel(DeepEvalBaseLLM):\n",
    "    ...\n",
    "    def load_model(self):\n",
    "        if self.should_use_azure_openai():\n",
    "            openai_api_key = KEY_FILE_HANDLER.fetch_data(\n",
    "                KeyValues.AZURE_OPENAI_API_KEY\n",
    "            )\n",
    "            openai_api_version = KEY_FILE_HANDLER.fetch_data(\n",
    "                KeyValues.OPENAI_API_VERSION\n",
    "            )\n",
    "            azure_deployment = KEY_FILE_HANDLER.fetch_data(\n",
    "                KeyValues.AZURE_DEPLOYMENT_NAME\n",
    "            )\n",
    "            azure_endpoint = KEY_FILE_HANDLER.fetch_data(\n",
    "                KeyValues.AZURE_OPENAI_ENDPOINT\n",
    "            )\n",
    "            model_version = KEY_FILE_HANDLER.fetch_data(\n",
    "                KeyValues.AZURE_MODEL_VERSION\n",
    "            )\n",
    "\n",
    "            if model_version is None:\n",
    "                model_version = \"\"\n",
    "\n",
    "            return AzureChatOpenAI(\n",
    "                openai_api_version=openai_api_version,\n",
    "                azure_deployment=azure_deployment,\n",
    "                azure_endpoint=azure_endpoint,\n",
    "                openai_api_key=openai_api_key,\n",
    "                model_version=model_version,\n",
    "                *self.args,\n",
    "                **self.kwargs,\n",
    "            )\n",
    "        elif self.should_use_local_model():\n",
    "            model_name = KEY_FILE_HANDLER.fetch_data(KeyValues.LOCAL_MODEL_NAME)\n",
    "            openai_api_key = KEY_FILE_HANDLER.fetch_data(\n",
    "                KeyValues.LOCAL_MODEL_API_KEY\n",
    "            )\n",
    "            base_url = KEY_FILE_HANDLER.fetch_data(\n",
    "                KeyValues.LOCAL_MODEL_BASE_URL\n",
    "            )\n",
    "            format = KEY_FILE_HANDLER.fetch_data(KeyValues.LOCAL_MODEL_FORMAT)\n",
    "            return CustomChatOpenAI(\n",
    "                model_name=model_name,\n",
    "                openai_api_key=openai_api_key,\n",
    "                base_url=base_url,\n",
    "                # format=json to enable Ollama JSON mode\n",
    "                format=format,\n",
    "                # Hardcoded temperature to 0 to improve output JSON generation reliability\n",
    "                temperature=0,\n",
    "                *self.args,\n",
    "                **self.kwargs,\n",
    "            )\n",
    "```\n",
    "\n",
    "\n",
    "Tóm lại, project này sử dụng một pipeline RAG linh hoạt với khả năng tùy chỉnh các mô hình retriever, reranker (tùy chọn) và generator thông qua cấu hình. Các mô hình chính được sử dụng là E5 cho retriever và Llama 3 cho generator, nhưng có thể được thay đổi dễ dàng thông qua cấu hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trong path file này: `flashrag/retriever/retriever.py`\n",
    "\n",
    "```python\n",
    "local_files_only = kwargs.get(\"local_files_only\", False)\n",
    "```\n",
    "\n",
    "    Dòng này đang set giá trị mặc định của `local_files_only` là `False`. Điều này có nghĩa:\n",
    "\n",
    "    1. Nếu không có tham số `local_files_only` được truyền vào qua `kwargs`, giá trị mặc định sẽ là `False`.\n",
    "\n",
    "    2. Khi `local_files_only` là `False`, nó cho phép hệ thống tự động tải model từ internet nếu không tìm thấy model trong thư mục cache local.\n",
    "\n",
    "    3. Điều này rất hữu ích khi bạn muốn hệ thống tự động tải model nếu chưa có sẵn, giúp quá trình setup dễ dàng hơn.\n",
    "\n",
    "    4. Ngược lại, nếu bạn set `local_files_only=True` khi gọi hàm, nó sẽ chỉ sử dụng các file có sẵn trong máy và sẽ báo lỗi nếu không tìm thấy model locally.\n",
    "\n",
    "Vì vậy, với giá trị mặc định là `False`, hệ thống sẽ tự động download model nếu cần thiết, đúng như bạn đã hiểu."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
