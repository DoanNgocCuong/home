{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Từ code được chia sẻ, tôi thấy PROMPT đang được sử dụng như sau:\n",
    "\n",
    "1. **Định nghĩa PROMPT** trong `api/core/llm_generator/prompts.py`:\n",
    "```python\n",
    "SUGGESTED_QUESTIONS_AFTER_ANSWER_INSTRUCTION_PROMPT = (\n",
    "    \"I have some topics or key questions, and I want you to create a set of guideline questions to help users effectively explore and address the provided issues. The guideline questions should be specific, easy to understand, and offer clear directions, helping users focus on important aspects of the problem.\"\n",
    "    \"Based on the topics or questions I provide, create 3 guideline questions following this structure:\"\n",
    "\n",
    "    \"How to...\"\n",
    "    \"What is the best way to...\"\n",
    "    \"How often should...\"\n",
    "    \n",
    "    # Example provided in prompt\n",
    "    \"Example:\"\n",
    "    \"Topic/Key Question:\"\n",
    "    \"Have there been any specific feedback from employees about why they are struggling to use these tools?\"\n",
    "    \n",
    "    \"Guideline Questions:\"\n",
    "    \"How to collect employee feedback effectively?\"\n",
    "    \"What channels (e.g., surveys, interviews, team meetings) should be used to gather detailed feedback?\"\n",
    "    \"How often should feedback be collected to track changes effectively?\"\n",
    "    \n",
    "    \"Note: Make the guideline questions diverse, relevant to each topic/key question, and focused on actionable methods or directions. \"\n",
    "    \"NOTE: Câu hỏi gợi ý ở tiếng Việt\"\n",
    "    \n",
    "    # Output format requirement\n",
    "    \"The output must be an array in JSON format following the specified schema:\\n\"\n",
    "    '[\"question1\",\"question2\",\"question3\"]\\n'\n",
    ")\n",
    "```\n",
    "\n",
    "2. **Xử lý Output** trong `api/core/llm_generator/output_parser/suggested_questions_after_answer.py`:\n",
    "```python\n",
    "class SuggestedQuestionsAfterAnswerOutputParser:\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return SUGGESTED_QUESTIONS_AFTER_ANSWER_INSTRUCTION_PROMPT\n",
    "\n",
    "    def parse(self, text: str) -> Any:\n",
    "        # Tìm và parse JSON array từ output\n",
    "        action_match = re.search(r\"\\[.*?\\]\", text.strip(), re.DOTALL)\n",
    "        if action_match is not None:\n",
    "            json_obj = json.loads(action_match.group(0).strip())\n",
    "        else:\n",
    "            json_obj = []\n",
    "            print(f\"Could not parse LLM output: {text}\")\n",
    "        return json_obj\n",
    "```\n",
    "\n",
    "3. **Sử dụng trong Frontend** qua API endpoint:\n",
    "```typescript\n",
    "const suggestions = await minpalChatUserInputSuggestQuestionNodeExecution(\n",
    "  appId,\n",
    "  nodeExecution?.id,\n",
    "  variable,\n",
    ")\n",
    "```\n",
    "\n",
    "Đặc điểm của PROMPT:\n",
    "1. Yêu cầu tạo 3 câu hỏi gợi ý\n",
    "2. Cấu trúc câu hỏi theo dạng:\n",
    "   - \"How to...\"\n",
    "   - \"What is the best way to...\"\n",
    "   - \"How often should...\"\n",
    "3. Yêu cầu câu hỏi phải:\n",
    "   - Cụ thể\n",
    "   - Dễ hiểu\n",
    "   - Có hướng dẫn rõ ràng\n",
    "4. Output phải là JSON array\n",
    "5. Yêu cầu câu hỏi gợi ý bằng tiếng Việt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "        # api\\services\\message_service.py\n",
    "        # historis này sẽ được: generate_suggested_questions_after_answer sử dụng\n",
    "        histories = memory.get_history_prompt_text(\n",
    "            max_token_limit=3000,\n",
    "            message_limit=3,\n",
    "        )\n",
    "\n",
    "        with measure_time() as timer:\n",
    "            questions = LLMGenerator.generate_suggested_questions_after_answer(\n",
    "                tenant_id=app_model.tenant_id, histories=histories\n",
    "            )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # Phương thức lấy lịch sử chat dưới dạng list các message prompt\n",
    "    # api\\core\\memory\\token_buffer_memory.py\n",
    "    # get_history_prompt_messages từ các prompt message trước đó\n",
    "    def get_history_prompt_messages(\n",
    "        self, max_token_limit: int = 2000, message_limit: Optional[int] = None\n",
    "    ) -> list[PromptMessage]:\n",
    "        \"\"\"\n",
    "        Get history prompt messages.\n",
    "        :param max_token_limit: max token limit\n",
    "        :param message_limit: message limit\n",
    "        \"\"\"\n",
    "        app_record = self.conversation.app\n",
    "\n",
    "        # fetch limited messages, and return reversed\n",
    "        query = (\n",
    "            db.session.query(\n",
    "                Message.id,\n",
    "                Message.query,\n",
    "                Message.answer,\n",
    "                Message.created_at,\n",
    "                Message.workflow_run_id,\n",
    "                Message.parent_message_id,\n",
    "            )\n",
    "            .filter(\n",
    "                Message.conversation_id == self.conversation.id,\n",
    "            )\n",
    "            .order_by(Message.created_at.desc())\n",
    "        )\n",
    "\n",
    "        if message_limit and message_limit > 0:\n",
    "            message_limit = min(message_limit, 500)\n",
    "        else:\n",
    "            message_limit = 500\n",
    "\n",
    "        messages = query.limit(message_limit).all()\n",
    "\n",
    "        # instead of all messages from the conversation, we only need to extract messages\n",
    "        # that belong to the thread of last message\n",
    "        thread_messages = extract_thread_messages(messages)\n",
    "\n",
    "        # for newly created message, its answer is temporarily empty, we don't need to add it to memory\n",
    "        if thread_messages and not thread_messages[0].answer:\n",
    "            thread_messages.pop(0)\n",
    "\n",
    "        messages = list(reversed(thread_messages))\n",
    "\n",
    "        message_file_parser = MessageFileParser(tenant_id=app_record.tenant_id, app_id=app_record.id)\n",
    "        prompt_messages = []\n",
    "        for message in messages:\n",
    "            files = db.session.query(MessageFile).filter(MessageFile.message_id == message.id).all()\n",
    "            if files:\n",
    "                file_extra_config = None\n",
    "                if self.conversation.mode not in {AppMode.ADVANCED_CHAT.value, AppMode.WORKFLOW.value}:\n",
    "                    file_extra_config = FileUploadConfigManager.convert(self.conversation.model_config)\n",
    "                else:\n",
    "                    if message.workflow_run_id:\n",
    "                        workflow_run = (\n",
    "                            db.session.query(WorkflowRun).filter(WorkflowRun.id == message.workflow_run_id).first()\n",
    "                        )\n",
    "\n",
    "                        if workflow_run:\n",
    "                            file_extra_config = FileUploadConfigManager.convert(\n",
    "                                workflow_run.workflow.features_dict, is_vision=False\n",
    "                            )\n",
    "\n",
    "                if file_extra_config:\n",
    "                    file_objs = message_file_parser.transform_message_files(files, file_extra_config)\n",
    "                else:\n",
    "                    file_objs = []\n",
    "\n",
    "                if not file_objs:\n",
    "                    prompt_messages.append(UserPromptMessage(content=message.query))\n",
    "                else:\n",
    "                    prompt_message_contents = [TextPromptMessageContent(data=message.query)]\n",
    "                    for file_obj in file_objs:\n",
    "                        prompt_message_contents.append(file_obj.prompt_message_content)\n",
    "\n",
    "                    prompt_messages.append(UserPromptMessage(content=prompt_message_contents))\n",
    "            else:\n",
    "                prompt_messages.append(UserPromptMessage(content=message.query))\n",
    "\n",
    "            prompt_messages.append(AssistantPromptMessage(content=message.answer))\n",
    "\n",
    "        if not prompt_messages:\n",
    "            return []\n",
    "\n",
    "        # prune the chat message if it exceeds the max token limit\n",
    "        curr_message_tokens = self.model_instance.get_llm_num_tokens(prompt_messages)\n",
    "\n",
    "        if curr_message_tokens > max_token_limit:\n",
    "            pruned_memory = []\n",
    "            while curr_message_tokens > max_token_limit and len(prompt_messages) > 1:\n",
    "                pruned_memory.append(prompt_messages.pop(0))\n",
    "                curr_message_tokens = self.model_instance.get_llm_num_tokens(prompt_messages)\n",
    "\n",
    "        return prompt_messages\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Để cập nhật đoạn code trong file `suggestions.tsx` theo yêu cầu của bạn, bạn có thể thay thế đoạn mã hiện tại bằng đoạn mã mới mà bạn đã cung cấp. Dưới đây là cách thực hiện:\n",
    "\n",
    "```typescript:web/app/components/mindpal/workflow/visualizer/item/collapse-children-user-input/chat/chat-model/suggestions.tsx\n",
    "// ... existing code ...\n",
    "const onSend = async (suggestion: string) => {\n",
    "    setSuggestions([...[]])\n",
    "    setIsResponding(true)\n",
    "\n",
    "    // Lấy context từ nodeExecution\n",
    "    let nodeContext = '';\n",
    "\n",
    "    if (nodeExecution?.outputs) {\n",
    "        if (nodeExecution.outputs.json?.[0]?.text) {\n",
    "            nodeContext = nodeExecution.outputs.json[0].text; // Lấy text từ outputs.json\n",
    "        } else if (nodeExecution.outputs.text) {\n",
    "            try {\n",
    "                const parsed = JSON.parse(nodeExecution.outputs.text); // Parse JSON từ outputs.text\n",
    "                nodeContext = parsed.text;\n",
    "            } catch (e) {\n",
    "                console.error('Lỗi khi parse outputs.text:', e);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Sử dụng ngữ cảnh lấy được\n",
    "    const contextToUse = nodeContext || \"Default shared context\"; // Ngữ cảnh mặc định nếu không có\n",
    "\n",
    "    // Gửi tin nhắn với context\n",
    "    try {\n",
    "        await window.chatbot.sendMessageWindow(suggestion, contextToUse); // Gọi hàm gửi tin nhắn với context\n",
    "    } catch (error) {\n",
    "        console.error('Error sending message:', error);\n",
    "    }\n",
    "\n",
    "    // ... existing code ...\n",
    "}\n",
    "// ... existing code ...\n",
    "```\n",
    "\n",
    "### Giải thích các thay đổi:\n",
    "1. **Lấy ngữ cảnh**: Đoạn mã mới đã thêm vào để lấy ngữ cảnh từ `nodeExecution`, giống như cách bạn đã làm trong `recommendQ.tsx`.\n",
    "2. **Gửi ngữ cảnh**: Khi gọi `sendMessageWindow`, ngữ cảnh được truyền cùng với câu hỏi.\n",
    "\n",
    "Bằng cách này, bạn đã cập nhật đoạn mã trong `suggestions.tsx` để sử dụng ngữ cảnh từ `nodeExecution`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
