- [DoanNgocCuong/Big-Data-Project3-RealtimeStreamingEngineering](https://github.com/DoanNgocCuong/Big-Data-Project3-RealtimeStreamingEngineering)

![[Pasted image 20241216091835.png]]

Questions: 
1. What types of Dataset Processing or not do you save in to Hadoops ? - short answer
2. Batch Pipeline: 
- Data from Yelp Dataset. 
- To setup Batch Pipeline: We run docker-compose that file will start  sequence each technology by pulling images for docker hub. 
- 1. We setup Kafka batch:  (Apach Kafka Consumer) will get data from Kafka Producer and that consumer data.
- 1. Next we start Airflow and Dags, we can access 8080 for view Data Airflow. After setup processing Kafka and Airflow finished, we can view at UI Airflow: Data will sent   Airflow and Dags for scheduler for processing (such as: send data for daily, ....)
- 2. Apach Haddoop technology : that for 
- Save raw data from Kafka sent to Lưu trữ dữ liệu thô (raw data)
- Phân tán dữ liệu để đảm bảo fault tolerance
- Quản lý tài nguyên tính toán

- 2. The next step, we set up Apach Spark Worker 8081 and Apach Spark Master porrt 7077 internal and 9090:8080 external. We access UI for view full information about Spark of pipelin. 

---
Let me explain the Batch Layer architecture:

1. **Các công nghệ và vai trò trong Batch Layer:**

![Batch Layer Architecture](https://i.imgur.com/QXQGqGp.png)

a) **Apache Airflow (Orchestration):**
```yaml
services:
  postgres:      # Database for Airflow metadata
  webserver:     # Airflow UI & API
  scheduler:     # Manages DAG execution
```
- Điều phối và lập lịch các task trong batch processing
- Quản lý workflow và dependencies giữa các task

b) **Apache Kafka (Message Queue):**
```yaml
services:
  zookeeper:    # Manages Kafka cluster
  kafka:        # Message broker
```
- Thu thập dữ liệu từ nhiều nguồn
- Buffer dữ liệu trước khi xử lý
- Đảm bảo fault tolerance và scalability

c) **Apache Hadoop (Storage):**
```yaml
services:
  namenode:        # HDFS master node
  datanode:        # HDFS storage node
  resourcemanager: # YARN resource management
  nodemanager:     # YARN node execution
```
- Lưu trữ dữ liệu thô (raw data)
- Phân tán dữ liệu để đảm bảo fault tolerance
- Quản lý tài nguyên tính toán

d) **Apache Spark (Processing):**
```yaml
services:
  spark-master:  # Spark cluster manager
  spark-worker:  # Spark execution node
```
- Xử lý dữ liệu batch
- Thực hiện các phép transform phức tạp
- Tính toán phân tán

2. **Sequential Process trong Batch Layer:**

```mermaid
graph LR
    A[Data Collection] --> B[Data Storage]
    B --> C[Data Processing]
    C --> D[Data Storage]
    D --> E[Data Serving]
```

a) **Data Collection:**
```python
# HDFS_consumer.py
def consum_hdfs():
    consumer = KafkaConsumer(topic,...)
    for message in consumer:
        store_data_in_hdfs(data)
```
- Kafka consumer nhận dữ liệu
- Lưu vào HDFS

b) **Data Processing:**
```python
# batch_layer.py
def batch_layer():
    data = spark_tranform()  # Transform data using Spark
    save_data(data)          # Save to PostgreSQL
```
- Spark đọc dữ liệu từ HDFS
- Thực hiện các phép transform
- Lưu kết quả vào PostgreSQL

c) **Data Flow:**
1. Data → Kafka Topic
2. Kafka → HDFS (raw data storage)
3. HDFS → Spark (processing)
4. Spark → PostgreSQL (processed data)
5. Airflow orchestrates the entire process

Toàn bộ quá trình được Airflow điều phối thông qua DAGs, đảm bảo:
- Fault tolerance
- Scalability
- Data consistency
- Process monitoring
