- [DoanNgocCuong/Big-Data-Project3-RealtimeStreamingEngineering](https://github.com/DoanNgocCuong/Big-Data-Project3-RealtimeStreamingEngineering)

![[Pasted image 20241216091835.png]]

![[Pasted image 20241216131002.png]]

Questions: 
1. What types of Dataset Processing or not do you save in to Hadoops ? - short answer
2. Batch Pipeline: 
- Data from Yelp Dataset. 
- To setup Batch Pipeline: We run docker-compose that file will start  sequence each technology by pulling images for docker hub. 
- 1. We setup Kafka batch:  (Apach Kafka Consumer) will get data from Kafka Producer and that consumer data.
- 1. Next we start Airflow and Dags, we can access 8080 for view Data Airflow. After setup processing Kafka and Airflow finished, we can view at UI Airflow: Data will sent   Airflow and Dags for scheduler for processing (such as: send data for daily, ....)
- 2. Apach Haddoop technology : that for 
- Save raw data from Kafka sent to Lưu trữ dữ liệu thô (raw data)
- Phân tán dữ liệu để đảm bảo fault tolerance
- Quản lý tài nguyên tính toán

- 2. The next step, we set up Apach Spark Worker 8081 and Apach Spark Master porrt 7077 internal and 9090:8080 external. We access UI for view full information about Spark of pipelin. 

---
Let me explain the Batch Layer architecture:
![[Pasted image 20241216104949.png]]
1. **Các công nghệ và vai trò trong Batch Layer:**

![Batch Layer Architecture](https://i.imgur.com/QXQGqGp.png)

a) **Apache Airflow (Orchestration):**
```yaml
services:
  postgres:      # Database for Airflow metadata
  webserver:     # Airflow UI & API
  scheduler:     # Manages DAG execution
```
- Điều phối và lập lịch các task trong batch processing
- Quản lý workflow và dependencies giữa các task

b) **Apache Kafka (Message Queue):**
```yaml
services:
  zookeeper:    # Manages Kafka cluster
  kafka:        # Message broker
```
- Thu thập dữ liệu từ nhiều nguồn
- Buffer dữ liệu trước khi xử lý
- Đảm bảo fault tolerance và scalability

c) **Apache Hadoop (Storage):**
```yaml
services:
  namenode:        # HDFS master node
  datanode:        # HDFS storage node
  resourcemanager: # YARN resource management
  nodemanager:     # YARN node execution
```
- Lưu trữ dữ liệu thô (raw data)
- Phân tán dữ liệu để đảm bảo fault tolerance
- Quản lý tài nguyên tính toán

d) **Apache Spark (Processing):**
```yaml
services:
  spark-master:  # Spark cluster manager
  spark-worker:  # Spark execution node
```
- Xử lý dữ liệu batch
- Thực hiện các phép transform phức tạp
- Tính toán phân tán

2. **Sequential Process trong Batch Layer:**

```mermaid
graph LR
    A[Data Collection] --> B[Data Storage]
    B --> C[Data Processing]
    C --> D[Data Storage]
    D --> E[Data Serving]
```

a) **Data Collection:**
```python
# HDFS_consumer.py
def consum_hdfs():
    consumer = KafkaConsumer(topic,...)
    for message in consumer:
        store_data_in_hdfs(data)
```
- Kafka consumer nhận dữ liệu
- Lưu vào HDFS

b) **Data Processing:**
```python
# batch_layer.py
def batch_layer():
    data = spark_tranform()  # Transform data using Spark
    save_data(data)          # Save to PostgreSQL
```
- Spark đọc dữ liệu từ HDFS
- Thực hiện các phép transform
- Lưu kết quả vào PostgreSQL

c) **Data Flow:**
1. Data → Kafka Topic
2. Kafka → HDFS (raw data storage)
3. HDFS → Spark (processing)
4. Spark → PostgreSQL (processed data)
5. Airflow orchestrates the entire process

Toàn bộ quá trình được Airflow điều phối thông qua DAGs, đảm bảo:
- Fault tolerance
- Scalability
- Data consistency
- Process monitoring

---
Dựa trên Yelp Dataset, tôi sẽ tạo một file CSV mẫu với các trường tương tự:

```csv
business_id,name,address,city,state,postal_code,latitude,longitude,stars,review_count,categories,attributes,hours,is_open
B1001,Pho Delicious,123 Main St,Phoenix,AZ,85001,33.4484,-112.0740,4.5,256,"Vietnamese, Restaurants, Noodles","{delivery: true, takeout: true, price_range: 2}","Mon-Sat: 10:00-22:00, Sun: 11:00-21:00",1
B1002,Tech Gadgets Store,456 Oak Ave,Las Vegas,NV,89101,36.1699,-115.1398,4.0,189,"Electronics, Retail, Mobile Phones","{wifi: true, parking: street, credit_cards: true}","Mon-Fri: 9:00-20:00, Sat: 10:00-18:00",1
B1003,Coffee Haven,789 Pine Rd,Phoenix,AZ,85004,33.4538,-112.0711,4.8,423,"Coffee & Tea, Breakfast & Brunch, Cafes","{outdoor_seating: true, noise_level: average}","Mon-Sun: 7:00-19:00",1
B1004,Fitness Zone,321 Elm St,Las Vegas,NV,89102,36.1723,-115.1465,3.9,167,"Gyms, Fitness, Personal Training","{appointment_required: true, parking: garage}","Mon-Fri: 6:00-22:00, Sat-Sun: 8:00-20:00",1
B1005,Pizza Palace,654 Maple Dr,Phoenix,AZ,85006,33.4591,-112.0648,4.2,312,"Pizza, Italian, Restaurants","{delivery: true, takeout: true, price_range: 2}","Mon-Sun: 11:00-23:00",1
B1006,Desert Spa,987 Cedar Ln,Las Vegas,NV,89103,36.1147,-115.1728,4.7,289,"Spa, Massage, Beauty & Wellness","{appointment_required: true, credit_cards: true}","Tue-Sat: 10:00-19:00, Sun: 11:00-17:00",1
B1007,Burger Joint,147 Birch St,Phoenix,AZ,85008,33.4484,-112.0440,3.8,178,"Burgers, Fast Food, American","{drive_thru: true, takeout: true, price_range: 1}","Mon-Sun: 10:30-22:00",1
B1008,Book Corner,258 Willow Ave,Las Vegas,NV,89104,36.1577,-115.1466,4.6,145,"Books, Retail, Coffee & Tea","{wifi: true, quiet: true, parking: street}","Mon-Sat: 9:00-21:00, Sun: 10:00-19:00",1
B1009,Auto Service Pro,369 Palm Rd,Phoenix,AZ,85010,33.4519,-112.0576,4.3,234,"Auto Repair, Car Service, Mechanics","{appointment_required: true, credit_cards: true}","Mon-Fri: 8:00-18:00, Sat: 9:00-15:00",1
B1010,Sushi Express,741 Beach Blvd,Las Vegas,NV,89106,36.1659,-115.1489,4.4,267,"Japanese, Sushi, Asian Fusion","{delivery: true, takeout: true, price_range: 3}","Mon-Sun: 11:30-22:30",1
```

Dataset này bao gồm các trường:
1. business_id: ID doanh nghiệp
2. name: Tên doanh nghiệp
3. address: Địa chỉ
4. city: Thành phố
5. state: Bang
6. postal_code: Mã bưu chính
7. latitude: Vĩ độ
8. longitude: Kinh độ
9. stars: Đánh giá sao (1-5)
10. review_count: Số lượng đánh giá
11. categories: Danh mục kinh doanh
12. attributes: Các thuộc tính bổ sung
13. hours: Giờ hoạt động
14. is_open: Trạng thái hoạt động (1: mở, 0: đóng)

---


Ah, để sử dụng bộ dataset Yelp, tôi sẽ tạo sample data và điều chỉnh code phù hợp:

1. **Tạo sample test_data.csv theo cấu trúc Yelp:**
```csv
business_id,name,address,city,state,postal_code,latitude,longitude,stars,review_count,categories,attributes,hours,is_open
B1001,Pho Delicious,123 Main St,Phoenix,AZ,85001,33.4484,-112.0740,4.5,256,"Vietnamese, Restaurants, Noodles","{delivery: true, takeout: true, price_range: 2}","Mon-Sat: 10:00-22:00, Sun: 11:00-21:00",1
B1002,Tech Gadgets Store,456 Oak Ave,Las Vegas,NV,89101,36.1699,-115.1398,4.0,189,"Electronics, Retail, Mobile Phones","{wifi: true, parking: street, credit_cards: true}","Mon-Fri: 9:00-20:00, Sat: 10:00-18:00",1
B1003,Coffee Haven,789 Pine Rd,Phoenix,AZ,85004,33.4538,-112.0711,4.8,423,"Coffee & Tea, Breakfast & Brunch, Cafes","{outdoor_seating: true, noise_level: average}","Mon-Sun: 7:00-19:00",1
```

2. **Điều chỉnh put_data_hdfs.py:**
```python
import pandas as pd
from hdfs import InsecureClient

def store_data_in_hdfs(business_data):
    # Update columns to match Yelp data structure
    columns = ['business_id', 'name', 'address', 'city', 'state', 
              'postal_code', 'latitude', 'longitude', 'stars', 
              'review_count', 'categories', 'attributes', 'hours', 'is_open']
    
    transaction_df = pd.DataFrame([business_data], columns=columns)
    
    hdfs_host = 'localhost'
    hdfs_port = 9870  # Updated port
    
    client = InsecureClient(f'http://{hdfs_host}:{hdfs_port}')
    
    # Store in HDFS
    if not client.content('/batch-layer/yelp_data.csv'):
        transaction_df.to_csv('/batch-layer/yelp_data.csv', index=False)
    else:
        with client.read('/batch-layer/yelp_data.csv') as reader:
            existing_df = pd.read_csv(reader)
        combined_df = pd.concat([existing_df, transaction_df], ignore_index=True)
        with client.write('/batch-layer/yelp_data.csv', overwrite=True) as writer:
            combined_df.to_csv(writer, index=False)
```

3. **Điều chỉnh spark_transformation.py:**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def spark_tranform():
    spark = SparkSession.builder \
        .appName("Yelp Business Analysis") \
        .getOrCreate()

    # Read data
    df = spark.read.csv("/batch-layer/yelp_data.csv", header=True)

    # Business Analytics
    business_analytics = df.select(
        "business_id",
        "name",
        "city",
        "state",
        "categories",
        "stars",
        "review_count",
        "is_open"
    )

    # Aggregate metrics
    city_analytics = business_analytics.groupBy("city", "state") \
        .agg(
            avg("stars").alias("avg_rating"),
            sum("review_count").alias("total_reviews"),
            count("*").alias("business_count"),
            avg(when(col("is_open") == 1, 1).otherwise(0)).alias("active_business_ratio")
        )

    return city_analytics
```

4. **Điều chỉnh save_data_postgresql.py:**
```python
from sqlalchemy import create_engine

def save_data(data):
    # Create PostgreSQL connection
    engine = create_engine('postgresql://postgres:aymane2002@localhost:5432/yelp_analytics')
    
    # Convert Spark DataFrame to Pandas and save
    pandas_df = data.toPandas()
    
    # Save to PostgreSQL
    pandas_df.to_sql('city_business_analytics', 
                     engine, 
                     if_exists='replace', 
                     index=False)
    
    print("Data successfully stored in PostgreSQL")
```

5. **Điều chỉnh batch_pipeline.py:**
```python
import time
from producer import send_message
from HDFS_consumer import consum_hdfs
import threading
from Stream_data.stream_data import generate_yelp_data  # Updated function name

def producer_thread():
    while True:
        try:
            file_path = '../Stream_data/test_data.csv'
            message = generate_yelp_data(file_path)  # Generate Yelp format data
            
            send_message(message)
            print("Yelp business data sent to Kafka topic")
            
            time.sleep(5)
            
        except Exception as e:
            print(f"Error in producer_thread: {str(e)}")
```

Luồng xử lý dữ liệu:
1. Yelp data → Kafka producer
2. Kafka → HDFS consumer
3. HDFS → Spark transformation
   - Phân tích theo thành phố/tiểu bang
   - Tính rating trung bình
   - Đếm số lượng review
   - Phân tích tỷ lệ doanh nghiệp đang hoạt động
4. Transformed data → PostgreSQL

Analytics sẽ tập trung vào:
- Phân bố doanh nghiệp theo khu vực
- Rating và review trends
- Categorization analysis
- Business activity metrics


---
1. Yelp Data -> Kafka producer

 **Apache Kafka (Message Queue):**
```yaml
services:
  zookeeper:    # Manages Kafka cluster
  kafka:        # Message broker
```
- collect data from Yelp dataset. 
- Zoopkeeper fo manages kafka cluster 
- Kafke: message broker 

1. Kafka -> HDFS consumer: 
- Kafka distributed data to HDFS (Distributed file systems) for HDFS consumer this data. 
- HDFS for saving dataset and ....

Yelp data send data from producer() to consumer() (HDFS_consumer.py)
and do task: put_data_hdfs.py  (storage data in HDFS). 
2. Air flow and Dags for scheduler and task manager. 
Mini Services into that: 
- postgres: Database for Airflow  (data from batch Kafka batch send to HDFS inthrough by scheduler by Airflow is save into postgres)
- Webserver: Airflow UI and API  
- Scheduler: Manages DAG excutio

**Apache Airflow (Orchestration):**
```yaml
services:
  postgres:      # Database for Airflow metadata
  webserver:     # Airflow UI & API
  scheduler:     # Manages DAG execution
```

3. After save HDFS, data is processed by (spark_transformation.py), the reason that spark is used in Processing Data Tranformation because: Spark is fast and 

```bash
Why Hadoop for saving dataset ? 

Because : 
1. Distributed Storage & Scalability
graph LR
    A[Data] --> B[Block 1]
    A --> C[Block 2]
    A --> D[Block N]
    B --> E[Node 1]
    C --> F[Node 2]
    D --> G[Node N]
2. 
HDFS can handle:
- Petabytes of data
- Thousands of nodes
- Millions of files

3. Data Locality: Data Location = Processing Location: 
- Reduces network congestion
- Improves processing speed
```

```
Why Apache Spark for Data Processing?



**3 Key Reasons for Using Spark Processing with HDFS:**

1. **In-Memory Processing & Speed**
mermaid
graph LR
    A[HDFS Data] --> B[Spark Memory]
    B --> C[Process]
    C --> D[Cache]
    D --> C

- 100x faster than MapReduce
- Caches data in memory
- Minimizes disk I/O

2. **Advanced Analytics Capabilities**
python
# Multiple Processing Options
df = spark.read.csv("hdfs://data.csv")

# SQL
df.createOrReplaceTempView("data")
spark.sql("SELECT * FROM data")

# DataFrame API
df.groupBy("city").agg(avg("stars"))

# ML Pipeline
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[...])

- SQL processing
- Machine Learning
- Graph processing
- Streaming

3. **Data Locality with HDFS**
python
# Spark reads HDFS blocks locally
HDFS Block → Spark Executor (Same Node)

- Direct access to HDFS blocks
- Minimizes data transfer
- Optimized data processing

These features make Spark + HDFS a powerful combination for large-scale data processing.

```