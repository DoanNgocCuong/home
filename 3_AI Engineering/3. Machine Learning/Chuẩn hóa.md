
### Gi·∫£i th√≠ch si√™u ƒë∆°n gi·∫£n v·ªÅ 3 ki·ªÉu chu·∫©n h√≥a cho h·ªçc sinh c·∫•p 2:

---

1. **Min-Max Scaling** (K√©o m·ªçi th·ª© v·ªÅ t·ª´ 0 ƒë·∫øn 1)
    - **H√¨nh dung**:
        - Gi·∫£ s·ª≠ b·∫°n c√≥ c√°c s·ªë ƒëi·ªÉm ki·ªÉm tra t·ª´ 5 ƒë·∫øn 10. B·∫°n mu·ªën t·∫•t c·∫£ ƒëi·ªÉm n·∫±m trong kho·∫£ng t·ª´ 0 ƒë·∫øn 1.
        - N·∫øu 5 l√† ƒëi·ªÉm th·∫•p nh·∫•t v√† 10 l√† cao nh·∫•t, th√¨ 5 s·∫Ω th√†nh 0, 10 s·∫Ω th√†nh 1, v√† c√°c ƒëi·ªÉm kh√°c s·∫Ω n·∫±m gi·ªØa.
    - **C√°ch l√†m**:
        - L·∫•y ƒëi·ªÉm tr·ª´ ƒëi ƒëi·ªÉm th·∫•p nh·∫•t (5), r·ªìi chia cho kho·∫£ng c√°ch gi·ªØa ƒëi·ªÉm cao nh·∫•t v√† th·∫•p nh·∫•t (10 - 5).
    - **V√≠ d·ª•**: ƒêi·ªÉm l√† 7.  
        ƒêi·ªÉm¬†m·ªõi=7‚àí510‚àí5=0.4\text{ƒêi·ªÉm m·ªõi} = \frac{7 - 5}{10 - 5} = 0.4

---

2. **Standard Scaling** (K√©o m·ªçi th·ª© v·ªÅ m·ª©c trung b√¨nh 0 v√† tr·∫£i ƒë·ªÅu)
    - **H√¨nh dung**:
        - T∆∞·ªüng t∆∞·ª£ng l·ªõp b·∫°n c√≥ chi·ªÅu cao t·ª´ 1.4m ƒë·∫øn 1.8m, v√† chi·ªÅu cao trung b√¨nh l√† 1.6m.
        - B·∫°n mu·ªën bi·∫øt b·∫°n cao h∆°n hay th·∫•p h∆°n trung b√¨nh bao nhi√™u l·∫ßn (theo m·ªôt "ƒë∆°n v·ªã chu·∫©n").
    - **C√°ch l√†m**:
        - L·∫•y chi·ªÅu cao c·ªßa b·∫°n tr·ª´ ƒëi chi·ªÅu cao trung b√¨nh (1.6m), r·ªìi chia cho ƒë·ªô ch√™nh l·ªách trung b√¨nh (ƒë·ªô l·ªách chu·∫©n) c·ªßa c·∫£ l·ªõp.
    - **V√≠ d·ª•**: Chi·ªÅu cao c·ªßa b·∫°n l√† 1.7m.  
        ChieÀÜÀãu¬†cao¬†m·ªõi=1.7‚àí1.60.1=1\text{Chi·ªÅu cao m·ªõi} = \frac{1.7 - 1.6}{0.1} = 1 (B·∫°n cao h∆°n 1 l·∫ßn so v·ªõi trung b√¨nh).

---

3. **Log Transformation** (Thu nh·ªè s·ªë l·ªõn, gi·ªØ s·ªë nh·ªè)
    - **H√¨nh dung**:
        - B·∫°n c√≥ c√°c s·ªë r·∫•t l·ªõn, nh∆∞ 1000, v√† mu·ªën thu nh·ªè ch√∫ng ƒë·ªÉ d·ªÖ nh√¨n nh∆∞ng v·∫´n gi·ªØ ƒë√∫ng th·ª© t·ª±.
        - S·ªë l·ªõn s·∫Ω b·ªã "n√©n l·∫°i", trong khi s·ªë nh·ªè kh√¥ng thay ƒë·ªïi nhi·ªÅu.
    - **C√°ch l√†m**:
        - L·∫•y "logarit" c·ªßa s·ªë ƒë√≥ (b·∫°n c√≥ th·ªÉ h√¨nh dung log l√† m·ªôt c√°ch ƒë·ªÉ thu nh·ªè s·ªë l·ªõn theo b·∫≠c).
    - **V√≠ d·ª•**: S·ªë l√† 1000.  
        $\( \text{S·ªë m·ªõi} = \log(1000) = 3 \) (v√¨ \( 10^3 = 1000 \))$

---

### T√≥m t·∫Øt:

- **Min-Max Scaling**: Co k√©o m·ªçi th·ª© v·ªÅ t·ª´ 0 ƒë·∫øn 1, gi·ªëng nh∆∞ v·∫°ch s·ªë t·ª´ ƒëi·ªÉm th·∫•p nh·∫•t ƒë·∫øn cao nh·∫•t.
- **Standard Scaling**: ƒêi·ªÅu ch·ªânh ƒë·ªÉ m·ªçi th·ª© xoay quanh "trung b√¨nh 0" v√† so s√°nh theo ƒë·ªô ch√™nh l·ªách chu·∫©n.
- **Log Transformation**: Thu nh·ªè s·ªë l·ªõn ƒë·ªÉ d·ªÖ x·ª≠ l√Ω, nh∆∞ "n√©n" ch√∫ng l·∫°i.
### B·∫£ng So S√°nh Chi Ti·∫øt C√°c Ki·ªÉu Chu·∫©n H√≥a

| **Chu·∫©n h√≥a**          | **C√°ch th·ª©c**                                                                                                   | **∆Øu ƒëi·ªÉm**                                                                                                         | **Nh∆∞·ª£c ƒëi·ªÉm**                                                                                                 |
| ---------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| **Min-Max Scaling**    | [0, 1]: x‚Ä≤=x‚àíxminxmax‚àíxminx' = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}x‚Ä≤=xmax‚Äã‚àíxmin‚Äãx‚àíxmin‚Äã‚Äã | - Gi·ªØ nguy√™n t·ª∑ l·ªá gi·ªØa c√°c gi√° tr·ªã.  <br>- D·ªÖ hi·ªÉu v√† ph·ªï bi·∫øn.  <br>- Hi·ªáu qu·∫£ khi d·ªØ li·ªáu kh√¥ng c√≥ ngo·∫°i l·ªá l·ªõn. | - Nh·∫°y c·∫£m v·ªõi ngo·∫°i l·ªá (outliers).  <br>- Gi√° tr·ªã n·∫±m ngo√†i ph·∫°m vi training (khi predict) c√≥ th·ªÉ b·ªã l·ªách.    |
| **Standard Scaling**   | Ph√¢n ph·ªëi chu·∫©n: x‚Ä≤=x‚àíŒºœÉx' = \frac{x - \mu}{\sigma}x‚Ä≤=œÉx‚àíŒº‚Äã                                                     | - Kh√¥ng nh·∫°y c·∫£m v·ªõi ngo·∫°i l·ªá.  <br>- Ph√π h·ª£p khi c√°c feature c√≥ ph√¢n ph·ªëi chu·∫©n.  <br>- T·ªët cho Ridge Regression.  | - Kh√¥ng gi·ªØ nguy√™n t·ª∑ l·ªá gi·ªØa c√°c gi√° tr·ªã g·ªëc.  <br>- Kh√¥ng ph√π h·ª£p khi d·ªØ li·ªáu c√≥ ph√¢n ph·ªëi kh√¥ng chu·∫©n.      |
| **Log Transformation** | Bi·∫øn ƒë·ªïi log: x‚Ä≤=log‚Å°(x+1)x' = \log(x + 1)x‚Ä≤=log(x+1)                                                           | - X·ª≠ l√Ω t·ªët s·ª± ch√™nh l·ªách gi·ªØa c√°c gi√° tr·ªã l·ªõn v√† nh·ªè.  <br>- Gi·∫£m t√°c ƒë·ªông c·ªßa ngo·∫°i l·ªá.                           | - Kh√¥ng th·ªÉ x·ª≠ l√Ω gi√° tr·ªã √¢m ho·∫∑c 0 (c·∫ßn x·ª≠ l√Ω tr∆∞·ªõc).  <br>- D·ªÖ g√¢y m·∫•t th√¥ng tin n·∫øu kh√¥ng √°p d·ª•ng c·∫©n th·∫≠n. |

### B·∫£ng So S√°nh Chi Ti·∫øt C√°c Ki·ªÉu Chu·∫©n H√≥a

| **Chu·∫©n h√≥a**          | **C√°ch th·ª©c**                                                                               | **∆Øu ƒëi·ªÉm**                                                                                             | **Nh∆∞·ª£c ƒëi·ªÉm**                                                                                           |
| ---------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **Min-Max Scaling**    | [0, 1]: $x‚Ä≤=x‚àíxminxmax‚àíxminx' = \frac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}$ | - Gi·ªØ nguy√™n t·ª∑ l·ªá gi·ªØa c√°c gi√° tr·ªã.- D·ªÖ hi·ªÉu v√† ph·ªï bi·∫øn.- Hi·ªáu qu·∫£ khi d·ªØ li·ªáu kh√¥ng c√≥ ngo·∫°i l·ªá l·ªõn. | - Nh·∫°y c·∫£m v·ªõi ngo·∫°i l·ªá (outliers).- Gi√° tr·ªã n·∫±m ngo√†i ph·∫°m vi training (khi predict) c√≥ th·ªÉ b·ªã l·ªách.    |
| **Standard Scaling**   | Ph√¢n ph·ªëi chu·∫©n: $x‚Ä≤=x‚àíŒºœÉx' = \frac{x - \mu}{\sigma}$                                       | - Kh√¥ng nh·∫°y c·∫£m v·ªõi ngo·∫°i l·ªá.- Ph√π h·ª£p khi c√°c feature c√≥ ph√¢n ph·ªëi chu·∫©n.- T·ªët cho Ridge Regression.  | - Kh√¥ng gi·ªØ nguy√™n t·ª∑ l·ªá gi·ªØa c√°c gi√° tr·ªã g·ªëc.- Kh√¥ng ph√π h·ª£p khi d·ªØ li·ªáu c√≥ ph√¢n ph·ªëi kh√¥ng chu·∫©n.      |
| **Log Transformation** | Bi·∫øn ƒë·ªïi log: $x‚Ä≤=log‚Å°(x+1)x' = \log(x + 1)$                                                | - X·ª≠ l√Ω t·ªët s·ª± ch√™nh l·ªách gi·ªØa c√°c gi√° tr·ªã l·ªõn v√† nh·ªè.- Gi·∫£m t√°c ƒë·ªông c·ªßa ngo·∫°i l·ªá.                     | - Kh√¥ng th·ªÉ x·ª≠ l√Ω gi√° tr·ªã √¢m ho·∫∑c 0 (c·∫ßn x·ª≠ l√Ω tr∆∞·ªõc).- D·ªÖ g√¢y m·∫•t th√¥ng tin n·∫øu kh√¥ng √°p d·ª•ng c·∫©n th·∫≠n. |


---
## Chu·∫©n h√≥a d·ªØ li·ªáu v√† c√°c lo·∫°i chu·∫©n h√≥a d·ªØ li·ªáu trong Machine Learning

### 1. Chu·∫©n h√≥a d·ªØ li·ªáu l√† g√¨?

Chu·∫©n h√≥a d·ªØ li·ªáu (Data Normalization) l√† qu√° tr√¨nh bi·∫øn ƒë·ªïi c√°c gi√° tr·ªã c·ªßa d·ªØ li·ªáu v·ªÅ m·ªôt ph·∫°m vi ho·∫∑c ph√¢n ph·ªëi nh·∫•t ƒë·ªãnh, gi√∫p m√¥ h√¨nh h·ªçc m√°y ho·∫°t ƒë·ªông hi·ªáu qu·∫£ h∆°n. M·ª•c ti√™u c·ªßa chu·∫©n h√≥a l√†:

- **Gi·∫£m s·ª± ch√™nh l·ªách gi·ªØa c√°c thu·ªôc t√≠nh**: M·ªôt s·ªë ƒë·∫∑c tr∆∞ng c√≥ gi√° tr·ªã r·∫•t l·ªõn so v·ªõi ƒë·∫∑c tr∆∞ng kh√°c c√≥ th·ªÉ l√†m ·∫£nh h∆∞·ªüng ƒë·∫øn m√¥ h√¨nh.
- **TƒÉng t·ªëc ƒë·ªô h·ªôi t·ª•**: C√°c thu·∫≠t to√°n nh∆∞ Gradient Descent s·∫Ω ho·∫°t ƒë·ªông nhanh h∆°n n·∫øu d·ªØ li·ªáu ƒë∆∞·ª£c chu·∫©n h√≥a.
- **C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c**: Gi√∫p m√¥ h√¨nh tr√°nh b·ªã ·∫£nh h∆∞·ªüng b·ªüi s·ª± kh√°c bi·ªát v·ªÅ ƒë∆°n v·ªã ƒëo l∆∞·ªùng gi·ªØa c√°c ƒë·∫∑c tr∆∞ng.

### 2. C√°c lo·∫°i chu·∫©n h√≥a d·ªØ li·ªáu ph·ªï bi·∫øn

#### 2.1. **Min-Max Scaling (Feature Scaling)**

- ƒê∆∞a d·ªØ li·ªáu v·ªÅ kho·∫£ng **[0, 1]** ho·∫∑c **[-1, 1]**.
- C√¥ng th·ª©c: X‚Ä≤=X‚àíXmin‚Å°Xmax‚Å°‚àíXmin‚Å°X' = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
- **∆Øu ƒëi·ªÉm**: Gi·ªØ nguy√™n m·ªëi quan h·ªá gi·ªØa c√°c gi√° tr·ªã trong d·ªØ li·ªáu.
- **Nh∆∞·ª£c ƒëi·ªÉm**: Nh·∫°y c·∫£m v·ªõi ngo·∫°i l·ªá (outliers).
- **Khi s·ª≠ d·ª•ng**: Khi d·ªØ li·ªáu c√≥ gi·ªõi h·∫°n r√µ r√†ng, ch·∫≥ng h·∫°n trong c√°c b√†i to√°n th·ªã gi√°c m√°y t√≠nh.

#### 2.2. **Z-score Normalization (Standardization)**

- Bi·∫øn ƒë·ªïi d·ªØ li·ªáu v·ªÅ ph√¢n ph·ªëi chu·∫©n v·ªõi gi√° tr·ªã trung b√¨nh **0** v√† ƒë·ªô l·ªách chu·∫©n **1**.
- C√¥ng th·ª©c: X‚Ä≤=X‚àíŒºœÉX' = \frac{X - \mu}{\sigma} Trong ƒë√≥:
    - Œº\mu l√† gi√° tr·ªã trung b√¨nh c·ªßa d·ªØ li·ªáu.
    - œÉ\sigma l√† ƒë·ªô l·ªách chu·∫©n.
- **∆Øu ƒëi·ªÉm**: √çt b·ªã ·∫£nh h∆∞·ªüng b·ªüi ngo·∫°i l·ªá h∆°n Min-Max Scaling.
- **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng gi·ªõi h·∫°n ph·∫°m vi gi√° tr·ªã.
- **Khi s·ª≠ d·ª•ng**: Khi d·ªØ li·ªáu c√≥ ph√¢n ph·ªëi Gauss (chu·∫©n) ho·∫∑c khi s·ª≠ d·ª•ng m√¥ h√¨nh tuy·∫øn t√≠nh.

#### 2.3. **Robust Scaling**

- D√πng **median (trung v·ªã)** v√† **IQR (Interquartile Range ‚Äì kho·∫£ng t·ª© ph√¢n v·ªã)** ƒë·ªÉ gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa ngo·∫°i l·ªá.
- C√¥ng th·ª©c: X‚Ä≤=X‚àímedian(X)IQR(X)X' = \frac{X - \text{median}(X)}{\text{IQR}(X)}
- **∆Øu ƒëi·ªÉm**: √çt b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers.
- **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng ph√π h·ª£p n·∫øu d·ªØ li·ªáu kh√¥ng c√≥ outliers r√µ r√†ng.
- **Khi s·ª≠ d·ª•ng**: Khi d·ªØ li·ªáu c√≥ nhi·ªÅu ngo·∫°i l·ªá.

#### 2.4. **Log Transformation (Bi·∫øn ƒë·ªïi log)**

- Chuy·ªÉn d·ªØ li·ªáu theo h√†m logarit: X‚Ä≤=log‚Å°(X+c)X' = \log(X + c) (c√≥ th·ªÉ th√™m h·∫±ng s·ªë cc ƒë·ªÉ tr√°nh gi√° tr·ªã √¢m).
- **∆Øu ƒëi·ªÉm**: Gi·∫£m s·ª± l·ªách c·ªßa ph√¢n ph·ªëi d·ªØ li·ªáu (th√≠ch h·ª£p v·ªõi d·ªØ li·ªáu l·ªách ph·∫£i).
- **Nh∆∞·ª£c ƒëi·ªÉm**: Kh√¥ng ph√π h·ª£p v·ªõi d·ªØ li·ªáu c√≥ gi√° tr·ªã √¢m ho·∫∑c b·∫±ng 0.
- **Khi s·ª≠ d·ª•ng**: Khi d·ªØ li·ªáu c√≥ ƒë·ªô ch√™nh l·ªách l·ªõn (skewed data).

#### 2.5. **Power Transformation (Box-Cox & Yeo-Johnson)**

- Gi√∫p ph√¢n ph·ªëi d·ªØ li·ªáu g·∫ßn v·ªõi ph√¢n ph·ªëi chu·∫©n h∆°n.
- Box-Cox ch·ªâ √°p d·ª•ng cho d·ªØ li·ªáu d∆∞∆°ng, trong khi Yeo-Johnson √°p d·ª•ng cho c·∫£ d·ªØ li·ªáu √¢m.
- **∆Øu ƒëi·ªÉm**: TƒÉng hi·ªáu qu·∫£ c·ªßa m√¥ h√¨nh khi d·ªØ li·ªáu c√≥ s·ª± ph√¢n b·ªë kh√¥ng chu·∫©n.
- **Nh∆∞·ª£c ƒëi·ªÉm**: Ph·ª©c t·∫°p h∆°n so v·ªõi c√°c ph∆∞∆°ng ph√°p tr√™n.
- **Khi s·ª≠ d·ª•ng**: Khi d·ªØ li·ªáu b·ªã l·ªách nhi·ªÅu v√† kh√¥ng tu√¢n theo ph√¢n ph·ªëi chu·∫©n.

### 3. Khi n√†o n√™n ch·ªçn ph∆∞∆°ng ph√°p n√†o?

|T√¨nh hu·ªëng d·ªØ li·ªáu|Ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t|
|---|---|
|D·ªØ li·ªáu c√≥ ph·∫°m vi c·ªë ƒë·ªãnh|Min-Max Scaling|
|D·ªØ li·ªáu c√≥ ph√¢n ph·ªëi chu·∫©n ho·∫∑c g·∫ßn chu·∫©n|Standardization (Z-score)|
|D·ªØ li·ªáu c√≥ outliers|Robust Scaling|
|D·ªØ li·ªáu b·ªã skewed (l·ªách)|Log Transformation / Power Transformation|
|D·ªØ li·ªáu c√≥ c·∫£ gi√° tr·ªã d∆∞∆°ng v√† √¢m|Yeo-Johnson|

### 4. C√°ch th·ª±c hi·ªán trong Python

D∆∞·ªõi ƒë√¢y l√† c√°ch √°p d·ª•ng c√°c ph∆∞∆°ng ph√°p chu·∫©n h√≥a b·∫±ng `scikit-learn`:

```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer
import numpy as np

# D·ªØ li·ªáu m·∫´u
data = np.array([[1], [2], [3], [100], [200], [300]])

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
data_minmax = min_max_scaler.fit_transform(data)

# Standardization (Z-score)
standard_scaler = StandardScaler()
data_standard = standard_scaler.fit_transform(data)

# Robust Scaling
robust_scaler = RobustScaler()
data_robust = robust_scaler.fit_transform(data)

# Power Transformation
power_transformer = PowerTransformer(method='yeo-johnson')
data_power = power_transformer.fit_transform(data)

print("Min-Max Scaling:\n", data_minmax)
print("Standardization:\n", data_standard)
print("Robust Scaling:\n", data_robust)
print("Power Transformation:\n", data_power)
```

### 5. K·∫øt lu·∫≠n

Chu·∫©n h√≥a d·ªØ li·ªáu l√† m·ªôt b∆∞·ªõc quan tr·ªçng trong Machine Learning gi√∫p c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh. Vi·ªác l·ª±a ch·ªçn ph∆∞∆°ng ph√°p ph√π h·ª£p ph·ª• thu·ªôc v√†o ƒë·∫∑c ƒëi·ªÉm c·ªßa t·∫≠p d·ªØ li·ªáu, s·ª± ph√¢n b·ªë c·ªßa n√≥ v√† s·ª± t·ªìn t·∫°i c·ªßa ngo·∫°i l·ªá.

B·∫°n c√≥ mu·ªën m√¨nh tri·ªÉn khai m·ªôt v√≠ d·ª• th·ª±c t·∫ø v·ªõi d·ªØ li·ªáu c·ª• th·ªÉ kh√¥ng? üöÄ