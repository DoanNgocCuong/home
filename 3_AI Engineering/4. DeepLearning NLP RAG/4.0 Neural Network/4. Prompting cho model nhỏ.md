
Link: [IELTS Coach - Prompt order - Google Trang tính](https://docs.google.com/spreadsheets/d/18eOvC4w3PfDjMg2tX1TqgpvorljPUYgklF49HRpUvVI/edit?gid=703819217#gid=703819217)

|   |   |
|---|---|
|prompt các model nhỏ 1B, 3B, 7B ko bị trả lời quá nhiều và lặp thông tin (prompt khó)|prompt khó, JSON FORMAT??|
|from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  <br>import torch  <br>model_name = "meta-llama/Llama-3.2-3B-Instruct" # Tên mô hình, thay thế theo tên chính thức của mô hình  <br>  <br># Tải mô hình và tokenizer  <br>model = AutoModelForCausalLM.from_pretrained(model_name)  <br>tokenizer = AutoTokenizer.from_pretrained(model_name)  <br>qa_pipeline = pipeline(  <br>"text-generation",  <br>model=model_name,  <br>model_kwargs={"torch_dtype": torch.bfloat16},  <br>device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),  <br>)  <br>  <br>def answer_question(context, question):  <br># Apply the chat template for the context and question  <br>messages=[  <br>{"role": "user", "content": f"Answer the question based on the given passages. Only give me the answer and do not output any other words.\n\nThe following are given passages.\n{context}\n\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\n\nQuestion: {question}\nAnswer:"}  <br>]  <br>prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  <br>  <br># Generate the answer using the pipeline  <br>outputs = qa_pipeline(  <br>prompt,  <br>max_new_tokens=128,  <br>do_sample=True,  <br>temperature=0.1,  <br>top_k=50,  <br>top_p=0.95  <br>)  <br>  <br># Extracting and returning the generated answer  <br>answer = outputs[0]["generated_text"][len(prompt):]  <br>return answer||
|def answer_question(context, question):  <br># Apply the chat template for the context and question  <br>messages=[  <br>{"role": "user", "content": f"BASE ON CONTEXT: \n{context}\n\nAnswer the question. Only give me the answer and do not output any other words.\n\nQuestion: {question}\nAnswer:"}  <br>]  <br>prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  <br>  <br># Generate the answer using the pipeline  <br>outputs = qa_pipeline(  <br>prompt,  <br>max_new_tokens=128,  <br>do_sample=True,  <br>temperature=0.1,  <br>top_k=50,  <br>top_p=0.95  <br>)  <br>  <br># Extracting and returning the generated answer  <br>answer = outputs[0]["generated_text"][len(prompt):]  <br># print(f"context:{context}")  <br>return answer  <br>  <br># Generate the answer  <br>short_answer = answer_question(retrieval_final_context, question)  <br>print(f"Short Answer: {short_answer}")||