
## Cách 1: Xài thư viện transformer - load model


### 1.1 Use a pipeline as a high-level **helper** - basic

```python
# Use a pipeline as a high-level helper
from transformers import pipeline

messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe = pipeline("text-generation", model="Qwen/Qwen2-1.5B-Instruct")
pipe(messages)
```

Full

```python
# from transformers import pipeline

# # Initialize the pipeline globally for efficiency
# pipe = pipeline("text-generation", model="Qwen/Qwen2-1.5B-Instruct", device=0)
import json

def answer_question(final_context, query):
    """
    Generates an answer for the query using the Hugging Face pipeline.

    Args:
        final_context (str): The context to provide for the query.
        query (str): The user query.

    Returns:
        str: The extracted short answer.
    """
    # Construct the prompt
    prompt = (
        f"Answer the question based on the provided context.\n\n"
        f"Context:\n{final_context}\n\n"
        f"Question: {query}\n\n"
        f"Provide a short answer in less than 5 words, strictly in the following JSON template (not other include other character such as Context, Question):\n"
        f'{{"short_answer": "<short answer in less than 5 words>"}}'
    )
 
    # Generate response
    outputs = pipe(prompt, max_new_tokens=128, do_sample=False)
    response = outputs[0]["generated_text"].strip()
    

    return response

# Example context and query
final_context = (
    "Miller v. California, 413 U.S. 15 (1973), was a landmark decision redefining obscenity. "
    "The Gates v. Collier case brought an end to the 'trusty system'."
)
query = "Which case was brought to court first?"

# Generate the answer
short_answer = answer_question(final_context, query)
print(f"Short Answer: {short_answer}")

```


## 1.2 tokenizer and model 

```python
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-1.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-1.5B-Instruct")
```

Full system Prompt , user prompt

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import json

# Load model and tokenizer
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def answer_question(context, question):
    """
    Generates an answer for the query using the Hugging Face model in a conversational format.

    Args:
        context (str): The context to provide for the query.
        question (str): The user query.

    Returns:
        str: The extracted short answer.
    """
    # Define the message structure with system and user roles
    messages = [
        {"role": "system", "content": "You are a knowledgeable assistant. Always provide concise and accurate answers."},
        {"role": "user", "content": (
            f"Answer the question based on the given passages. Only give me the answer and do not output any other words.\n\n"
            f"The following are given passages:\n{context}\n\n"
            f"Answer the question based on the given passages. Only give me the answer and do not output any other words.\n\n"
            f"Question: {question}\nAnswer:"
        )}
    ]
    
    # Convert messages to a single string prompt
    prompt = "\n".join(f"{msg['role'].capitalize()}: {msg['content']}" for msg in messages)
    
    # Tokenize the prompt
    model_inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)
    
    # Generate the response
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=128,
        do_sample=True,
        temperature=0.1,
        top_k=50,
        top_p=0.95
    )
    
    # Decode the response
    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    
    # Try parsing as JSON to extract 'short_answer'
    try:
        response_json = json.loads(response)
        return response_json.get("short_answer", "No answer provided")
    except json.JSONDecodeError:
        # Fallback: Extract the likely answer from raw text
        print(f"Warning: Invalid JSON response received: {response}")
        if "short_answer" in response:
            start_idx = response.find('"short_answer":') + len('"short_answer":') + 1
            end_idx = response.find('"', start_idx)
            return response[start_idx:end_idx].strip()
        return "Invalid JSON response"

# Example context and query
context = (
    "Miller v. California, 413 U.S. 15 (1973), was a landmark decision redefining obscenity. "
    "The Gates v. Collier case brought an end to the 'trusty system'."
)
question = "Which case was brought to court first?"

# Generate the answer
short_answer = answer_question(context, question)
print(f"Short Answer: {short_answer}")


```

### Apply template chat thay cho:     # Convert messages to a single string prompt
    prompt = "\n".join(f"{msg['role'].capitalize()}: {msg['content']}" for msg in messages)
    
```


```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import json

# Load model and tokenizer
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def answer_question(final_context, query):
    """
    Generates an answer for the query using the Hugging Face model in a conversational format.

    Args:
        final_context (str): The context to provide for the query.
        query (str): The user query.

    Returns:
        str: The extracted short answer.
    """
    # Construct the conversational message
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": (
            f"Answer the question based on the provided context.\n\n"
            f"Context:\n{final_context}\n\n"
            f"Question: {query}\n\n"
            f"Provide a short answer in less than 5 words, strictly in the following JSON format:\n"
            f'{{"short_answer": "<short answer in less than 5 words>"}}'
        )}
    ]
    
    # Format the message into a chat template
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Prepare model inputs
    model_inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
    
    # Generate the response
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=128,
        do_sample=False
    )
    
    # Decode the response
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    # Try parsing as JSON to extract 'short_answer'
    try:
        response_json = json.loads(response)
        return response_json.get("short_answer", "No answer provided")
    except json.JSONDecodeError:
        # Fallback: Extract the likely answer from raw text
        print(f"Warning: Invalid JSON response received: {response}")
        if "short_answer" in response:
            start_idx = response.find('"short_answer":') + len('"short_answer":') + 1
            end_idx = response.find('"', start_idx)
            return response[start_idx:end_idx].strip()
        return "Invalid JSON response"

# Example context and query
final_context = (
    "Miller v. California, 413 U.S. 15 (1973), was a landmark decision redefining obscenity. "
    "The Gates v. Collier case brought an end to the 'trusty system'."
)
query = "Which case was brought to court first?"

# Generate the answer
short_answer = answer_question(final_context, query)
print(f"Short Answer: {short_answer}")


```

# 1.3 Combine pipe và Tokenizer

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
model_name = "meta-llama/Llama-3.2-3B-Instruct"  # Tên mô hình, thay thế theo tên chính thức của mô hình

# Tải mô hình và tokenizer
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
qa_pipeline = pipeline(
            "text-generation",
            model=model_name,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
        )

def answer_question(context, question):
        # Apply the chat template for the context and question
        messages=[
              {"role": "user", "content": f"Answer the question based on the given passages. Only give me the answer and do not output any other words.\n\nThe following are given passages.\n{context}\n\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\n\nQuestion: {question}\nAnswer:"}
        ]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        # Generate the answer using the pipeline
        outputs = qa_pipeline(
            prompt,
            max_new_tokens=128,
            do_sample=True,
            temperature=0.1,
            top_k=50,
            top_p=0.95
        )
        
        # Extracting and returning the generated answer
        answer = outputs[0]["generated_text"][len(prompt):]
        return answer
```
## Cách 2: API 

Intro: Inference API[](https://huggingface.co/docs/hub/models-widgets#example-outputs)
Infernce API Huggingface bị giới hạn bao nhiêu? 

```python
from huggingface_hub import InferenceClient

def answer_question(final_context, query, model="Qwen/Qwen2-1.5B-Instruct", max_tokens=500, api_key="your_api_key_here"):
    """
    Generates an answer for the query using the Hugging Face Inference API with the specified model.

    Args:
        final_context (str): The context to provide for the query.
        query (str): The user query.
        model (str): The model to use for chat completion.
        max_tokens (int): Maximum tokens for the output.
        api_key (str): Hugging Face API key.

    Returns:
        str: The model's response to the query.
    """
    client = InferenceClient(api_key=api_key)
    messages = [
        {
            "role": "user",
            "content": (
                f"Answer the question based on the provided context.\n\n"
                f"Context:\n{final_context}\n\n"
                f"Question: {query}\n\n"
                f"Provide a short answer in less than 5 words, in the following JSON format:\n"
                f'{{"short_answer": "<short answer in less than 5 words>"}}'
            )
        }
    ]

    # Generate completion
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens
    )

    # Extract and return the assistant's response
    return completion.choices[0].message['content']

# Example usage
final_context = """
Every morning, as the first rays of sunlight peeked through the window, my mother had already been awake for hours, preparing breakfast for the family. I still vividly remember the image of her, wearing an old sweater, her hands swiftly moving the pan, with steam rising and reddening her cheeks. The aroma of the peanut sticky rice she used to cook lingers in my memory; even now, a similar scent instantly brings me back to my childhood home. Mother never sat down to eat with us; she would gently ask, "Is it good?" and then quietly continue tidying up. I grew up under her care, and with every step I take farther from home, my heart aches whenever I think of her. She doesn’t talk much, but her eyes always radiate love. The wrinkles on her face are like lines of time, telling stories of all the sacrifices and hardships she endured for our family. To me, she is not only the one who gave me life but also the sky full of love and gratitude that I carry forever in my heart.
"""
query = "Can you summarize the significance of the mother's role in this passage?"
response = answer_question(final_context, query, api_key="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")
print(f"Model Response: {response}")


```




## Nhận xét nhanh 
- Với các mô hình nhỏ như: Llama 3 3B 7B, Qwen 2 1.5 B 
	- Với cách call API (cách 2): response JSON format rất ngon 
	- Với các cách 1 : prompt mãi. response bị lặp lại => Khi đưa JSON vào thì response bị trả ra nhiều thông tin chứ không chỉ ONLY JSON. 
	