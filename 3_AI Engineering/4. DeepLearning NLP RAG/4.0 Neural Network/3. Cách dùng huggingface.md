
## Cách 1: Xài thư viện transformer - load model


### 1.1 Use a pipeline as a high-level **helper** - basic

```python
# Use a pipeline as a high-level helper
from transformers import pipeline

messages = [
    {"role": "user", "content": "Who are you?"},
]
pipe = pipeline("text-generation", model="Qwen/Qwen2-1.5B-Instruct")
pipe(messages)
```

Full

```python
# from transformers import pipeline

# # Initialize the pipeline globally for efficiency
# pipe = pipeline("text-generation", model="Qwen/Qwen2-1.5B-Instruct", device=0)
import json

def answer_question(final_context, query):
    """
    Generates an answer for the query using the Hugging Face pipeline.

    Args:
        final_context (str): The context to provide for the query.
        query (str): The user query.

    Returns:
        str: The extracted short answer.
    """
    # Construct the prompt
    prompt = (
        f"Answer the question based on the provided context.\n\n"
        f"Context:\n{final_context}\n\n"
        f"Question: {query}\n\n"
        f"Provide a short answer in less than 5 words, strictly in the following JSON template (not other include other character such as Context, Question):\n"
        f'{{"short_answer": "<short answer in less than 5 words>"}}'
    )
 
    # Generate response
    outputs = pipe(prompt, max_new_tokens=128, do_sample=False)
    response = outputs[0]["generated_text"].strip()
    

    return response

# Example context and query
final_context = (
    "Miller v. California, 413 U.S. 15 (1973), was a landmark decision redefining obscenity. "
    "The Gates v. Collier case brought an end to the 'trusty system'."
)
query = "Which case was brought to court first?"

# Generate the answer
short_answer = answer_question(final_context, query)
print(f"Short Answer: {short_answer}")

```


## 1.2 tokenizer and model 

```python
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-1.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-1.5B-Instruct")
```

Full system Prompt , user prompt

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-1.5B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

```

### Apply template chat??


```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
model_name = "meta-llama/Llama-3.2-3B-Instruct"  # Tên mô hình, thay thế theo tên chính thức của mô hình

# Tải mô hình và tokenizer
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
qa_pipeline = pipeline(
            "text-generation",
            model=model_name,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
        )

def answer_question(context, question):
        # Apply the chat template for the context and question
        messages=[
              {"role": "user", "content": f"Answer the question based on the given passages. Only give me the answer and do not output any other words.\n\nThe following are given passages.\n{context}\n\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\n\nQuestion: {question}\nAnswer:"}
        ]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        # Generate the answer using the pipeline
        outputs = qa_pipeline(
            prompt,
            max_new_tokens=128,
            do_sample=True,
            temperature=0.1,
            top_k=50,
            top_p=0.95
        )
        
        # Extracting and returning the generated answer
        answer = outputs[0]["generated_text"][len(prompt):]
        return answer
```

## Cách 2: API

Intro: Inference API[](https://huggingface.co/docs/hub/models-widgets#example-outputs)
Infernce API Huggingface bị giới hạn bao nhiêu? 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-1.5B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

```

