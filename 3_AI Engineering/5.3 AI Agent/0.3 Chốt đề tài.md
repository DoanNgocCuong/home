### **ğŸ“„ Äá»“ Ã¡n nghiÃªn cá»©u: LONG TERM MEMORY, MEMORY-AUGMENTED AI AGENTS**

ğŸ“ **TÃ¡c giáº£:** (TÃªn cá»§a báº¡n)  
ğŸ« **ÄÆ¡n vá»‹ nghiÃªn cá»©u:** (TÃªn trÆ°á»ng Ä‘áº¡i há»c / viá»‡n nghiÃªn cá»©u)  
ğŸ“… **NgÃ y thá»±c hiá»‡n:** (NgÃ y báº¯t Ä‘áº§u nghiÃªn cá»©u)

---

## **ğŸ“Œ 1. Giá»›i thiá»‡u (Introduction)**

### **1.1. Äáº·t váº¥n Ä‘á»: 

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) nhÆ° GPT, BERT, LLaMA hay PaLM Ä‘Ã£ chá»©ng tá» nÄƒng lá»±c ná»•i báº­t trong viá»‡c hiá»ƒu vÃ  táº¡o ra ngÃ´n ngá»¯ tá»± nhiÃªn, Ä‘áº·c biá»‡t khi Ä‘Æ°á»£c triá»ƒn khai dÆ°á»›i dáº¡ng há»‡ thá»‘ng há»™i thoáº¡i hoáº·c trá»£ lÃ½ AI. Tuy nhiÃªn, trong mÃ´i trÆ°á»ng á»©ng dá»¥ng thá»±c táº¿, nháº¥t lÃ  táº¡i cÃ¡c doanh nghiá»‡p, nhá»¯ng mÃ´ hÃ¬nh nÃ y pháº£i Ä‘á»‘i máº·t vá»›i má»™t bÃ i toÃ¡n Ä‘Ã¡ng ká»ƒ liÃªn quan Ä‘áº¿n viá»‡c duy trÃ¬ ngá»¯ cáº£nh, cáº­p nháº­t dá»¯ liá»‡u Ä‘á»™ng vÃ  tÆ°Æ¡ng tÃ¡c lÃ¢u dÃ i cÃ¹ng ngÆ°á»i dÃ¹ng. Má»—i khi lÆ°á»£ng thÃ´ng tin cá»§a cÃ¡c cuá»™c há»™i thoáº¡i trá»Ÿ nÃªn Ä‘á»“ sá»™, viá»‡c Ä‘Æ°a táº¥t cáº£ ná»™i dung vÃ o pháº§n ngá»¯ cáº£nh (prompt) trá»Ÿ nÃªn báº¥t kháº£ thi do giá»›i háº¡n vá» máº·t ká»¹ thuáº­t cÅ©ng nhÆ° chi phÃ­ tÃ­nh toÃ¡n. BÃªn cáº¡nh Ä‘Ã³, dá»¯ liá»‡u táº¡i cÃ¡c doanh nghiá»‡p khÃ´ng tÄ©nh mÃ  luÃ´n Ä‘Æ°á»£c Ä‘iá»u chá»‰nh vÃ  bá»• sung, Ä‘Ã²i há»i má»™t cÆ¡ cháº¿ thÆ°á»ng xuyÃªn cáº­p nháº­t Ä‘á»ƒ trÃ¡nh tÃ¬nh tráº¡ng mÃ´ hÃ¬nh sá»­ dá»¥ng thÃ´ng tin cÅ© hoáº·c láº¡c háº­u.

Váº¥n Ä‘á» cá»§a nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ nÃ y cÃ ng trá»Ÿ nÃªn phá»©c táº¡p hÆ¡n khi ngÆ°á»i dÃ¹ng mong Ä‘á»£i kháº£ nÄƒng â€œnhá»›â€ cÃ¡c luá»“ng há»™i thoáº¡i kÃ©o dÃ i nhiá»u phiÃªn, tháº­m chÃ­ nhiá»u thÃ¡ng. Trong cÃ¡c tÃ¬nh huá»‘ng thá»±c táº¿, nhÆ° khi tÆ°Æ¡ng tÃ¡c vá»›i khÃ¡ch hÃ ng, viá»‡c chatbot quÃªn máº¥t nhá»¯ng trao Ä‘á»•i trÆ°á»›c Ä‘Ã³ khiáº¿n ngÆ°á»i dÃ¹ng pháº£i láº·p láº¡i thÃ´ng tin vÃ  gÃ¢y nÃªn sá»± khÃ³ chá»‹u khÃ´ng nhá». Náº¿u nhÆ° trÆ°á»›c Ä‘Ã¢y, cÃ¡c giáº£i phÃ¡p RAG (Retrieval-Augmented Generation) chá»§ yáº¿u táº­p trung vÃ o cÃ¡c táº­p dá»¯ liá»‡u tÄ©nh Ä‘á»ƒ giáº£m thiá»ƒu sai sÃ³t vÃ  trÃ¡nh tÃ¬nh tráº¡ng mÃ´ hÃ¬nh â€œbá»‹aâ€ thÃ´ng tin, thÃ¬ nay, nhu cáº§u quáº£n lÃ½ luá»“ng há»™i thoáº¡i Ä‘á»™ng vÃ  dá»¯ liá»‡u doanh nghiá»‡p liÃªn tá»¥c thay Ä‘á»•i láº¡i Ä‘Ã²i há»i má»™t cÆ¡ cháº¿ linh hoáº¡t hÆ¡n. RAG truyá»n thá»‘ng khÃ´ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ thÆ°á»ng xuyÃªn cÃ¡c cáº­p nháº­t, thÃªm bá»›t hay xÃ³a bá» ná»™i dung trong cÆ¡ sá»Ÿ dá»¯ liá»‡u, vÃ¬ Ä‘a sá»‘ táº­p trung vÃ o vÄƒn báº£n â€œtÄ©nhâ€ nhÆ° cáº©m nang, FAQ hoáº·c hÆ°á»›ng dáº«n.

KhÃ³ khÄƒn khÃ¡c náº£y sinh khi nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ nÃ y trá»Ÿ nÃªn quÃ¡ táº£i do pháº£i náº¡p toÃ n bá»™ lá»‹ch sá»­ cá»§a nhiá»u phiÃªn há»™i thoáº¡i, dáº«n Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n tÄƒng vá»t vÃ  Ä‘á»™ trá»… xá»­ lÃ½ cÅ©ng khÃ´ng cÃ²n Ä‘Ã¡p á»©ng Ä‘Æ°á»£c yÃªu cáº§u thá»±c táº¿. Tá»« gÃ³c Ä‘á»™ ngÆ°á»i dÃ¹ng, viá»‡c cÃ³ má»™t chatbot thÃ´ng minh vÃ  nhanh nháº¡y lÃ  Æ°u tiÃªn hÃ ng Ä‘áº§u, nhÆ°ng náº¿u há»‡ thá»‘ng máº¥t quÃ¡ nhiá»u thá»i gian Ä‘á»ƒ suy xÃ©t hoáº·c tráº£ lá»i khÃ´ng chÃ­nh xÃ¡c do rá»‘i loáº¡n thÃ´ng tin, tráº£i nghiá»‡m tÆ°Æ¡ng tÃ¡c sáº½ bá»‹ áº£nh hÆ°á»Ÿng náº·ng ná». Äá»‘i vá»›i doanh nghiá»‡p, lÆ°á»£ng há»™i thoáº¡i cÃ³ thá»ƒ lÃªn Ä‘áº¿n hÃ ng triá»‡u dÃ²ng, káº¿t há»£p vá»›i nhiá»u loáº¡i dá»¯ liá»‡u khÃ¡c nhÆ° há»“ sÆ¡ khÃ¡ch hÃ ng, thá»‘ng kÃª kinh doanh hay cÃ¡c bÃ¡o cÃ¡o ná»™i bá»™. Sá»± hiá»‡n diá»‡n cá»§a má»™t kiáº¿n trÃºc cÃ³ thá»ƒ trÃ­ch xuáº¥t pháº§n thÃ´ng tin cáº§n thiáº¿t Ä‘á»ƒ Ä‘Æ°a vÃ o ngá»¯ cáº£nh vÃ  bá» qua nhá»¯ng yáº¿u tá»‘ khÃ´ng cÃ²n há»¯u Ã­ch lÃ  Ä‘iá»u báº¯t buá»™c, nháº±m tá»‘i Æ°u cáº£ chi phÃ­ vÃ  kháº£ nÄƒng váº­n hÃ nh.

BÃ i toÃ¡n â€œlÃ£ng quÃªnâ€ hay â€œÆ°u tiÃªn thÃ´ng tinâ€ Ä‘áº·t ra yÃªu cáº§u Ä‘áº·c biá»‡t vá» cÃ¡ch tá»• chá»©c vÃ  gÃ¡n nhÃ£n dá»¯ liá»‡u. Náº¿u giá»¯ láº¡i táº¥t cáº£ thÃ¬ há»‡ thá»‘ng bá»‹ quÃ¡ táº£i, cÃ²n náº¿u xÃ³a bá»›t má»™t cÃ¡ch tÃ¹y tiá»‡n, mÃ´ hÃ¬nh cÃ³ thá»ƒ bá» lá»¡ nhá»¯ng chi tiáº¿t quan trá»ng vá»‘n dÄ© cáº§n thiáº¿t Ä‘á»ƒ suy luáº­n chÃ­nh xÃ¡c. HÆ¡n ná»¯a, má»—i khi cÃ³ xung Ä‘á»™t thÃ´ng tin hoáº·c thay Ä‘á»•i vá» dá»¯ liá»‡u, quÃ¡ trÃ¬nh cáº­p nháº­t sao cho mÃ´ hÃ¬nh khÃ´ng tráº£ lá»i dá»±a trÃªn nhá»¯ng gÃ¬ Ä‘Ã£ lá»—i thá»i láº¡i trá»Ÿ thÃ nh má»™t thÃ¡ch thá»©c. Tá»« Ä‘Ã³, cÃ¢u há»i trung tÃ¢m Ä‘Æ°á»£c Ä‘áº·t ra lÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ thiáº¿t káº¿ má»™t cÆ¡ cháº¿ â€œbá»™ nhá»› ngoÃ iâ€ cho mÃ´ hÃ¬nh ngÃ´n ngá»¯, cÃ³ kháº£ nÄƒng quáº£n lÃ½ luá»“ng há»™i thoáº¡i dÃ i háº¡n, tiáº¿p nháº­n vÃ  loáº¡i bá» thÃ´ng tin linh hoáº¡t, Ä‘á»“ng thá»i báº£o Ä‘áº£m tá»‘c Ä‘á»™ vÃ  cháº¥t lÆ°á»£ng tráº£ lá»i khÃ´ng suy giáº£m.

Táº¥t cáº£ cÃ¡c yáº¿u tá»‘ vá»«a Ä‘á» cáº­p nháº¥n máº¡nh nhu cáº§u nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn má»™t lá»›p â€œbá»™ nhá»› dÃ i háº¡nâ€ (long-term memory layer) cÃ³ thá»ƒ bá»• sung cho LLM, giÃºp lÆ°u trá»¯, truy xuáº¥t vÃ  cáº­p nháº­t thÃ´ng tin hiá»‡u quáº£ trong mÃ´i trÆ°á»ng há»™i thoáº¡i nhiá»u phiÃªn vÃ  dá»¯ liá»‡u doanh nghiá»‡p liÃªn tá»¥c phÃ¡t sinh. Giáº£i phÃ¡p lÃ½ tÆ°á»Ÿng cáº§n cho phÃ©p tÃ¡ch biá»‡t quÃ¡ trÃ¬nh lÆ°u trá»¯ vÃ  trÃ­ch xuáº¥t khá»i mÃ´ hÃ¬nh ngÃ´n ngá»¯, Ä‘á»“ng thá»i duy trÃ¬ tÃ­nh chÃ­nh xÃ¡c trong viá»‡c truy váº¥n nhá»¯ng Ä‘oáº¡n thÃ´ng tin quan trá»ng má»—i khi cáº§n dÃ¹ng Ä‘áº¿n. Má»¥c tiÃªu lÃ  nÃ¢ng cao cháº¥t lÆ°á»£ng há»™i thoáº¡i, giáº£m thiá»ƒu tÃ¬nh tráº¡ng láº·p láº¡i cÃ¢u há»i, háº¡n cháº¿ chi phÃ­ vá» token vÃ  thá»i gian, qua Ä‘Ã³ Ä‘Ã¡p á»©ng tá»‘t hÆ¡n yÃªu cáº§u triá»ƒn khai thá»±c táº¿ trong doanh nghiá»‡p.

### **1.2. CÃ¡c giáº£i phÃ¡p hiá»‡n táº¡i vÃ  háº¡n cháº¿
- .... PhÃ¢n Ä‘oáº¡n dá»±a trÃªn ...

**Trá»£ lÃ½ há»™i thoáº¡i cÃ¡ nhÃ¢n hÃ³a cÃ³ trÃ­ nhá»› (Memory-based Personalized Dialogue Agents)**

Sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c trá»£ lÃ½ há»™i thoáº¡i cÃ¡ nhÃ¢n hÃ³a cÃ³ trÃ­ nhá»› Ä‘Ã£ nÃ¢ng cao Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng tÆ°Æ¡ng tÃ¡c dÃ i háº¡n, báº±ng cÃ¡ch cho phÃ©p há»‡ thá»‘ng **lÆ°u giá»¯ vÃ  sá»­ dá»¥ng láº¡i thÃ´ng tin tá»« cÃ¡c cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³** (Bae et al., 2022).

Nhá»¯ng phÆ°Æ¡ng phÃ¡p ban Ä‘áº§u, cháº³ng háº¡n nhÆ° **CoMemNN** (Pei et al., 2021), giá»›i thiá»‡u cÃ¡c cÆ¡ cháº¿ Ä‘á»ƒ **tá»«ng bÆ°á»›c xÃ¢y dá»±ng há»“ sÆ¡ ngÆ°á»i dÃ¹ng** trong quÃ¡ trÃ¬nh Ä‘á»‘i thoáº¡i.

Tuy nhiÃªn, viá»‡c thu tháº­p dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n Ä‘á»§ lá»›n Ä‘á»ƒ huáº¥n luyá»‡n má»™t há»‡ thá»‘ng cÃ¡ nhÃ¢n hÃ³a lÃ¢u dÃ i lÃ  **ráº¥t khÃ³** (Tseng et al., 2024).

Gáº§n Ä‘Ã¢y, cÃ¡c nghiÃªn cá»©u táº­p trung vÃ o viá»‡c **káº¿t há»£p LLM vá»›i module bá»™ nhá»›**. VÃ­ dá»¥:

| Bae et al., "Keep Me Updated!"                     | 2022 | [https://aclanthology.org/2022.findings-emnlp.276](https://aclanthology.org/2022.findings-emnlp.276) |
| -------------------------------------------------- | ---- | ---------------------------------------------------------------------------------------------------- |
| Pei et al., "Cooperative Memory Network (CoMemNN)" | 2021 | [https://doi.org/10.1145/3442381.3449843](https://doi.org/10.1145/3442381.3449843)                   |

## ğŸ”— **Link cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c trÃ­ch dáº«n:**

| Paper                                              | NÄƒm  | Link                                                                                                 |
| -------------------------------------------------- | ---- | ---------------------------------------------------------------------------------------------------- |
| Bae et al., "Keep Me Updated!"                     | 2022 | [https://aclanthology.org/2022.findings-emnlp.276](https://aclanthology.org/2022.findings-emnlp.276) |
| Pei et al., "Cooperative Memory Network (CoMemNN)" | 2021 | [https://doi.org/10.1145/3442381.3449843](https://doi.org/10.1145/3442381.3449843)                   |
| Tseng et al., "Two Tales of Persona in LLMs"       | 2024 | [https://aclanthology.org/2024.findings-emnlp.969](https://aclanthology.org/2024.findings-emnlp.969) |
| Li et al., "LD-Agent"                              | 2024 | [https://arxiv.org/abs/2406.05925](https://arxiv.org/abs/2406.05925)                                 |
| Zhong et al., "MemoryBank"                         | 2024 | [https://doi.org/10.1609/aaai.v38i17.29946](https://doi.org/10.1609/aaai.v38i17.29946)               |
| Kim et al., "Theanine"                             | 2024 | [https://arxiv.org/abs/2406.10996](https://arxiv.org/abs/2406.10996)                                 |

---

Náº¿u Quá»‘c muá»‘n, mÃ¬nh cÃ³ thá»ƒ tá»•ng há»£p Ä‘oáº¡n nÃ y thÃ nh **má»™t pháº§n â€œRelated Workâ€ hoÃ n chá»‰nh cho research paper** hoáº·c váº½ sÆ¡ Ä‘á»“ so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p. Quá»‘c muá»‘n chá»n hÆ°á»›ng nÃ o?
### **1.3. Má»¥c tiÃªu vÃ  Ä‘á»‹nh hÆ°á»›ng giáº£i phÃ¡p

Äá»“ Ã¡n hÆ°á»›ng Ä‘áº¿n má»¥c tiÃªu sau:
1. TÃ¬m hiá»ƒu vÃ  nghiÃªn cá»©u cÃ¡c phÆ°Æ¡ng phÃ¡p, ká»¹ thuáº­t phÃ¢n Ä‘oáº¡n khÃ¡c nhau,
tá»« Ä‘Ã³ Ä‘Æ°a ra Ä‘Ã¡nh giÃ¡ vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p káº¿t há»£p Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cÃ¡c
phÆ°Æ¡ng phÃ¡p cÅ©ng nhÆ° háº¡n cháº¿, nhÆ°á»£c Ä‘iá»ƒm phÃ¹ há»£p vá»›i tÃ¬nh huá»‘ng Ä‘áº·t ra vá» tÃ i
nguyÃªn sá»­ dá»¥ng.
2. ÄÆ°a ra Ä‘á» xuáº¥t cáº£i tiáº¿n vÃ  hiá»‡u quáº£ cÃ³ kháº£ nÄƒng lÃ m nÃ¢ng cao hiá»‡u quáº£ trong
mÃ´ Ä‘un truy xuáº¥t.
3. CÃ i Ä‘áº·t há»‡ thá»‘ng vÃ  thá»­ nghiá»‡m vá»›i cÃ¡c ká»‹ch báº£n khÃ¡c nhau. NÃªu vÃ  phÃ¢n tÃ­ch
Ä‘Æ°á»£c nhá»¯ng káº¿t quáº£ thá»±c nghiá»‡m, so sÃ¡nh Ä‘Ã¡nh giÃ¡ giá»¯a cÃ¡c phÆ°Æ¡ng phÃ¡p vÃ  Ä‘Æ°a
ra cÃ¡c Æ°u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a cÃ¡c ká»¹ thuáº­t thá»­ nghiá»‡m.
TrÃªn cÆ¡ sá»Ÿ cÃ¡c phÃ¢n tÃ­ch vÃ  Ä‘Ã¡nh giÃ¡ á»Ÿ pháº§n 0.2 vÃ  Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng má»¥c tiÃªu
Ä‘Ã£ nÃªu phÃ­a trÃªn, Ä‘á»“ Ã¡n nÃ y sáº½ táº­p trung vÃ o cÃ¡c khÃ­a cáº¡nh:
4. Thá»±c hiá»‡n káº¿t há»£p phÆ°Æ¡ng phÃ¡p phÃ¢n Ä‘oáº¡n cá»• Ä‘iá»ƒn vá»›i phÆ°Æ¡ng phÃ¡p phÃ¢n
Ä‘oáº¡n sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng phÃ¢n Ä‘oáº¡n.
4
5. Thiáº¿t káº¿ luá»“ng truy xuáº¥t hiá»‡u quáº£ báº±ng cÃ¡ch káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p truy
xuáº¥t vÃ  sá»­ dá»¥ng thÃªm mÃ´-Ä‘un xáº¿p háº¡ng Ä‘á»ƒ cáº£i thiá»‡n thÃªm cháº¥t lÆ°á»£ng truy xuáº¥t.

Váº¥n Ä‘á» vá»›i phÆ°Æ¡ng phÃ¡p cÅ© (LongMemEval gá»‘c)

1. **Granularity chÆ°a tá»‘i Æ°u**:
    
    - Viá»‡c trÃ­ch xuáº¥t `summary`, `fact`, `keyphrase` tá»« **toÃ n bá»™ session** hoáº·c **round riÃªng láº»** cÃ³ thá»ƒ gáº·p tÃ¬nh tráº¡ng:
        - Äoáº¡n quÃ¡ **ngáº¯n** (khÃ´ng Ä‘á»§ ngá»¯ cáº£nh Ä‘á»ƒ LLM trÃ­ch xuáº¥t meaningful facts).
        - Äoáº¡n quÃ¡ **dÃ i** (gÃ¢y nhiá»…u thÃ´ng tin, LLM khÃ´ng thá»ƒ tÃ³m táº¯t chÃ­nh xÃ¡c, dá»… máº¥t detail).
    - KhÃ´ng cÃ³ cÃ¡ch kiá»ƒm soÃ¡t má»©c Ä‘á»™ coherence hoáº·c topic shift trong session dÃ i.
2. **Chá»‰ dÃ¹ng 1 loáº¡i key duy nháº¥t cho indexing**:
    
    - `K = V + fact` hoáº·c `K = V + summary` lÃ  tá»‘t, nhÆ°ng má»—i loáº¡i key cÃ³ Ä‘iá»ƒm máº¡nh khÃ¡c nhau:
        - `summary`: tá»‘t cho match semantic tá»•ng thá»ƒ.
        - `keyphrase`: báº¯t cá»¥ thá»ƒ keyword.
        - `fact`: truy xuáº¥t chÃ­nh xÃ¡c cÃ¡c entity, sá»‘ liá»‡u, má»‘c thá»i gian.
    - KhÃ´ng táº­n dá»¥ng Ä‘Æ°á»£c hiá»‡u á»©ng **ensemble giá»¯a cÃ¡c loáº¡i key**.
3. **Lacking structure in indexing**:
    
    - Indexing hiá»‡n táº¡i lÃ  **flat** â†’ khÃ´ng táº­n dá»¥ng tÃ­nh cháº¥t "táº§ng" cá»§a vÄƒn báº£n há»™i thoáº¡i: Ä‘oáº¡n â€“ session â€“ timeline.
    - Thiáº¿u kháº£ nÄƒng Ä‘iá»u hÆ°á»›ng mÆ°á»£t mÃ  giá»¯a cÃ¡c má»©c khÃ¡i quÃ¡t (coarse) vÃ  chi tiáº¿t (fine).

---

## âœ… Giáº£i phÃ¡p Ä‘á» xuáº¥t: Káº¿t há»£p **LLMs + Raptor + Multi-Key Embedding + Hierarchical Indexing**

### **1. Conversation-Aware Chunking trÆ°á»›c khi Extract**

#### âœ‚ï¸ 1.1. LLM-based Chunking

- DÃ¹ng LLM Ä‘á»ƒ phÃ¢n chia session thÃ nh cÃ¡c Ä‘oáº¡n nhá» (chunk) theo chuyá»ƒn chá»§ Ä‘á», má»¥c Ä‘Ã­ch cÃ¢u há»i, hoáº·c hÃ nh vi ngÆ°á»i dÃ¹ng.
- Lá»£i Ã­ch:
    - TÃ¡ch Ä‘Æ°á»£c cÃ¡c segment theo topic.
    - Giá»¯ Ä‘Æ°á»£c coherence bÃªn trong má»—i chunk.

#### ğŸ§± 1.2. Raptor Chunking

- DÃ¹ng **RAPTOR (recursive abstractive chunking)** Ä‘á»ƒ táº¡o cÃ¢y phÃ¢n cáº¥p cho tá»«ng session.
- Má»—i node lÃ  má»™t chunk hoáº·c summary cá»§a chunk con â†’ cÃ³ thá»ƒ phá»¥c vá»¥ **hierarchical retrieval**.


=> **Sau Ä‘Ã³ ta thu Ä‘Æ°á»£c: thay vÃ¬ K = Session + fact thÃ¬ cá»§a ta lÃ : K = Session1.i + Fact (Vá»›i i lÃ  Session 1.i Ä‘Æ°á»£c Chunking nhá» ra tá»« Session to ban Ä‘áº§u).** 

![[Pasted image 20250322071142.png]]
---

### **2. Embedding: Flatten & Index**

#### ğŸ§¾ 2.1 Raptor Flat Embedding

- ÄÆ°a tá»«ng chunk (ban Ä‘áº§u + LLM-chunked + summary chunk) vÃ o embedding encoder.
- Táº¡o index **dáº¡ng pháº³ng (flat)**, cÃ³ thá»ƒ dÃ¹ng Reranker Ä‘á»ƒ chá»n top-K chunk cÃ³ kháº£ nÄƒng cao nháº¥t.

#### ğŸ§  2.2 Hierarchical Indexing (2-phase Retrieval)

**Pha 1: Coarse Retrieval**

- Embed summary / keyphrase cá»§a chunk.
- DÃ¹ng query Ä‘á»ƒ so sÃ¡nh, chá»n Top-K chunk liÃªn quan.

**Pha 2: Fine Retrieval**

- Vá»›i má»—i chunk Ä‘Ã£ chá»n á»Ÿ coarse stage â†’ Ä‘i sÃ¢u vÃ o level fine:
    - Embed láº¡i cÃ¡c cÃ¢u gá»‘c / facts / sub-chunks.
    - Láº¥y top-Kâ€™ fine-grained memory units.

=> **Cuá»‘i cÃ¹ng Ä‘Æ°a vÃ o LLM Ä‘á»ƒ Ä‘á»c vÃ  tráº£ lá»i (Reading stage).**

---

### **3. Multi-Key Embedding cho Indexing**

- Vá»›i má»—i chunk â†’ táº¡o vÃ  embed song song:
    - `K1 = V + summary`
    - `K2 = V + fact`
    - `K3 = V + keyphrase`
- Káº¿t há»£p káº¿t quáº£ truy há»“i tá»« cÃ¡c luá»“ng (voting / weighted fusion / union-rerank).
- LÃ½ do:
    - Summary báº¯t ngá»¯ nghÄ©a chung.
    - Fact giÃºp reasoning logic.
    - Keyphrase giÃºp match keyword trong truy váº¥n cá»¥ thá»ƒ.

---

## ğŸ” Tá»•ng pipeline cáº£i tiáº¿n

```
Session
  â†“
Conversation-aware Chunking (LLMs Chunk + Raptor Chunk)
  â†“
Chunk-level Value â†’ Extract (summary, fact, keyphrase)
  â†“
Embed:
   - K1 = V + summary
   - K2 = V + fact
   - K3 = V + keyphrase
  â†“
Phase 1: Coarse Retrieval tá»« summary/keyphrase
  â†“
Phase 2: Fine Retrieval tá»« facts
  â†“
Reading Strategy: CoN + JSON (Chain-of-Note)
  â†“
Answer
```

---

## ğŸ“ˆ Lá»£i Ã­ch chÃ­nh

- âœ… Giáº£m nhiá»…u, tá»‘i Æ°u granularity nhá» LLM chunking.
- âœ… Táº­n dá»¥ng tá»‘t semantic, factual vÃ  keyword-level cues qua multi-key.
- âœ… TÄƒng kháº£ nÄƒng truy há»“i Ä‘Ãºng vá»›i hierarchical search.
- âœ… PhÃ¹ há»£p Ä‘á»ƒ scale cho 500+ sessions nhÆ° LongMemEvalM.

---

Náº¿u báº¡n cáº§n mÃ¬nh mÃ´ phá»ng pipeline nÃ y báº±ng sÆ¡ Ä‘á»“ (sequence hoáº·c flowchart), hoáº·c viáº¿t pháº§n giáº£i thÃ­ch thÃ nh vÄƒn academic Ä‘á»ƒ ghÃ©p vÃ o má»™t paper / research blog, mÃ¬nh cÅ©ng lÃ m Ä‘Æ°á»£c ngay nhÃ©!

---
DÆ°á»›i Ä‘Ã¢y lÃ  toÃ n bá»™ **ká»‹ch báº£n thá»­ nghiá»‡m** (experimental settings) tÆ°Æ¡ng á»©ng vá»›i tá»«ng Ã½ tÆ°á»Ÿng cáº£i tiáº¿n mÃ  Quá»‘c Ä‘á» xuáº¥t â€” Ä‘Æ°á»£c tá»• chá»©c theo dáº¡ng **ma tráº­n thÃ­ nghiá»‡m** Ä‘á»ƒ cÃ³ thá»ƒ dá»… dÃ ng triá»ƒn khai thá»±c nghiá»‡m, Ä‘Ã¡nh giÃ¡ tá»«ng thÃ nh pháº§n vÃ  káº¿t há»£p cá»§a pipeline.

---

## ğŸ¯ **Má»¤C TIÃŠU THá»¬ NGHIá»†M**

> Kiá»ƒm chá»©ng cÃ¡c cáº£i tiáº¿n vá» chunking, indexing, embedding, retrieval vÃ  reading strategy nháº±m cáº£i thiá»‡n hiá»‡u quáº£ cá»§a há»‡ thá»‘ng long-term memory QA (vÃ­ dá»¥ trÃªn benchmark nhÆ° LONGMEMEVAL).

---

## ğŸ§ª **Ká»ŠCH Báº¢N THá»¬ NGHIá»†M CHÃNH**

### ğŸ”¹ **I. Chunking Strategy**

|MÃ£|TÃªn phÆ°Æ¡ng phÃ¡p|MÃ´ táº£|
|---|---|---|
|C1|No Chunking (baseline)|DÃ¹ng cáº£ session hoáº·c round lÃ m value trá»±c tiáº¿p|
|C2|LLM-based Chunking|PhÃ¢n chia Ä‘oáº¡n theo chá»§ Ä‘á»/ngá»¯ nghÄ©a báº±ng LLM|
|C3|RAPTOR Chunking|Chunking dáº¡ng cÃ¢y phÃ¢n cáº¥p theo RAPTOR|
|C4|LLM + RAPTOR Hybrid|Chunk theo LLM â†’ dÃ¹ng RAPTOR Ä‘á»ƒ tÃ³m táº¯t tá»«ng chunk|

---

### ğŸ”¹ **II. Value Representation**

|MÃ£|Dáº¡ng value Ä‘áº§u vÃ o|MÃ´ táº£|
|---|---|---|
|V1|Full Session|KhÃ´ng chia nhá», Ä‘á»ƒ nguyÃªn session|
|V2|Round-based|Má»—i round lÃ  má»™t value|
|V3|Chunked|Chunk theo chiáº¿n lÆ°á»£c C2, C3, C4|
|V4|Summary|TÃ³m táº¯t cá»§a chunk hoáº·c session|
|V5|Fact|Fact trÃ­ch tá»« chunk/session|

---

### ğŸ”¹ **III. Key Design (Indexing)**

|MÃ£|TÃªn thiáº¿t káº¿ key|MÃ´ táº£|
|---|---|---|
|K1|K = V|DÃ¹ng raw value lÃ m key|
|K2|K = fact|Key lÃ  facts Ä‘Ã£ trÃ­ch|
|K3|K = summary|Key lÃ  summary|
|K4|K = V + fact|Ná»‘i fact vÃ o value Ä‘á»ƒ táº¡o key|
|K5|K = V + summary|Ná»‘i summary vÃ o value|
|K6|K = V + fact + summary + keyphrase|Multi-key (concat táº¥t cáº£)|
|K7|Multi-path index|Táº¡o nhiá»u loáº¡i key riÃªng biá»‡t, embed Ä‘á»™c láº­p|

---

### ğŸ”¹ **IV. Retrieval Strategy**

|MÃ£|PhÆ°Æ¡ng phÃ¡p truy há»“i|MÃ´ táº£|
|---|---|---|
|R1|Flat Retrieval|Retrieval Ä‘Æ¡n lá»›p, cosine / FAISS|
|R2|Coarse â†’ Fine Retrieval (2-phase)|Truy xuáº¥t 2 pha: summary â†’ fact|
|R3|Flat + Reranker|Retrieval sÆ¡ cáº¥p rá»“i rerank báº±ng LLM|
|R4|Multi-path Fusion|Truy há»“i theo tá»«ng key, rá»“i há»£p káº¿t quáº£ (voting / union)|

---

### ğŸ”¹ **V. Reading Strategy**

|MÃ£|Ká»¹ thuáº­t Ä‘á»c káº¿t quáº£|MÃ´ táº£|
|---|---|---|
|RS1|Direct Answer|ÄÆ°a chunk vÃ o, yÃªu cáº§u LLM tráº£ lá»i tháº³ng|
|RS2|Chain-of-Note (CoN)|TrÃ­ch info trÆ°á»›c rá»“i reasoning sau|
|RS3|JSON + CoN|ÄÆ°a input dáº¡ng JSON cÃ³ cáº¥u trÃºc, dÃ¹ng CoN|
|RS4|CoT + CoN|Káº¿t há»£p chain-of-thought reasoning vá»›i CoN|

---

## âœ… **Káº¾T Há»¢P THá»¬ NGHIá»†M Gá»¢I Ã (FULL COMBO)**

|#|Chunking|Value|Key|Retrieval|Reading|
|---|---|---|---|---|---|
|1|C1|V1|K1|R1|RS1|
|2|C2|V3|K4|R2|RS3|
|3|C3|V3|K6|R2|RS2|
|4|C4|V3|K7|R4|RS3|
|5|C3|V5|K2|R3|RS1|
|6|C2|V3|K5|R1|RS2|
|7|C4|V3|K6|R3|RS4|

> ğŸ’¡ _Báº¡n cÃ³ thá»ƒ chá»n 3-5 combo Ä‘á»ƒ thá»­ nghiá»‡m A/B, Ä‘o cÃ¡c metric nhÆ°:_
> 
> - **QA Accuracy (top-K)**
> - **Recall@K, NDCG@K**
> - **Inference time / latency**
> - **Token count (input to reader)**

---

## ğŸ“¦ **Dá»¯ liá»‡u sá»­ dá»¥ng**

- Dataset chÃ­nh: `LONGMEMEVAL-S` vÃ  `LONGMEMEVAL-M`
- CÃ³ thá»ƒ má»Ÿ rá»™ng thá»­ nghiá»‡m vá»›i `MemoryBank`, `PerLTQA` Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ generalize.

---

Náº¿u báº¡n cáº§n mÃ¬nh giÃºp viáº¿t script pipeline cháº¡y thá»­ nghiá»‡m hoáº·c dá»±ng YAML config cho tá»«ng scenario Ä‘á»ƒ quáº£n lÃ½ cháº¡y báº±ng `Hydra`/`WandB` hoáº·c há»‡ thá»‘ng ML pipeline khÃ¡c thÃ¬ cá»© nÃ³i nhÃ©. CÅ©ng cÃ³ thá»ƒ dá»±ng láº¡i báº£ng nÃ y dÆ°á»›i dáº¡ng Notion template hoáº·c Google Sheet Ä‘á»ƒ dá»… quáº£n lÃ½.


---
# SAU KHI HIá»‚U Gá»C, BÃ€I BÃO Gá»C, QUAY Láº I LANGGRAPH THÃŒ THáº¤Y á»’. RA LANGGRAPH NÃ“ Äá»€ XUáº¤T KIáº¾N TRÃšC THÃ”I, CÃ’N CÆ  Báº¢N THÃŒ LÃ€ BÃ€I BÃO ÄANG SEARCH NÃ€Y 

**LangGraph** lÃ  má»™t **framework** Ä‘Æ°á»£c giá»›i thiá»‡u trong khÃ³a há»c "Long-Term Agentic Memory with LangGraph" do Harrison Chase, Co-Founder vÃ  CEO cá»§a LangChain, giáº£ng dáº¡y. KhÃ³a há»c nÃ y hÆ°á»›ng dáº«n cÃ¡ch xÃ¢y dá»±ng má»™t **agent** vá»›i kháº£ nÄƒng **ghi nhá»› dÃ i háº¡n**, cá»¥ thá»ƒ lÃ  trong viá»‡c quáº£n lÃ½ email cÃ¡ nhÃ¢n.

**Äiá»ƒm má»›i mÃ  LangGraph Ä‘á» cáº­p Ä‘áº¿n**:

1. **TÃ­ch há»£p ba loáº¡i memory trong agent**:
    
    - **Semantic Memory**: LÆ°u trá»¯ cÃ¡c **facts** vá» ngÆ°á»i dÃ¹ng, nhÆ° sá»Ÿ thÃ­ch, thÃ³i quen, Ä‘á»ƒ sá»­ dá»¥ng trong cÃ¡c tÆ°Æ¡ng tÃ¡c sau nÃ y.
    - **Episodic Memory**: Ghi nhá»› cÃ¡c **tÃ¬nh huá»‘ng cá»¥ thá»ƒ** Ä‘Ã£ xáº£y ra trong quÃ¡ khá»©, giÃºp agent hiá»ƒu ngá»¯ cáº£nh vÃ  cáº£i thiá»‡n pháº£n há»“i.
    - **Procedural Memory**: LÆ°u trá»¯ cÃ¡c **hÆ°á»›ng dáº«n vÃ  quy trÃ¬nh** mÃ  agent cáº§n tuÃ¢n theo, giÃºp tá»‘i Æ°u hÃ³a hÃ nh vi dá»±a trÃªn pháº£n há»“i.


---
https://github.com/DoanNgocCuong/MiniProj_RAG3_RAG6_LegalChatbot_16032025

---
# Hiá»ƒu sÃ¢u hÆ¡n vá» Datase: 


LongMemEval lÃ  má»™t bá»™ dá»¯ liá»‡u toÃ n diá»‡n, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n. Bá»™ dá»¯ liá»‡u nÃ y bao gá»“m 500 cÃ¢u há»i cháº¥t lÆ°á»£ng cao, táº­p trung vÃ o nÄƒm kháº£ nÄƒng cá»‘t lÃµi:îˆ†

1. **TrÃ­ch xuáº¥t thÃ´ng tin (Information Extraction):** Kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin cá»¥ thá»ƒ tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c dÃ i, bao gá»“m cáº£ chi tiáº¿t do ngÆ°á»i dÃ¹ng hoáº·c trá»£ lÃ½ cung cáº¥p.îˆ†
    
2. **LÃ½ luáº­n Ä‘a phiÃªn (Multi-Session Reasoning):** Kháº£ nÄƒng tá»•ng há»£p thÃ´ng tin tá»« nhiá»u phiÃªn trÃ² chuyá»‡n Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i phá»©c táº¡p yÃªu cáº§u sá»± tá»•ng há»£p vÃ  so sÃ¡nh.îˆ†
    
3. **Cáº­p nháº­t kiáº¿n thá»©c (Knowledge Updates):** Kháº£ nÄƒng nháº­n biáº¿t vÃ  cáº­p nháº­t thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng theo thá»i gian.îˆ†
    
4. **LÃ½ luáº­n thá»i gian (Temporal Reasoning):** Nháº­n thá»©c vá» cÃ¡c khÃ­a cáº¡nh thá»i gian cá»§a thÃ´ng tin ngÆ°á»i dÃ¹ng, bao gá»“m cáº£ thá»i gian Ä‘Æ°á»£c Ä‘á» cáº­p rÃµ rÃ ng vÃ  siÃªu dá»¯ liá»‡u thá»i gian trong cÃ¡c tÆ°Æ¡ng tÃ¡c.îˆ†
    
5. **Tá»« chá»‘i tráº£ lá»i (Abstention):** Kháº£ nÄƒng tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
    

Láº¥y cáº£m há»©ng tá»« bÃ i kiá»ƒm tra "tÃ¬m kim trong Ä‘á»‘ng cá» khÃ´", LongMemEval sá»­ dá»¥ng má»™t quy trÃ¬nh kiá»ƒm soÃ¡t thuá»™c tÃ­nh Ä‘á»ƒ táº¡o ra lá»‹ch sá»­ trÃ² chuyá»‡n máº¡ch láº¡c, cÃ³ thá»ƒ má»Ÿ rá»™ng vÃ  Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u thá»i gian cho má»—i cÃ¢u há»i. Há»‡ thá»‘ng trÃ² chuyá»‡n cáº§n phÃ¢n tÃ­ch cÃ¡c tÆ°Æ¡ng tÃ¡c Ä‘á»™ng Ä‘á»ƒ ghi nhá»› vÃ  tráº£ lá»i cÃ¢u há»i sau khi táº¥t cáº£ cÃ¡c phiÃªn tÆ°Æ¡ng tÃ¡c Ä‘Ã£ diá»…n ra.îˆ†

**Cáº¥u trÃºc Bá»™ Dá»¯ Liá»‡u:**

Bá»™ dá»¯ liá»‡u bao gá»“m ba tá»‡p chÃ­nh:îˆ†

1. **longmemeval_s.json:** Má»—i lá»‹ch sá»­ trÃ² chuyá»‡n tiÃªu thá»¥ khoáº£ng 115.000 token (~40 phiÃªn lá»‹ch sá»­).îˆ†
    
2. **longmemeval_m.json:** Má»—i lá»‹ch sá»­ trÃ² chuyá»‡n chá»©a khoáº£ng 500 phiÃªn.îˆ†
    
3. **longmemeval_oracle.json:** Chá»‰ bao gá»“m cÃ¡c phiÃªn chá»©a báº±ng chá»©ng cáº§n thiáº¿t.îˆ†
    

Má»—i tá»‡p chá»©a 500 trÆ°á»ng há»£p Ä‘Ã¡nh giÃ¡, má»—i trÆ°á»ng há»£p bao gá»“m cÃ¡c trÆ°á»ng:îˆ†

- **question_id:** ID duy nháº¥t cho má»—i cÃ¢u há»i.îˆ†
    
- **question_type:** Loáº¡i cÃ¢u há»i, nhÆ° single-session-user, single-session-assistant, single-session-preference, temporal-reasoning, knowledge-update, vÃ  multi-session. Náº¿u question_id káº¿t thÃºc báº±ng _abs, Ä‘Ã³ lÃ  cÃ¢u há»i tá»« chá»‘i tráº£ lá»i.îˆ†
    
- **question:** Ná»™i dung cÃ¢u há»i.îˆ†
    
- **answer:** CÃ¢u tráº£ lá»i mong Ä‘á»£i tá»« mÃ´ hÃ¬nh.îˆ†
    
- **question_date:** NgÃ y cá»§a cÃ¢u há»i.îˆ†
    
- **haystack_session_ids:** Danh sÃ¡ch ID cá»§a cÃ¡c phiÃªn lá»‹ch sá»­ (sáº¯p xáº¿p theo thá»i gian).îˆ†
    
- **haystack_dates:** Danh sÃ¡ch cÃ¡c má»‘c thá»i gian cá»§a cÃ¡c phiÃªn lá»‹ch sá»­.îˆ†
    
- **haystack_sessions:** Danh sÃ¡ch ná»™i dung thá»±c táº¿ cá»§a cÃ¡c phiÃªn trÃ² chuyá»‡n giá»¯a ngÆ°á»i dÃ¹ng vÃ  trá»£ lÃ½. Má»—i phiÃªn lÃ  má»™t danh sÃ¡ch cÃ¡c lÆ°á»£t trao Ä‘á»•i, má»—i lÆ°á»£t cÃ³ Ä‘á»‹nh dáº¡ng {"role": user/assistant, "content": ná»™i dung tin nháº¯n}. Äá»‘i vá»›i cÃ¡c lÆ°á»£t chá»©a báº±ng chá»©ng cáº§n thiáº¿t, cÃ³ thÃªm trÆ°á»ng has_answer: true.îˆ†
    
- **answer_session_ids:** Danh sÃ¡ch ID cá»§a cÃ¡c phiÃªn chá»©a báº±ng chá»©ng, dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a viá»‡c nhá»› láº¡i á»Ÿ cáº¥p Ä‘á»™ phiÃªn.îˆ†
    

**Thiáº¿t láº­p MÃ´i TrÆ°á»ng:**

Äá»ƒ sá»­ dá»¥ng bá»™ dá»¯ liá»‡u, báº¡n cÃ³ thá»ƒ táº£i xuá»‘ng tá»« [Hugging Face](https://huggingface.co/datasets/xiaowu0162/longmemeval) vÃ  giáº£i nÃ©n vÃ o thÆ° má»¥c `data/`. Khuyáº¿n nghá»‹ sá»­ dá»¥ng mÃ´i trÆ°á»ng conda Ä‘á»ƒ cÃ i Ä‘áº·t cÃ¡c yÃªu cáº§u cáº§n thiáº¿t:îˆ†

```bash
conda create -n longmemeval python=3.9
conda activate longmemeval
pip install -r requirements-full.txt
```

îˆ†

**ÄÃ¡nh GiÃ¡ Há»‡ Thá»‘ng:**

Äá»ƒ kiá»ƒm tra há»‡ thá»‘ng cá»§a báº¡n trÃªn LongMemEval, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c táº­p lá»‡nh Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c cung cáº¥p. LÆ°u Ä‘áº§u ra cá»§a há»‡ thá»‘ng vÃ o tá»‡p JSONL vá»›i má»—i dÃ²ng chá»©a hai trÆ°á»ng: `question_id` vÃ  `hypothesis`. Sau Ä‘Ã³, cháº¡y táº­p lá»‡nh Ä‘Ã¡nh giÃ¡:îˆ†

```bash
export OPENAI_API_KEY=YOUR_API_KEY
cd src/evaluation
python3 evaluate_qa.py gpt-4o your_hypothesis_file ../../data/longmemeval_oracle.json
```

îˆ†

Táº­p lá»‡nh nÃ y sáº½ lÆ°u nháº­t kÃ½ Ä‘Ã¡nh giÃ¡ vÃ o tá»‡p `[your_hypothesis_file].log`. Báº¡n cÃ³ thá»ƒ tá»•ng há»£p cÃ¡c Ä‘iá»ƒm sá»‘ tá»« nháº­t kÃ½ báº±ng lá»‡nh:îˆ†

```bash
python3 print_qa_metrics.py gpt-4o your_hypothesis_file.log ../../data/longmemeval_oracle.json
```

îˆ†

**Táº¡o Lá»‹ch Sá»­ TrÃ² Chuyá»‡n TÃ¹y Chá»‰nh:**

LongMemEval há»— trá»£ biÃªn soáº¡n lá»‹ch sá»­ trÃ² chuyá»‡n vá»›i Ä‘á»™ dÃ i tÃ¹y Ã½ cho má»—i trÆ°á»ng há»£p cÃ¢u há»i, cho phÃ©p báº¡n dá»… dÃ ng tÄƒng Ä‘á»™ khÃ³. Äá»ƒ táº¡o lá»‹ch sá»­ tÃ¹y chá»‰nh, báº¡n cÃ³ thá»ƒ lÃ m theo Ä‘á»‹nh dáº¡ng trong `2_questions` vÃ  `6_session_cache` Ä‘á»ƒ táº¡o cÃ¢u há»i vÃ  cÃ¡c phiÃªn báº±ng chá»©ng, sau Ä‘Ã³ cháº¡y táº­p lá»‡nh `sample_haystack_and_timestamp.py` vá»›i cÃ¡c tham sá»‘ phÃ¹ há»£p.îˆ†

**Cháº¡y Thá»­ Nghiá»‡m Há»‡ Thá»‘ng Ghi Nhá»›:**

ChÃºng tÃ´i cung cáº¥p mÃ£ thá»­ nghiá»‡m cho viá»‡c truy xuáº¥t bá»™ nhá»› vÃ  táº¡o cÃ¢u tráº£ lá»i cÃ³ há»— trá»£ truy xuáº¥t dÆ°á»›i cÃ¡c thÆ° má»¥c `src/retrieval


---
Long-TermMemoryMethods Toequipchatassistantswithlong-termmemorycapabilities, three major techniques are commonly explored. The first approach involves directly adapting LLMs to process extensive history information as long-context inputs (Beltagy et al., 2020; Kitaev et al., 2020; Fu et al., 2024; An et al., 2024). While this method avoids the need for complex architectures, it is inefficient and susceptible to the â€œlost-in-the-middleâ€ phenomenon, where the ability of LLMs to utilize contextual information weakens as the input length grows (Shi et al., 2023; Liu et al., 2024). A second line of research integrates differentiable memory modules into language models, proposing specialized architectural designs and training strategies to enhance memory capabilities (Weston et al., 2014; Wu et al., 2022; Zhong et al., 2022; Wang et al., 2023). Lastly, several studies approach long-term memory from the perspective of context compression, developing techniques 3 Published as a conference paper at ICLR 2025 to condense lengthy histories into compact representations, whether in the form of LLM internal representations (Mu et al., 2023; Chevalier et al., 2023), discrete tokens (Jiang et al., 2023; Xu et al., 2024), or retrievable text segments via retrieval-augmented generation (RAG, Shi et al. (2024); Wang et al. (2023); Sarthi et al. (2024); Chen et al. (2023a); GutiÂ´ errez et al. (2024)). Although LONGMEMEVAL can evaluate any memory system, we will take an online context compression perspective, where each history interaction session is sequentially processed, stored, and accessed on-demand through indexing and retrieval mechanisms (Â§4). This formulation aligns with current literature (Zhong et al., 2024; GutiÂ´ errez et al., 2024) and commercial systems (OpenAI, 2024; Coze, 2024). Its plug-and-play nature also facilitates the integration into existing chat assistant systems


DÆ°á»›i Ä‘Ã¢y lÃ  báº£n dá»‹ch tiáº¿ng Viá»‡t Ä‘oáº¡n vÄƒn báº¡n cung cáº¥p:

---

### **CÃ¡c phÆ°Æ¡ng phÃ¡p trÃ­ nhá»› dÃ i háº¡n (Long-Term Memory Methods)**

Äá»ƒ trang bá»‹ kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cho cÃ¡c trá»£ lÃ½ há»™i thoáº¡i, hiá»‡n cÃ³ ba ká»¹ thuáº­t chÃ­nh thÆ°á»ng Ä‘Æ°á»£c nghiÃªn cá»©u:

1. **PhÆ°Æ¡ng phÃ¡p thá»© nháº¥t** lÃ  Ä‘iá»u chá»‰nh trá»±c tiáº¿p cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) Ä‘á»ƒ xá»­ lÃ½ lÆ°á»£ng lá»›n thÃ´ng tin lá»‹ch sá»­ dÆ°á»›i dáº¡ng Ä‘áº§u vÃ o dÃ i (long-context input)  
    _(Beltagy et al., 2020; Kitaev et al., 2020; Fu et al., 2024; An et al., 2024)_.  
    PhÆ°Æ¡ng phÃ¡p nÃ y giÃºp trÃ¡nh viá»‡c pháº£i thiáº¿t káº¿ kiáº¿n trÃºc phá»©c táº¡p,  
    tuy nhiÃªn láº¡i **kÃ©m hiá»‡u quáº£** vÃ  dá»… gáº·p hiá»‡n tÆ°á»£ng **"máº¥t thÃ´ng tin á»Ÿ giá»¯a" (lost-in-the-middle)** â€“  
    khi mÃ  kháº£ nÄƒng cá»§a LLM trong viá»‡c táº­n dá»¥ng thÃ´ng tin ngá»¯ cáº£nh suy giáº£m theo Ä‘á»™ dÃ i Ä‘áº§u vÃ o tÄƒng lÃªn _(Shi et al., 2023; Liu et al., 2024)_.
    
2. **HÆ°á»›ng nghiÃªn cá»©u thá»© hai** lÃ  tÃ­ch há»£p cÃ¡c **module bá»™ nhá»› phÃ¢n biá»‡t Ä‘Æ°á»£c (differentiable memory modules)** vÃ o trong mÃ´ hÃ¬nh ngÃ´n ngá»¯.  
    CÃ¡c nghiÃªn cá»©u nÃ y Ä‘á» xuáº¥t cÃ¡c thiáº¿t káº¿ kiáº¿n trÃºc chuyÃªn biá»‡t vÃ  chiáº¿n lÆ°á»£c huáº¥n luyá»‡n nháº±m tÄƒng cÆ°á»ng kháº£ nÄƒng ghi nhá»› cá»§a mÃ´ hÃ¬nh  
    _(Weston et al., 2014; Wu et al., 2022; Zhong et al., 2022; Wang et al., 2023)_.
    
3. **Cuá»‘i cÃ¹ng**, nhiá»u nghiÃªn cá»©u tiáº¿p cáº­n trÃ­ nhá»› dÃ i háº¡n tá»« gÃ³c Ä‘á»™ **nÃ©n ngá»¯ cáº£nh (context compression)**, phÃ¡t triá»ƒn cÃ¡c ká»¹ thuáº­t nháº±m **tinh gá»n lá»‹ch sá»­ há»™i thoáº¡i dÃ i** thÃ nh cÃ¡c biá»ƒu diá»…n nhá» gá»n hÆ¡n â€“  
    cÃ³ thá»ƒ dÆ°á»›i dáº¡ng biá»ƒu diá»…n ná»™i táº¡i trong LLM _(Mu et al., 2023; Chevalier et al., 2023)_,  
    cÃ¡c token rá»i ráº¡c _(Jiang et al., 2023; Xu et al., 2024)_,  
    hoáº·c cÃ¡c Ä‘oáº¡n vÄƒn báº£n cÃ³ thá»ƒ truy xuáº¥t Ä‘Æ°á»£c thÃ´ng qua ká»¹ thuáº­t sinh cÃ³ há»— trá»£ truy há»“i (Retrieval-Augmented Generation - RAG)  
    _(Shi et al., 2024; Wang et al., 2023; Sarthi et al., 2024; Chen et al., 2023a; GutiÃ©rrez et al., 2024)_.
    

Máº·c dÃ¹ **LONGMEMEVAL** cÃ³ thá»ƒ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ báº¥t ká»³ há»‡ thá»‘ng trÃ­ nhá»› nÃ o,  
trong bÃ i nÃ y chÃºng tÃ´i chá»n cÃ¡ch tiáº¿p cáº­n theo hÆ°á»›ng **nÃ©n ngá»¯ cáº£nh trá»±c tuyáº¿n (online context compression)**,  
nÆ¡i mÃ  má»—i phiÃªn tÆ°Æ¡ng tÃ¡c trong lá»‹ch sá»­ sáº½ Ä‘Æ°á»£c **xá»­ lÃ½ tuáº§n tá»±, lÆ°u trá»¯ vÃ  truy xuáº¥t theo yÃªu cáº§u** thÃ´ng qua cÃ¡c cÆ¡ cháº¿ Ä‘Ã¡nh chá»‰ má»¥c (indexing) vÃ  truy há»“i (retrieval) (Â§4).

CÃ¡ch tiáº¿p cáº­n nÃ y phÃ¹ há»£p vá»›i cÃ¡c cÃ´ng trÃ¬nh hiá»‡n táº¡i _(Zhong et al., 2024; GutiÃ©rrez et al., 2024)_  
cÅ©ng nhÆ° cÃ¡c há»‡ thá»‘ng thÆ°Æ¡ng máº¡i nhÆ° **OpenAI (2024)** vÃ  **Coze (2024)**.  
Äáº·c biá»‡t, nhá» vÃ o tÃ­nh **"plug-and-play"** (cáº¯m vÃ o lÃ  cháº¡y), phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ dá»… dÃ ng tÃ­ch há»£p vÃ o cÃ¡c há»‡ thá»‘ng trá»£ lÃ½ há»™i thoáº¡i hiá»‡n cÃ³.

---

Náº¿u báº¡n muá»‘n mÃ¬nh tÃ³m láº¡i thÃ nh báº£ng so sÃ¡nh 3 hÆ°á»›ng tiáº¿p cáº­n hoáº·c biá»ƒu Ä‘á»“ sÆ¡ Ä‘á»“ hÃ³a thÃ¬ mÃ¬nh cÃ³ thá»ƒ váº½ liá»n nhÃ©!

## 1.4 ÄÃ³ng gÃ³p cá»§a Ä‘á»“ Ã¡n
Äá»“ Ã¡n nÃ y cÃ³ 2 Ä‘Ã³ng gÃ³p chÃ­nh nhÆ° sau:
1. Äá»“ Ã¡n Ä‘á» xuáº¥t giáº£i phÃ¡p káº¿t há»£p cÃ¡c ká»¹ thuáº­t phÃ¢n Ä‘oáº¡n khÃ¡c nhau nháº±m tÄƒng
hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng truy xuáº¥t thÃ´ng tin.
2. Thá»±c hiá»‡n thá»­ nghiá»‡m káº¿t há»£p cÃ¡c ká»¹ thuáº­t truy xuáº¥t nháº±m cáº£i thiá»‡n káº¿t quáº£
Ä‘áº§u ra.
## 1.5 Bá»‘ cá»¥c Ä‘á»“ Ã¡n
ToÃ n bá»™ bÃ¡o cÃ¡o Ä‘á»“ Ã¡n tá»‘t nghiá»‡p Ä‘Æ°á»£c triá»ƒn khai trong 5 chÆ°Æ¡ng. CÃ¡c chÆ°Æ¡ng
cÃ²n láº¡i cá»§a bÃ¡o cÃ¡o cÃ³ ná»™i dung nhÆ° sau.
ChÆ°Æ¡ng 2 Ä‘á» cáº­p Ä‘áº¿n cÃ¡c ná»™i dung lÃ½ thuyáº¿t nháº±m phá»¥c vá»¥ viá»‡c nghiÃªn cá»©u, xÃ¢y
dá»±ng thá»­ nghiá»‡m vÃ  Ä‘Ã¡nh giÃ¡ giáº£i phÃ¡p Ä‘á» xuáº¥t. Trong chÆ°Æ¡ng nÃ y, tÃ´i sáº½ trÃ¬nh bÃ y
tá»•ng quan vá» mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n, cÃ¡c á»©ng dá»¥ng, háº¡n cháº¿ vÃ  má»™t sá»‘ dÃ²ng mÃ´
hÃ¬nh ngÃ´n ngá»¯ lá»›n phá»• biáº¿n. Ká»¹ thuáº­t RAG vá»›i cÃ¡c thÃ nh pháº§n vÃ  cÃ¡c giáº£i phÃ¡p
hiá»‡n cÃ³ cÅ©ng sáº½ Ä‘Æ°á»£c phÃ¢n tÃ­ch chi tiáº¿t á»Ÿ chÆ°Æ¡ng nÃ y.
ChÆ°Æ¡ng 3 trÃ¬nh bÃ y chi tiáº¿t vá» giáº£i phÃ¡p Ä‘á» xuáº¥t. TrÆ°á»›c háº¿t, tÃ´i mÃ´ táº£ tá»•ng quan
vá» luá»“ng xá»­ lÃ½, sau Ä‘Ã³ lÃ  Ä‘i sÃ¢u vÃ o tá»«ng mÃ´-Ä‘un. Trong mÃ´-Ä‘un phÃ¢n Ä‘oáº¡n, tÃ´i
trÃ¬nh bÃ y hai ká»¹ thuáº­t phÃ¢n Ä‘oáº¡n tÃ´i láº¥y lÃ m Ã½ tÆ°á»Ÿng Ä‘Ã³ lÃ  phÃ¢n Ä‘oáº¡n sá»­ dá»¥ng mÃ´
hÃ¬nh ngÃ´n ngá»¯ lá»›n vÃ  RAPTOR. Sau Ä‘Ã³, tÃ´i Ä‘á» xuáº¥t viá»‡c káº¿t há»£p hai ká»¹ thuáº­t nÃ y
Ä‘á»ƒ bá»• trá»£ cho nhau. Trong mÃ´-Ä‘un truy xuáº¥t, tÃ´i trÃ¬nh bÃ y viá»‡c káº¿t há»£p hai ká»¹ thuáº­t
Ä‘Ã³ lÃ : i) tÃ¬m kiáº¿m má»©c ngá»¯ nghÄ©a vÃ  ii) tÃ¬m kiáº¿m má»©c tá»« vá»±ng nháº±m cáº£i thiá»‡n má»©c
Ä‘á»™ phÃ¹ há»£p cá»§a cÃ¡c tÃ i liá»‡u tÃ¬m kiáº¿m Ä‘Æ°á»£c.
ChÆ°Æ¡ng 4 trÃ¬nh bÃ y cá»¥ thá»ƒ vá» cÃ¡c ká»‹ch báº£n thá»­ nghiá»‡m, thÃ´ng sá»‘ cáº¥u hÃ¬nh thá»­
nghiá»‡m, káº¿t quáº£ thá»±c nghiá»‡m vÃ  cÃ¡c Ä‘Ã¡nh giÃ¡, nháº­n xÃ©t vá» cÃ¡c phÆ°Æ¡ng phÃ¡p thá»­
nghiá»‡m. Trong chÆ°Æ¡ng nÃ y, tÃ´i sá»­ dá»¥ng má»™t sá»‘ Ä‘á»™ Ä‘o tá»± Ä‘á»™ng thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng
cho há»i Ä‘Ã¡p vÃ  Ä‘Ã¡nh giÃ¡ báº±ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. Nhá»¯ng nháº­n xÃ©t vÃ  Ä‘Ã¡nh giÃ¡
hiá»‡u nÄƒng cá»§a phÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tham chiáº¿u cÅ©ng Ä‘Æ°á»£c
trÃ¬nh bÃ y táº¡i chÆ°Æ¡ng nÃ y.
ChÆ°Æ¡ng 5 lÃ  chÆ°Æ¡ng cuá»‘i cÃ¹ng. Trong chÆ°Æ¡ng nÃ y, tÃ´i nÃªu ra káº¿t luáº­n vá» phÆ°Æ¡ng
phÃ¡p Ä‘á» xuáº¥t, nhá»¯ng Æ°u Ä‘iá»ƒm cÅ©ng nhÆ° nhá»¯ng háº¡n cháº¿ cÃ²n tá»“n táº¡i cÅ©ng nhÆ° Ä‘á» ra
cÃ¡c hÆ°á»›ng phÃ¡t triá»ƒn trong tÆ°Æ¡ng lai.

---

## **ğŸ“Œ 2. Tá»•ng quan nghiÃªn cá»©u (Related Work)**

### **2.1. Háº¡n cháº¿ cá»§a LLMs vá» trÃ­ nhá»›**

- LLMs hiá»‡n nay **chá»‰ cÃ³ trÃ­ nhá»› ngáº¯n háº¡n**, bá»‹ giá»›i háº¡n bá»Ÿi context window (128K tokens vá»›i GPT-4-turbo, 1M tokens vá»›i Claude 3). - 2M ráº¥t to
- CÃ¡c mÃ´ hÃ¬nh khÃ´ng thá»ƒ duy trÃ¬ bá»‘i cáº£nh há»™i thoáº¡i **qua nhiá»u phiÃªn lÃ m viá»‡c**.

### **2.2. CÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i**

#### **(1) LLMs lÆ°u trá»¯ ngáº¯n háº¡n



#### **(2) Retrieval-Augmented Generation (RAG)**

- **Æ¯u Ä‘iá»ƒm**: LLM cÃ³ thá»ƒ truy xuáº¥t dá»¯ liá»‡u tá»« nguá»“n ngoÃ i khi cáº§n.
- **NhÆ°á»£c Ä‘iá»ƒm**: KhÃ´ng nhá»› thÃ´ng tin theo thá»i gian, chá»‰ hoáº¡t Ä‘á»™ng khi cÃ³ truy váº¥n tÃ¬m kiáº¿m.

#### **(3) CÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y**

- OpenAI Ä‘ang phÃ¡t triá»ƒn **tÃ¡c nhÃ¢n cÃ³ trÃ­ nhá»›** nhÆ°ng chÆ°a cÃ´ng bá»‘ chi tiáº¿t.
- Meta AI thá»­ nghiá»‡m chatbot cÃ³ kháº£ nÄƒng **nhá»› sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng** nhÆ°ng gáº·p thÃ¡ch thá»©c vá» quyá»n riÃªng tÆ°.
![[Pasted image 20250322054143.png]]

ğŸ“Œ **Äiá»ƒm khÃ¡c biá»‡t cá»§a nghiÃªn cá»©u nÃ y:**  
âœ… Äá» xuáº¥t mÃ´ hÃ¬nh **Memory-Augmented AI** tá»‘i Æ°u hÆ¡n, cÃ³ thá»ƒ **há»c há»i theo thá»i gian mÃ  khÃ´ng bá»‹ quÃ¡ táº£i dá»¯ liá»‡u**.  
âœ… Káº¿t há»£p giá»¯a **Memory-Augmented Learning & RAG** Ä‘á»ƒ tá»‘i Æ°u hÃ³a bá»™ nhá»›.

---

## **ğŸ“Œ 3. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u (Methodology)**

### **3.1. Kiáº¿n trÃºc Ä‘á» xuáº¥t**

MÃ´ hÃ¬nh **Memory-Augmented AI Agent** gá»“m cÃ¡c thÃ nh pháº§n chÃ­nh:  
1ï¸âƒ£ **Short-Term Memory (STM)**: LÆ°u trá»¯ thÃ´ng tin trong pháº¡m vi cá»­a sá»• ngá»¯ cáº£nh hiá»‡n táº¡i.  
2ï¸âƒ£ **Long-Term Memory (LTM)**: LÆ°u trá»¯ thÃ´ng tin quan trá»ng vÃ o **Vector Database**.  
3ï¸âƒ£ **Memory Management Algorithm**: Quyáº¿t Ä‘á»‹nh **nÃªn nhá»› gÃ¬, quÃªn gÃ¬**.  (lÆ°u táº¥t thÃ¬ bá»‹ phÃ¬ng bá»™ nhá»›? )
-bá»:  TrÃ­ nhá»› vá» sá»Ÿ thÃ­ch 
- bá»: TrÃ­ nhá»› vá» cÃ¡c sá»± kiá»‡n Ä‘Ã£ qua 
- TrÃ­ nhá»› vá» cÃ¡c lá»‹ch sáº¯p tá»›i
- 
4ï¸âƒ£ **Knowledge Update Mechanism**: Cáº­p nháº­t vÃ  quÃªn thÃ´ng tin cÅ© khi cáº§n.
- Cáº­p nháº­t dá»±a trÃªn thá»i gian (User ngÃ y xÆ°a thÃ­ch chÆ¡i Ä‘Ã¡ bÃ³ng.Gáº«y chÃ¢n => Hiá»‡n táº¡i thÃ¬ khÃ´ng). 

- 
ğŸ“Œ **MÃ´ hÃ¬nh sá»­ dá»¥ng cÃ¡c cÃ´ng nghá»‡:**

- **LLM (GPT-4, Claude 3, Llama 2)**.
- **Vector Database (FAISS, Pinecone, Weaviate)** Ä‘á»ƒ lÆ°u trÃ­ nhá»› dÃ i háº¡n.
- **LangChain / LlamaIndex** Ä‘á»ƒ quáº£n lÃ½ truy xuáº¥t thÃ´ng tin.

---

## **ğŸ“Œ 4. Thá»±c nghiá»‡m & Káº¿t quáº£ (Experiments & Results)**

### **4.1. Thiáº¿t láº­p thá»­ nghiá»‡m**

**BÃ i toÃ¡n:** So sÃ¡nh hiá»‡u suáº¥t giá»¯a **Memory-Augmented AI Agent** vÃ  **LLM thÃ´ng thÆ°á»ng** trong há»™i thoáº¡i dÃ i háº¡n.

ğŸ”¹ **Dá»¯ liá»‡u thá»­ nghiá»‡m:**

- **Táº­p há»™i thoáº¡i thá»±c táº¿** (chÄƒm sÃ³c khÃ¡ch hÃ ng, trá»£ lÃ½ áº£o).
- **Táº­p há»™i thoáº¡i tá»•ng há»£p** (há»™i thoáº¡i kÃ©o dÃ i > 10,000 tokens).
## 4. Thá»±c nghiá»‡m vÃ  Ä‘Ã¡nh giÃ¡

### 4.1 Deep Memory Retrieval (DMR)

- **DMR** (giá»›i thiá»‡u trong MemGPT) cÃ³ 500 cuá»™c há»™i thoáº¡i nhiá»u phiÃªn (multi-session).
- Zep Ä‘áº¡t **94.8%** Ä‘á»™ chÃ­nh xÃ¡c khi dÃ¹ng GPT-4-turbo (vÃ  98.2% khi dÃ¹ng má»™t biáº¿n thá»ƒ GPT-4o-mini), nhá»‰nh hÆ¡n so vá»›i MemGPT (93.4%).
- Tuy nhiÃªn, bá»™ DMR chá»‰ cÃ³ há»™i thoáº¡i khÃ¡ ngáº¯n (khoáº£ng 60 tin nháº¯n má»—i cuá»™c), chÆ°a thá»±c sá»± kiá»ƒm tra kháº£ nÄƒng â€œsiÃªu dÃ i háº¡nâ€.

### 4.2 LongMemEval (LME)

- **LongMemEval** cÃ³ cÃ¡c Ä‘oáº¡n há»™i thoáº¡i dÃ i hÆ¡n nhiá»u (trung bÃ¬nh 115.000 tokens), mÃ´ phá»ng tÃ¬nh huá»‘ng doanh nghiá»‡p thá»±c táº¿ phá»©c táº¡p.

CÃ¡c há»‡ thá»‘ng trá»£ lÃ½ trÃ² chuyá»‡n ngÃ´n ngá»¯ lá»›n gáº§n Ä‘Ã¢y (LLM) cÃ³ cÃ¡c thÃ nh pháº§n bá»™ nhá»› tÃ­ch há»£p Ä‘á»ƒ theo dÃµi lá»‹ch sá»­ trÃ² chuyá»‡n cÃ³ sá»± há»— trá»£ cá»§a ngÆ°á»i dÃ¹ng, cho phÃ©p cÃ¡c pháº£n há»“i chÃ­nh xÃ¡c vÃ  cÃ¡ nhÃ¢n hÃ³a hÆ¡n. Tuy nhiÃªn, kháº£ nÄƒng bá»™ nhá»› dÃ i háº¡n cá»§a há» trong cÃ¡c tÆ°Æ¡ng tÃ¡c bá»n vá»¯ng váº«n chÆ°a Ä‘Æ°á»£c khai thÃ¡c. BÃ i viáº¿t nÃ y giá»›i thiá»‡u Longmemeval, má»™t Ä‘iá»ƒm chuáº©n toÃ n diá»‡n Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ nÄƒm kháº£ nÄƒng bá»™ nhá»› dÃ i háº¡n cá»‘t lÃµi cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n: trÃ­ch xuáº¥t thÃ´ng tin, lÃ½ luáº­n Ä‘a phiÃªn, lÃ½ luáº­n thá»i gian, cáº­p nháº­t kiáº¿n â€‹â€‹thá»©c vÃ  kiÃªng khem. Vá»›i 500 cÃ¢u há»i Ä‘Æ°á»£c quáº£n lÃ½ tá»‰ má»‰ Ä‘Æ°á»£c nhÃºng trong lá»‹ch sá»­ trÃ² chuyá»‡n há»— trá»£ ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ má»Ÿ rá»™ng, Longmemeval Ä‘Æ°a ra má»™t thÃ¡ch thá»©c Ä‘Ã¡ng ká»ƒ Ä‘á»‘i vá»›i cÃ¡c há»‡ thá»‘ng bá»™ nhá»› dÃ i háº¡n hiá»‡n cÃ³, vá»›i cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n thÆ°Æ¡ng máº¡i vÃ  LLM bá»‘i cáº£nh dÃ i cho tháº¥y Ä‘á»™ chÃ­nh xÃ¡c giáº£m 30% khi ghi nhá»› thÃ´ng tin qua cÃ¡c tÆ°Æ¡ng tÃ¡c Ä‘Æ°á»£c duy trÃ¬. Sau Ä‘Ã³, chÃºng tÃ´i trÃ¬nh bÃ y má»™t khung thá»‘ng nháº¥t phÃ¢n chia thiáº¿t káº¿ bá»™ nhá»› dÃ i háº¡n thÃ nh bá»‘n lá»±a chá»n thiáº¿t káº¿ trÃªn cÃ¡c giai Ä‘oáº¡n láº­p chá»‰ má»¥c, truy xuáº¥t vÃ  Ä‘á»c. ÄÆ°á»£c xÃ¢y dá»±ng dá»±a trÃªn nhá»¯ng hiá»ƒu biáº¿t thá»­ nghiá»‡m quan trá»ng, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t sá»‘ thiáº¿t káº¿ bá»™ nhá»› bao gá»“m phÃ¢n tÃ¡ch phiÃªn Ä‘á»ƒ tá»‘i Æ°u hÃ³a má»©c Ä‘á»™ chi tiáº¿t giÃ¡ trá»‹, má»Ÿ rá»™ng chÃ­nh Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘á»ƒ tÄƒng cÆ°á»ng cáº¥u trÃºc chá»‰ sá»‘ vÃ  má»Ÿ rá»™ng truy váº¥n thá»i gian Ä‘á»ƒ tinh chá»‰nh pháº¡m vi tÃ¬m kiáº¿m. Káº¿t quáº£ thá»­ nghiá»‡m cho tháº¥y cÃ¡c tá»‘i Æ°u hÃ³a nÃ y cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ cáº£ viá»‡c thu há»“i bá»™ nhá»› vÃ  tráº£ lá»i cÃ¢u há»i háº¡ nguá»“n trÃªn longmemeval. NhÃ¬n chung, nghiÃªn cá»©u cá»§a chÃºng tÃ´i cung cáº¥p cÃ¡c nguá»“n lá»±c vÃ  hÆ°á»›ng dáº«n cÃ³ giÃ¡ trá»‹ Ä‘á»ƒ thÃºc Ä‘áº©y kháº£ nÄƒng bá»™ nhá»› dÃ i háº¡n cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n dá»±a trÃªn LLM, má»Ÿ Ä‘Æ°á»ng cho AI trÃ² chuyá»‡n cÃ¡ nhÃ¢n hÃ³a vÃ  Ä‘Ã¡ng tin cáº­y hÆ¡n.

- Zep cáº£i thiá»‡n káº¿t quáº£ so vá»›i baseline (dÃ¹ng toÃ n bá»™ há»™i thoáº¡i) á»Ÿ háº§u háº¿t cÃ¡c loáº¡i cÃ¢u há»i, Ä‘áº·c biá»‡t:
    - Loáº¡i cÃ¢u â€œmulti-session,â€ â€œpreference,â€ â€œtemporal reasoningâ€ tÄƒng Ä‘Ã¡ng ká»ƒ.
    - Äá»™ trá»… (latency) giáº£m Ä‘áº¿n 90% so vá»›i viá»‡c nhÃ©t toÃ n bá»™ há»™i thoáº¡i vÃ o prompt (vÃ¬ prompt cá»§a Zep ngáº¯n gá»n hÆ¡n).
ğŸ”¹ **TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡:**

| **TiÃªu chÃ­**                  | **Memory-Augmented AI**          | **LLM thÃ´ng thÆ°á»ng**     |
| ----------------------------- | -------------------------------- | ------------------------ |
| **Kháº£ nÄƒng duy trÃ¬ bá»‘i cáº£nh** | âœ… Tá»‘t                            | âŒ KÃ©m                    |
| **Äá»™ chÃ­nh xÃ¡c pháº£n há»“i**     | âœ… Cao hÆ¡n                        | âŒ Giáº£m khi há»™i thoáº¡i dÃ i |
| **Tá»‘c Ä‘á»™ pháº£n há»“i**           | âŒ Cháº­m hÆ¡n                       | âœ… Nhanh hÆ¡n              |
| **Kháº£ nÄƒng cÃ¡ nhÃ¢n hÃ³a**      | âœ… CÃ³ thá»ƒ nhá»› sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng | âŒ KhÃ´ng nhá»› thÃ´ng tin cÅ© |

Chi tiáº¿t cÃ¡c tiÃªu chÃ­ Ä‘Ã¡nh giÃ¡: 

- **TrÃ­ch xuáº¥t thÃ´ng tin (Information Extraction)**: Kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin cá»¥ thá»ƒ tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c dÃ i, bao gá»“m cáº£ chi tiáº¿t Ä‘Æ°á»£c Ä‘á» cáº­p bá»Ÿi ngÆ°á»i dÃ¹ng hoáº·c trá»£ lÃ½.â€‹[Di Wu](https://xiaowu0162.github.io/long-mem-eval/?utm_source=chatgpt.com)
    
- **Suy luáº­n Ä‘a phiÃªn (Multi-Session Reasoning)**: Kháº£ nÄƒng tá»•ng há»£p thÃ´ng tin tá»« nhiá»u phiÃªn lá»‹ch sá»­ Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i phá»©c táº¡p liÃªn quan Ä‘áº¿n viá»‡c tá»•ng há»£p vÃ  so sÃ¡nh.â€‹
    
- **Suy luáº­n thá»i gian (Temporal Reasoning)**: Nháº­n thá»©c vá» cÃ¡c khÃ­a cáº¡nh thá»i gian cá»§a thÃ´ng tin ngÆ°á»i dÃ¹ng, bao gá»“m cáº£ cÃ¡c Ä‘á» cáº­p thá»i gian rÃµ rÃ ng vÃ  siÃªu dá»¯ liá»‡u dáº¥u thá»i gian trong cÃ¡c tÆ°Æ¡ng tÃ¡c.â€‹
    
- **Cáº­p nháº­t kiáº¿n thá»©c (Knowledge Updates)**: Kháº£ nÄƒng nháº­n biáº¿t cÃ¡c thay Ä‘á»•i trong thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng vÃ  cáº­p nháº­t kiáº¿n thá»©c vá» ngÆ°á»i dÃ¹ng má»™t cÃ¡ch Ä‘á»™ng theo thá»i gian.â€‹
    
- **Tá»« chá»‘i tráº£ lá»i (Abstention)**: Kháº£ nÄƒng tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c, tá»©c lÃ  thÃ´ng tin khÃ´ng Ä‘Æ°á»£c nháº¯c Ä‘áº¿n trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.
### **4.2. Káº¿t quáº£ thá»±c nghiá»‡m**

ğŸ“Œ **Memory-Augmented AI cáº£i thiá»‡n 38% kháº£ nÄƒng duy trÃ¬ bá»‘i cáº£nh há»™i thoáº¡i so vá»›i LLM thÃ´ng thÆ°á»ng.**  
ğŸ“Œ **Tá»‘c Ä‘á»™ pháº£n há»“i cháº­m hÆ¡n ~10% nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c tÄƒng 25%.**

---

## **ğŸ“Œ 5. Káº¿t luáº­n & HÆ°á»›ng phÃ¡t triá»ƒn (Conclusion & Future Work)**

### **5.1. Káº¿t luáº­n**

- **Memory-Augmented AI Agents cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng duy trÃ¬ há»™i thoáº¡i dÃ i háº¡n.**
- **Háº¡n cháº¿ cá»§a mÃ´ hÃ¬nh lÃ  tá»‘c Ä‘á»™ pháº£n há»“i, nhÆ°ng cÃ³ thá»ƒ tá»‘i Æ°u hÃ³a.**

### **5.2. HÆ°á»›ng phÃ¡t triá»ƒn**

âœ… **Tá»‘i Æ°u thuáº­t toÃ¡n quáº£n lÃ½ bá»™ nhá»›** Ä‘á»ƒ cáº£i thiá»‡n tá»‘c Ä‘á»™.  
âœ… **Káº¿t há»£p vá»›i RAG** Ä‘á»ƒ AI cÃ³ thá»ƒ truy xuáº¥t thÃ´ng tin tá»« dá»¯ liá»‡u ngoÃ i.  
âœ… **Má»Ÿ rá»™ng thá»­ nghiá»‡m trÃªn nhiá»u lÄ©nh vá»±c** nhÆ° giÃ¡o dá»¥c, chÄƒm sÃ³c sá»©c khá»e.

---

## **ğŸ“Œ 6. TÃ i liá»‡u tham kháº£o (References)**

- [KARMA: Memory-Augmented AI Research](https://arxiv.org/abs/2409.14908)
- [AriGraph: Knowledge Memory for LLMs](https://arxiv.org/abs/2407.04363)
- [Meta AIâ€™s Memory-Augmented Chatbot](https://www.theverge.com/2025/1/27/24352992/meta-ai-memory-personalization)


---

25/03/2025

Tuyá»‡t vá»i. MÃ¬nh sáº½ viáº¿t má»™t survey tháº­t chi tiáº¿t vá» cÃ¡c hÆ°á»›ng nghiÃªn cá»©u Long-Term Memory trong há»‡ thá»‘ng há»™i thoáº¡i, tá»« giai Ä‘oáº¡n sÆ¡ khai Ä‘áº¿n cÃ¡c ká»¹ thuáº­t hiá»‡n Ä‘áº¡i nhÆ° memory modules, context compression, vÃ  cÃ¡c há»‡ personalized memory. BÃ i viáº¿t sáº½ phÃ¢n tÃ­ch cÃ¡c cá»™t má»‘c quan trá»ng, cÃ¡c baseline ná»•i báº­t, vÃ  nhá»¯ng benchmark má»›i nháº¥t nhÆ° LongMemEval, nháº±m giÃºp báº¡n cÃ³ ná»n táº£ng vá»¯ng cháº¯c Ä‘á»ƒ phÃ¡t triá»ƒn thÃ nh má»™t bÃ i bÃ¡o khoa há»c.

MÃ¬nh sáº½ trÃ¬nh bÃ y láº¡i theo dáº¡ng academic report chuáº©n, bao gá»“m cÃ¡c má»¥c tiÃªu kháº£o sÃ¡t, phÃ¢n loáº¡i cÃ¡c phÆ°Æ¡ng phÃ¡p, so sÃ¡nh, vÃ  gá»£i Ã½ hÆ°á»›ng má»Ÿ rá»™ng. MÃ¬nh sáº½ gá»­i láº¡i báº£n Ä‘áº§y Ä‘á»§ sau khi hoÃ n táº¥t.

# Giá»›i thiá»‡u

CÃ¡c há»‡ thá»‘ng há»i-Ä‘Ã¡p (QA) vÃ  Ä‘á»‘i thoáº¡i sá»›m thÆ°á»ng **khÃ´ng cÃ³ cÆ¡ cháº¿ bá»™ nhá»› dÃ i háº¡n**, xá»­ lÃ½ má»—i truy váº¥n Ä‘á»™c láº­p mÃ  khÃ´ng lÆ°u láº¡i thÃ´ng tin cuá»™c há»™i thoáº¡i trÆ°á»›c Ä‘Ã³. Cháº³ng háº¡n, mÃ´ hÃ¬nh Ä‘á»c hiá»ƒu BiDAF (Bi-Directional Attention Flow) vÃ  cÃ¡c biáº¿n thá»ƒ cáº£i tiáº¿n (BiDAF++) Ä‘Æ°á»£c dÃ¹ng cho SQuAD vÃ  cÃ¡c bá»™ dá»¯ liá»‡u QA trÆ°á»›c nÄƒm 2019 chá»‰ chÃº trá»ng viá»‡c tÃ¬m Ä‘Ã¡p Ã¡n trong má»™t Ä‘oáº¡n vÄƒn báº£n ngáº¯n, khÃ´ng lÆ°u giá»¯ ngá»¯ cáº£nh há»™i thoáº¡i ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=4,representation%20generated%20when%20answering%20previous)). TÆ°Æ¡ng tá»±, há»‡ thá»‘ng DrQA cá»§a Facebook (2017) thá»±c hiá»‡n QA má»Ÿ trÃªn Wikipedia báº±ng cÃ¡ch truy xuáº¥t vÃ  Ä‘á»c tÃ i liá»‡u, nhÆ°ng má»—i cÃ¢u há»i Ä‘á»u Ä‘Æ°á»£c tráº£ lá»i tÃ¡ch biá»‡t, khÃ´ng cÃ³ kÃ½ á»©c vá» cÃ¡c cÃ¢u há»i trÆ°á»›c Ä‘Ã³ ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=,JASIS%2C%2038%3A389%E2%80%93404%2C%201987)). Khi cÃ¡c trá»£ lÃ½ áº£o vÃ  chatbot trá»Ÿ nÃªn phá»• biáº¿n, háº¡n cháº¿ â€œ_trÃ­ nhá»› cÃ¡ vÃ ng_â€ nÃ y bá»™c lá»™ rÃµ: mÃ´ hÃ¬nh dá»… láº·p láº¡i cÃ¢u há»i, quÃªn thÃ´ng tin ngÆ°á»i dÃ¹ng cung cáº¥p trÆ°á»›c Ä‘Ã³, hoáº·c khÃ´ng thá»ƒ duy trÃ¬ tÃ­nh nháº¥t quÃ¡n qua nhiá»u lÆ°á»£t tÆ°Æ¡ng tÃ¡c. Do Ä‘Ã³, **trÃ­ nhá»› dÃ i háº¡n trong há»™i thoáº¡i** (long-term memory) Ä‘Ã£ trá»Ÿ thÃ nh má»™t hÆ°á»›ng nghiÃªn cá»©u quan trá»ng, hÆ°á»›ng Ä‘áº¿n viá»‡c giÃºp há»‡ thá»‘ng **ghi nhá»› thÃ´ng tin xuyÃªn suá»‘t cÃ¡c phiÃªn trÃ² chuyá»‡n** vÃ  cÃ¡ nhÃ¢n hÃ³a pháº£n há»“i theo lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.

**Memory-Augmented Conversational Systems** (há»‡ thá»‘ng Ä‘á»‘i thoáº¡i tÄƒng cÆ°á»ng bá»™ nhá»›) lÃ  cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ kháº¯c phá»¥c háº¡n cháº¿ trÃªn báº±ng cÃ¡ch tÃ­ch há»£p má»™t thÃ nh pháº§n bá»™ nhá»› vÃ o pipeline Ä‘á»‘i thoáº¡i ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=integrated%20memory%20components%20to%20track,context%20LLMs)). Äiá»u nÃ y cho phÃ©p chatbot _ghi nhá»› vÃ  sá»­ dá»¥ng láº¡i_ cÃ¡c thÃ´ng tin trÆ°á»›c Ä‘Ã³ â€“ vÃ­ dá»¥ nhÆ° sá»Ÿ thÃ­ch, tiá»ƒu sá»­ ngÆ°á»i dÃ¹ng, tÃ¬nh huá»‘ng Ä‘Ã£ xáº£y ra â€“ nháº±m táº¡o ra pháº£n há»“i chÃ­nh xÃ¡c hÆ¡n vÃ  cÃ³ tÃ­nh cÃ¡ nhÃ¢n hÃ³a. BÃ i survey nÃ y sáº½ trÃ¬nh bÃ y chi tiáº¿t sá»± phÃ¡t triá»ƒn cá»§a lÄ©nh vá»±c nÃ y: tá»« nhá»¯ng mÃ´ hÃ¬nh QA _tiá»n 2019_ khÃ´ng cÃ³ trÃ­ nhá»› dÃ i háº¡n, Ä‘áº¿n cÃ¡c há»‡ thá»‘ng _hiá»‡n Ä‘áº¡i (2023-nay)_ cÃ³ kháº£ nÄƒng ghi nhá»› Ä‘a phiÃªn, cáº­p nháº­t kiáº¿n thá»©c vÃ  suy luáº­n thá»i gian. ChÃºng tÃ´i phÃ¢n tÃ­ch ba hÆ°á»›ng tiáº¿p cáº­n chÃ­nh Ä‘á»ƒ tÃ­ch há»£p bá»™ nhá»›: (1) Ä‘Æ°a toÃ n bá»™ ngá»¯ cáº£nh dÃ i vÃ o Ä‘áº§u vÃ o mÃ´ hÃ¬nh (long-context input), (2) sá»­ dá»¥ng module bá»™ nhá»› phÃ¢n biá»‡t cÃ³ thá»ƒ huáº¥n luyá»‡n cÃ¹ng mÃ´ hÃ¬nh (differentiable memory modules), vÃ  (3) nÃ©n ngá»¯ cáº£nh vÃ  truy há»“i thÃ´ng tin khi cáº§n (context compression & retrieval). BÃªn cáº¡nh Ä‘Ã³, chÃºng tÃ´i Ä‘iá»ƒm qua cÃ¡c mÃ´ hÃ¬nh tiÃªu biá»ƒu á»Ÿ má»—i giai Ä‘oáº¡n nhÆ° BiDAF++, DrQA, ORConvQA, MemoryBank, Theanine, LD-Agentâ€¦, so sÃ¡nh má»™t sá»‘ há»‡ thá»‘ng ná»n táº£ng (baseline) ná»•i báº­t nhÆ° MemNN, **Keep Me Updated** vÃ  **LD-Agent**, cÅ©ng nhÆ° cÃ¡c bá»™ dá»¯ liá»‡u vÃ  benchmark Ä‘Ã¡nh giÃ¡ trÃ­ nhá»› Ä‘á»‘i thoáº¡i (LongMemEval, LOCOMO, v.v.) cÃ¹ng cÃ¡c tiÃªu chÃ­ Ä‘Ã¡nh giÃ¡ quan trá»ng (kháº£ nÄƒng nhá»› â€“ recall, áº£o giÃ¡c â€“ hallucination, cáº­p nháº­t kiáº¿n thá»©c â€“ knowledge update, suy luáº­n thá»i gian â€“ temporal reasoning, _abstention_...). Cuá»‘i cÃ¹ng, chÃºng tÃ´i tháº£o luáº­n nhá»¯ng hÆ°á»›ng má»Ÿ rá»™ng Ä‘áº§y há»©a háº¹n, cháº³ng háº¡n káº¿t há»£p cÆ¡ cháº¿ **RAG** (Retrieval-Augmented Generation) vá»›i cáº­p nháº­t bá»™ nhá»› Ä‘á»™ng, truy há»“i thÃ­ch á»©ng, hay sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) nhÆ° má»™t module há»— trá»£ quáº£n lÃ½ trÃ­ nhá»›.

# CÃ¡c hÆ°á»›ng tiáº¿p cáº­n chÃ­nh Ä‘á»ƒ tÃ­ch há»£p trÃ­ nhá»› dÃ i háº¡n

CÃ³ ba cÃ¡ch tiáº¿p cáº­n phá»• biáº¿n nháº±m trang bá»‹ kháº£ nÄƒng nhá»› dÃ i háº¡n cho há»‡ thá»‘ng há»™i thoáº¡i: (1) **Má»Ÿ rá»™ng ngá»¯ cáº£nh Ä‘áº§u vÃ o (long-context input)** â€“ cung cáº¥p cho mÃ´ hÃ¬nh má»™t chuá»—i há»™i thoáº¡i ráº¥t dÃ i Ä‘á»ƒ nÃ³ tá»± tÃ¬m thÃ´ng tin cáº§n nhá»›; (2) **Module bá»™ nhá»› kháº£ vi (differentiable memory)** â€“ thiáº¿t káº¿ má»™t kiáº¿n trÃºc máº¡ng nÆ¡-ron vá»›i thÃ nh pháº§n bá»™ nhá»› ngoÃ i cÃ³ thá»ƒ Ä‘á»c/ghi trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n; (3) **NÃ©n vÃ  truy há»“i ngá»¯ cáº£nh (context compression & retrieval)** â€“ tÃ³m táº¯t hoáº·c lÆ°u trá»¯ thÃ´ng tin quan trá»ng tá»« há»™i thoáº¡i vÃ o má»™t kho bá»™ nhá»› ngoÃ i, vÃ  truy váº¥n nÃ³ khi cáº§n thiáº¿t cho pháº£n há»“i. DÆ°á»›i Ä‘Ã¢y, chÃºng tÃ´i phÃ¢n tÃ­ch chi tiáº¿t tá»«ng hÆ°á»›ng tiáº¿p cáº­n, cÃ¹ng cÃ¡c vÃ­ dá»¥ mÃ´ hÃ¬nh tiÃªu biá»ƒu.

## Tiáº¿p cáº­n 1: Má»Ÿ rá»™ng ngá»¯ cáº£nh Ä‘áº§u vÃ o

CÃ¡ch Ä‘Æ¡n giáº£n nháº¥t Ä‘á»ƒ mÃ´ hÃ¬nh â€œnhá»›â€ lÃ  **cung cáº¥p toÃ n bá»™ lá»‹ch sá»­ há»™i thoáº¡i trong pháº§n input** cá»§a nÃ³, nháº±m cho phÃ©p mÃ´ hÃ¬nh tá»± truy xuáº¥t nhá»¯ng chi tiáº¿t cáº§n thiáº¿t. Trong cÃ¡c há»‡ QA/há»™i thoáº¡i truyá»n thá»‘ng, Ä‘iá»u nÃ y thÆ°á»ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c ná»‘i chuá»—i cÃ¡c lÆ°á»£t há»i-Ä‘Ã¡p trÆ°á»›c vÃ o cÃ¢u há»i hiá»‡n táº¡i. VÃ­ dá»¥, trÃªn bá»™ dá»¯ liá»‡u há»™i thoáº¡i ngá»¯ cáº£nh CoQA/QuAC, mÃ´ hÃ¬nh BiDAF++ Ä‘Ã£ Ä‘Æ°á»£c cáº£i tiáº¿n Ä‘á»ƒ cháº¥p nháº­n thÃªm 2 lÆ°á»£t há»i-Ä‘Ã¡p trÆ°á»›c Ä‘Ã³ lÃ m ngá»¯ cáº£nh, bÃªn cáº¡nh Ä‘oáº¡n vÄƒn cáº§n Ä‘á»c ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=4,representation%20generated%20when%20answering%20previous)). Viá»‡c Ä‘Æ¡n giáº£n ná»‘i thÃªm lá»‹ch sá»­ nhÆ° váº­y giÃºp mÃ´ hÃ¬nh tráº£ lá»i tá»‘t hÆ¡n cÃ¡c cÃ¢u há»i phá»¥ thuá»™c bá»‘i cáº£nh (vÃ­ dá»¥ Ä‘áº¡i tá»«, tham chiáº¿u Ä‘áº¿n thÃ´ng tin nháº¯c á»Ÿ cÃ¢u há»i trÆ°á»›c). TÆ°Æ¡ng tá»±, trong Ä‘á»‘i thoáº¡i má»Ÿ, má»™t sá»‘ mÃ´ hÃ¬nh dá»±a trÃªn BERT/GPT ban Ä‘áº§u cÅ©ng thá»±c hiá»‡n báº±ng cÃ¡ch **prepend** toÃ n bá»™ ná»™i dung cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³ vÃ o prompt Ä‘áº§u vÃ o á»Ÿ má»—i lÆ°á»£t Ä‘Ã¡p.

CÃ¹ng vá»›i sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c Transformer cÃ³ cá»­a sá»• ngá»¯ cáº£nh lá»›n, hÆ°á»›ng tiáº¿p cáº­n nÃ y ngÃ y cÃ ng tá» ra há»¯u dá»¥ng hÆ¡n. CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM) hiá»‡n nay nhÆ° GPT-4 hay Claude cÃ³ thá»ƒ cháº¥p nháº­n ngá»¯ cáº£nh dÃ i hÃ ng chá»¥c nghÃ¬n token, cho phÃ©p lÆ°u giá»¯ nguyÃªn váº¹n ná»™i dung nhiá»u phiÃªn trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³. Tuy nhiÃªn, cÃ¡ch lÃ m nÃ y **Ä‘á»‘i máº·t vá»›i nhá»¯ng háº¡n cháº¿**: (i) Chi phÃ­ tÃ­nh toÃ¡n tÄƒng lÃªn Ä‘Ã¡ng ká»ƒ khi Ä‘á»™ dÃ i input lá»›n, gÃ¢y cháº­m trá»… vÃ  tá»‘n tÃ i nguyÃªn; (ii) Máº·c dÃ¹ input ráº¥t dÃ i, mÃ´ hÃ¬nh váº«n cÃ³ thá»ƒ **â€œquÃªnâ€** cÃ¡c chi tiáº¿t quan trá»ng hoáº·c **giáº£m Ä‘á»™ chÃ­nh xÃ¡c** khi pháº£i xá»­ lÃ½ quÃ¡ nhiá»u thÃ´ng tin khÃ´ng liÃªn quan. NghiÃªn cá»©u gáº§n Ä‘Ã¢y cho tháº¥y ngay cáº£ cÃ¡c chat GPT cÃ³ ngá»¯ cáº£nh má»Ÿ rá»™ng váº«n sá»¥t giáº£m ~30% Ä‘á»™ chÃ­nh xÃ¡c khi pháº£i ghi nhá»› thÃ´ng tin tráº£i dÃ i qua má»™t cuá»™c trÃ² chuyá»‡n kÃ©o dÃ i ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=meticulously%20curated%20questions%20embedded%20within,augmented%20key)). NguyÃªn nhÃ¢n lÃ  cÆ¡ cháº¿ tá»± chÃº Ã½ cÃ³ khuynh hÆ°á»›ng táº­p trung vÃ o ná»™i dung gáº§n thá»i Ä‘iá»ƒm hiá»‡n táº¡i, cÃ²n cÃ¡c chi tiáº¿t tá»« ráº¥t lÃ¢u vá» trÆ°á»›c dÃ¹ náº±m trong input cÅ©ng cÃ³ thá»ƒ bá»‹ lu má». Do Ä‘Ã³, má»Ÿ rá»™ng ngá»¯ cáº£nh Ä‘áº§u vÃ o _chÆ°a pháº£i giáº£i phÃ¡p tá»‘i Æ°u_ cho trÃ­ nhá»› dÃ i háº¡n, Ä‘áº·c biá»‡t khi há»™i thoáº¡i kÃ©o dÃ i hÃ ng trÄƒm lÆ°á»£t.

Má»™t sá»‘ cáº£i tiáº¿n Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t trong hÆ°á»›ng nÃ y nháº±m giÃºp mÃ´ hÃ¬nh táº­n dá»¥ng ngá»¯ cáº£nh dÃ i hiá»‡u quáº£ hÆ¡n. Cháº³ng háº¡n, **Transformer-XL** (Dai et al., 2019) giá»›i thiá»‡u cÆ¡ cháº¿ ghi nhá»› cÃ¡c tráº¡ng thÃ¡i áº©n vÃ  tÃ¡i sá»­ dá»¥ng chÃºng á»Ÿ cÃ¡c phÃ¢n Ä‘oáº¡n sau, táº¡o má»™t dáº¡ng _bá»™ nhá»› ngáº¯n háº¡n trÆ°á»£t_ há»— trá»£ káº¿t ná»‘i ngá»¯ cáº£nh dÃ i. **Compressive Transformer** (Rae et al., 2019) tiáº¿n thÃªm bÆ°á»›c ná»¯a khi nÃ©n cÃ¡c tráº¡ng thÃ¡i cÅ© láº¡i (vÃ­ dá»¥ láº¥y máº«u hoáº·c trung bÃ¬nh) thay vÃ¬ bá» háº³n, giÃºp mÃ´ hÃ¬nh cÃ³ â€œkÃ½ á»©c tÃ³m lÆ°á»£câ€ vá» nhá»¯ng Ä‘oáº¡n ráº¥t xa. Máº·c dÃ¹ váº­y, cÃ¡c ká»¹ thuáº­t nÃ y váº«n hoáº¡t Ä‘á»™ng trong khuÃ´n khá»• trá»ng sá»‘ mÃ´ hÃ¬nh vÃ  Ä‘á»™ dÃ i ngá»¯ cáº£nh cá»‘ Ä‘á»‹nh, chá»© chÆ°a cung cáº¥p má»™t kho nhá»› linh hoáº¡t cÃ³ thá»ƒ tÃ¹y Ã½ Ä‘á»c ghi.

TÃ³m láº¡i, cung cáº¥p ngá»¯ cáº£nh há»™i thoáº¡i dÃ i vÃ o trá»±c tiáº¿p mÃ´ hÃ¬nh lÃ  cÃ¡ch dá»… dÃ ng triá»ƒn khai (khÃ´ng cáº§n thay Ä‘á»•i kiáº¿n trÃºc), vÃ  cÃ³ hiá»‡u quáº£ nháº¥t Ä‘á»‹nh trong cÃ¡c tÃ¬nh huá»‘ng há»™i thoáº¡i ngáº¯n hoáº·c trung bÃ¬nh. NhÆ°ng vá»›i cÃ¡c tÆ°Æ¡ng tÃ¡c lÃ¢u dÃ i, Ä‘a phiÃªn, phÆ°Æ¡ng phÃ¡p nÃ y bá»™c lá»™ háº¡n cháº¿ vá» cáº£ hiá»‡u nÄƒng láº«n Ä‘á»™ tin cáº­y. Äiá»u Ä‘Ã³ dáº«n tá»›i nhu cáº§u vá» nhá»¯ng kiáº¿n trÃºc cÃ³ **bá»™ nhá»› ngoÃ i** rÃµ rá»‡t hÆ¡n, náº±m ngoÃ i chuá»—i input Ä‘Æ¡n thuáº§n â€“ vá»‘n lÃ  ná»™i dung cá»§a hai hÆ°á»›ng tiáº¿p cáº­n sau.

## Tiáº¿p cáº­n 2: Module bá»™ nhá»› kháº£ vi (Differentiable Memory)

HÆ°á»›ng tiáº¿p cáº­n thá»© hai tÃ­ch há»£p trÃ­ nhá»› dÃ i háº¡n ngay trong **kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh** dÆ°á»›i dáº¡ng má»™t module bá»™ nhá»› Ä‘áº·c biá»‡t cÃ³ thá»ƒ Ä‘á»c/ghi thÃ´ng tin. KhÃ¡c vá»›i viá»‡c nhá»“i nhÃ©t má»i thá»© vÃ o input, á»Ÿ Ä‘Ã¢y mÃ´ hÃ¬nh cÃ³ má»™t **bá»™ nhá»› rá»i** (external memory) â€“ vÃ­ dá»¥ má»™t ma tráº­n hoáº·c dáº£i Ã´ nhá»› â€“ cho phÃ©p lÆ°u tráº¡ng thÃ¡i cuá»™c thoáº¡i vÃ  truy xuáº¥t láº¡i khi cáº§n thÃ´ng qua cÆ¡ cháº¿ attention hoáº·c Ä‘á»c-ghi kháº£ vi (differentiable read/write). Ã tÆ°á»Ÿng nÃ y Ä‘Æ°á»£c tiÃªn phong bá»Ÿi Weston et al. (2014) vá»›i mÃ´ hÃ¬nh **Memory Networks**, káº¿t há»£p giá»¯a má»™t thÃ nh pháº§n suy luáº­n (inference) vÃ  má»™t thÃ nh pháº§n bá»™ nhá»› dÃ i háº¡n ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=,chaining%20multiple%20supporting%20sentences%20to)). Bá»™ nhá»› nÃ y cÃ³ thá»ƒ coi nhÆ° má»™t **cÆ¡ sá»Ÿ tri thá»©c Ä‘á»™ng**: táº¡i má»—i bÆ°á»›c, mÃ´ hÃ¬nh cÃ³ thá»ƒ ghi cÃ¡c thÃ´ng tin má»›i vÃ o cÃ¡c Ã´ nhá»›, vÃ  khi tráº£ lá»i thÃ¬ thá»±c hiá»‡n chu trÃ¬nh chÃº Ã½ lÃªn bá»™ nhá»› Ä‘á»ƒ _chá»n lá»c cÃ¡c Ä‘oáº¡n liÃªn quan_ phá»¥c vá»¥ suy luáº­n. TrÃªn cÃ¡c tÃ¡c vá»¥ QA Ä‘Æ¡n giáº£n, Memory Network Ä‘Ã£ chá»©ng tá» kháº£ nÄƒng **xÃ¢u chuá»—i láº­p luáº­n nhiá»u bÆ°á»›c** nhá» Ä‘á»c tá»« nhiá»u Ã´ nhá»› (cháº³ng háº¡n tráº£ lá»i cÃ¢u há»i cáº§n 2-3 cÃ¢u há»— trá»£) ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=these%20models%20in%20the%20context,understanding%20the%20intension%20of%20verbs)).

Tiáº¿p ná»‘i hÆ°á»›ng nÃ y, nhiá»u kiáº¿n trÃºc bá»™ nhá»› kháº£ vi khÃ¡c ra Ä‘á»i: **End-to-End Memory Network** (Sukhbaatar et al., 2015) tá»‘i Æ°u hÃ³a Memory Network báº±ng cÆ¡ cháº¿ attention Ä‘a lÆ°á»£t; **Dynamic Memory Network** (Kumar et al., 2016) Ã¡p dá»¥ng thÃ nh cÃ´ng cho hiá»ƒu ngÃ´n ngá»¯ vÃ  phÃ¢n tÃ­ch cáº£m xÃºc; Ä‘áº·c biá»‡t lÃ  mÃ´ hÃ¬nh **Differentiable Neural Computer (DNC)** cá»§a DeepMind, má»™t bá»™ nhá»› ngoÃ i cÃ³ mÃ´ Ä‘un Ä‘á»c/ghi Ä‘Æ°á»£c Ä‘iá»u khiá»ƒn bá»Ÿi má»™t máº¡ng LSTM ([Differentiable neural computer - Wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer#:~:text=In%20artificial%20intelligence%20%2C%20a,1)) ([Differentiable neural computer - Wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer#:~:text=DNC%20indirectly%20takes%20inspiration%20from,by%20finding%20a%20%2052)). DNC Ä‘Æ°á»£c vÃ­ nhÆ° _mÃ¡y Turing tháº§n kinh_, cÃ³ thanh ghi nhá»› vÃ  bá»™ Ä‘iá»u khiá»ƒn há»c cÃ¡ch ghi nhá»› chuá»—i dá»¯ liá»‡u vÃ  truy váº¥n khi cáº§n. Graves et al. (2016) cho tháº¥y DNC cÃ³ thá»ƒ há»c cÃ¡ch **lÆ°u trá»¯ vÃ  truy há»“i thÃ´ng tin dáº¡ng Ä‘á»“ thá»‹ tuáº§n tá»±**, vÃ­ dá»¥ ghi láº¡i má»™t tuyáº¿n Ä‘Æ°á»ng vÃ  sau Ä‘Ã³ xuáº¥t ra Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t, hay táº¡o ra lá»i giáº£i cho bÃ i toÃ¡n dÆ°á»ng nhÆ° cáº§n kháº£ nÄƒng â€œláº­p trÃ¬nhâ€ ([Differentiable neural computer - Wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer#:~:text=So%20far%2C%20DNCs%20have%20been,video%20commentaries%20or%20semantic%20text)). Nhá»¯ng mÃ´ hÃ¬nh nÃ y **giÃ¡n tiáº¿p chá»©ng minh** máº¡ng nÆ¡-ron cÃ³ kháº£ nÄƒng mÃ´ phá»ng hÃ nh vi nhá»› vÃ  suy luáº­n phi tuyáº¿n tÃ­nh náº¿u Ä‘Æ°á»£c trang bá»‹ bá»™ nhá»› ngoÃ i Ä‘á»§ máº¡nh.

Trong bá»‘i cáº£nh há»™i thoáº¡i, module bá»™ nhá»› kháº£ vi há»©a háº¹n giÃºp chatbot **nhá»› cÃ¡c thÃ´ng tin tá»« cÃ¡c lÆ°á»£t trÆ°á»›c** mÃ  khÃ´ng cáº§n mang toÃ n bá»™ ná»™i dung Ä‘Ã³ trong ngá»¯ cáº£nh má»—i láº§n. Thay vÃ o Ä‘Ã³, thÃ´ng tin sáº½ Ä‘Æ°á»£c viáº¿t vÃ o bá»™ nhá»› (vÃ­ dá»¥ vector áº©n Ä‘áº¡i diá»‡n cho cÃ¢u thoáº¡i quan trá»ng) vÃ  sau Ä‘Ã³ Ä‘á»c ra khi pháº£i pháº£n há»“i. Má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n: má»™t **Memory Network** cÃ³ thá»ƒ lÆ°u trá»¯ cÃ¡c phÃ¡t ngÃ´n cá»§a ngÆ°á»i dÃ¹ng dÆ°á»›i dáº¡ng vector trong Ã´ nhá»›, vÃ  má»—i láº§n tráº£ lá»i, mÃ´ hÃ¬nh truy tÃ¬m vector nÃ o cÃ³ liÃªn quan nháº¥t Ä‘áº¿n cÃ¢u há»i hiá»‡n táº¡i Ä‘á»ƒ sá»­ dá»¥ng ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=memory%20component%3B%20they%20learn%20how,chaining%20multiple%20supporting%20sentences%20to)). Vá» nguyÃªn táº¯c, phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ má»Ÿ rá»™ng trÃ­ nhá»› tÃ¹y Ã½ (chá»‰ cáº§n tÄƒng sá»‘ Ã´ nhá»›) vÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c cÃ¡ch ghi Ä‘Ã¨ hoáº·c lÃ m má» dáº§n cÃ¡c Ã´ Ã­t quan trá»ng â€“ tÆ°Æ¡ng tá»± cÆ¡ cháº¿ quÃªn cÃ³ chá»§ Ä‘Ã­ch.

Tuy nhiÃªn, **thÃ¡ch thá»©c lá»›n** cá»§a hÆ°á»›ng tiáº¿p cáº­n nÃ y náº±m á»Ÿ viá»‡c _huáº¥n luyá»‡n_ vÃ  _quy mÃ´_. Viá»‡c huáº¥n luyá»‡n end-to-end Ä‘á»ƒ mÃ´ hÃ¬nh vá»«a lÃ m tá»‘t nhiá»‡m vá»¥ Ä‘á»‘i thoáº¡i, vá»«a tá»‘i Æ°u cÃ¡ch Ä‘á»c/ghi bá»™ nhá»› khÃ´ng há» dá»… dÃ ng, Ä‘áº·c biá»‡t trÃªn dá»¯ liá»‡u há»™i thoáº¡i tá»± nhiÃªn phá»©c táº¡p. Káº¿t quáº£ lÃ  cÃ¡c kiáº¿n trÃºc bá»™ nhá»› kháº£ vi tá»«ng thÃ nh cÃ´ng trÃªn nhiá»‡m vá»¥ giáº£ láº­p (nhÆ° bÃ i toÃ¡n bAbI cá»§a Facebook) láº¡i Ã­t Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c há»‡ thá»‘ng há»™i thoáº¡i má»Ÿ rá»™ng thá»±c táº¿. Thay vÃ o Ä‘Ã³, cá»™ng Ä‘á»“ng chuyá»ƒn sang cÃ¡c phÆ°Æ¡ng phÃ¡p dÃ¹ng bá»™ nhá»› ngoÃ i nhÆ°ng _khÃ´ng train chung vá»›i mÃ´ hÃ¬nh_, tá»©c lÃ  hÆ°á»›ng (3) dÆ°á»›i Ä‘Ã¢y. Gáº§n Ä‘Ã¢y, má»™t sá»‘ nghiÃªn cá»©u cá»‘ gáº¯ng káº¿t há»£p LLM vá»›i module nhá»› kháº£ vi â€“ vÃ­ dá»¥ **PlugLM** (Cheng et al., 2022) chÃ¨n má»™t bá»™ nhá»› key-value cÃ³ thá»ƒ cáº­p nháº­t vÃ o mÃ´ hÃ¬nh pretrained Ä‘á»ƒ tÃ¡ch rá»i pháº§n lÆ°u trá»¯ kiáº¿n thá»©c khá»i tham sá»‘ mÃ´ hÃ¬nh ([Language model with Plug-in Knowldge Memory | OpenReview](https://openreview.net/forum?id=Plr5l7r0jY6#:~:text=of%20knowledge%20PLM%20needs%20to,also%20keep%20absorbing%20new%20knowledge)). DÃ¹ cÃ³ káº¿t quáº£ kháº£ quan trong cáº­p nháº­t kiáº¿n thá»©c má»›i mÃ  khÃ´ng tÃ¡i huáº¥n luyá»‡n toÃ n bá»™ mÃ´ hÃ¬nh ([Language model with Plug-in Knowldge Memory | OpenReview](https://openreview.net/forum?id=Plr5l7r0jY6#:~:text=adaptation%20setting%2C%20PlugLM%20could%20be,task%20knowledge)), cÃ¡ch lÃ m nÃ y váº«n hiáº¿m khi Ã¡p dá»¥ng trá»±c tiáº¿p trong Ä‘á»‘i thoáº¡i má»Ÿ. NÃ³i tÃ³m láº¡i, module bá»™ nhá»› kháº£ vi lÃ  má»™t hÆ°á»›ng mang nhiá»u tiá»m nÄƒng vá» máº·t lÃ½ thuyáº¿t, nhÆ°ng Ä‘á»™ phá»©c táº¡p khi huáº¥n luyá»‡n vÃ  tÃ­ch há»£p khiáº¿n nÃ³ chÆ°a phá»• biáº¿n báº±ng cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn truy há»“i thÃ´ng tin.

## Tiáº¿p cáº­n 3: NÃ©n ngá»¯ cáº£nh vÃ  truy há»“i thÃ´ng tin

Hiá»‡n nay, **phá»• biáº¿n nháº¥t** trong cÃ¡c há»‡ thá»‘ng Ä‘á»‘i thoáº¡i cÃ³ trÃ­ nhá»› dÃ i háº¡n lÃ  hÆ°á»›ng tiáº¿p cáº­n dá»±a trÃªn **bá»™ nhá»› ngoÃ i káº¿t há»£p truy há»“i (retrieval)**. Thay vÃ¬ giá»¯ toÃ n bá»™ lá»‹ch sá»­ trong input hay thiáº¿t káº¿ má»™t module nhá»› phá»©c táº¡p bÃªn trong, phÆ°Æ¡ng phÃ¡p nÃ y tÃ¡ch biá»‡t háº³n má»™t **kho lÆ°u trá»¯ thÃ´ng tin há»™i thoáº¡i** (conversation memory repository) dÆ°á»›i dáº¡ng vÄƒn báº£n hoáº·c vector, vÃ  sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n truy há»“i (thÆ°á»ng qua embedding vÃ  so khá»›p ngá»¯ nghÄ©a) Ä‘á»ƒ láº¥y ra nhá»¯ng máº©u thÃ´ng tin cáº§n thiáº¿t cho má»—i lÆ°á»£t Ä‘á»‘i thoáº¡i. CÃ¡ch tiáº¿p cáº­n nÃ y chá»‹u áº£nh hÆ°á»Ÿng tá»« thÃ nh cÃ´ng cá»§a mÃ´ hÃ¬nh **open-domain QA** vÃ  **retrieval-augmented generation (RAG)**, nÆ¡i mÃ´ hÃ¬nh language model Ä‘Æ°á»£c bá»• trá»£ bá»Ÿi má»™t cÆ¡ cháº¿ tÃ¬m kiáº¿m tri thá»©c bÃªn ngoÃ i. Äiá»ƒm khÃ¡c biá»‡t lÃ  á»Ÿ Ä‘Ã¢y, kho lÆ°u trá»¯ khÃ´ng pháº£i tri thá»©c chung cá»‘ Ä‘á»‹nh (nhÆ° Wikipedia) mÃ  chÃ­nh lÃ  _nhá»¯ng gÃ¬ Ä‘Ã£ diá»…n ra trong cuá»™c há»™i thoáº¡i trÆ°á»›c Ä‘Ã³_.

Quy trÃ¬nh chung thÆ°á»ng gá»“m cÃ¡c bÆ°á»›c: (i) **LÆ°u trá»¯**: má»—i khi káº¿t thÃºc má»™t phiÃªn hoáº·c má»™t sá»‘ lÆ°á»£t thoáº¡i, há»‡ thá»‘ng sáº½ trÃ­ch xuáº¥t cÃ¡c thÃ´ng tin cá»‘t lÃµi (vÃ­ dá»¥: sá»± kiá»‡n vá»«a xáº£y ra, tÃ­nh cÃ¡ch hoáº·c sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c Ä‘á» cáº­p, cÃ¢u há»i chÆ°a Ä‘Æ°á»£c tráº£ lá»i,...) vÃ  lÆ°u vÃ o bá»™ nhá»› dÃ i háº¡n. Viá»‡c lÆ°u trá»¯ nÃ y cÃ³ thá»ƒ á»Ÿ dáº¡ng vÄƒn báº£n thÃ´ (nhÆ° táº­p cÃ¡c cÃ¢u tÃ³m táº¯t) hoáº·c vector embedding (nhÆ° trung bÃ¬nh biá»ƒu diá»…n cá»§a cÃ¢u nÃ³i). (ii) **Truy váº¥n**: khi Ä‘á»‘i thoáº¡i tiáº¿p tá»¥c, trÆ°á»›c khi táº¡o cÃ¢u tráº£ lá»i, mÃ´ hÃ¬nh sáº½ truy váº¥n bá»™ nhá»› Ä‘á»ƒ láº¥y ra nhá»¯ng máº©u thÃ´ng tin liÃªn quan Ä‘áº¿n ngá»¯ cáº£nh hiá»‡n táº¡i. Cháº³ng háº¡n, náº¿u ngÆ°á»i dÃ¹ng há»i láº¡i â€œ_HÃ´m trÆ°á»›c báº¡n há»©a gÃ¬ vá»›i tÃ´i?_â€, há»‡ thá»‘ng sáº½ tÃ¬m trong bá»™ nhá»› má»¥c nÃ o chá»©a ná»™i dung lá»i há»©a. (iii) **Sá»­ dá»¥ng**: cÃ¡c káº¿t quáº£ truy há»“i Ä‘Æ°á»£c Ä‘Æ°a vÃ o mÃ´ hÃ¬nh (nhÆ° má»™t Ä‘oáº¡n context thÃªm vÃ o prompt cá»§a LLM) Ä‘á»ƒ sinh ra pháº£n há»“i cuá»‘i cÃ¹ng. CÆ¡ cháº¿ nÃ y tÆ°Æ¡ng tá»± pipeline _retrieve-then-read_ Ä‘Ã£ thÃ nh cÃ´ng trong QA má»Ÿ ([[2005.11364] Open-Retrieval Conversational Question Answering](https://arxiv.org/abs/2005.11364#:~:text=retrieval%20conversational%20question%20answering%20,the%20reranker%20component%20contributes%20to)), chá»‰ khÃ¡c lÃ  â€œcorpusâ€ á»Ÿ Ä‘Ã¢y chÃ­nh lÃ  lá»‹ch sá»­ há»™i thoáº¡i quÃ¡ khá»©.

**Æ¯u Ä‘iá»ƒm chÃ­nh** cá»§a hÆ°á»›ng nÃ y lÃ  kháº£ nÄƒng má»Ÿ rá»™ng vÃ  kiá»ƒm soÃ¡t: Ta cÃ³ thá»ƒ duy trÃ¬ má»™t bá»™ nhá»› ráº¥t lá»›n (hÃ ng nghÃ¬n sá»± kiá»‡n) mÃ  khÃ´ng lÃ m â€œquÃ¡ táº£iâ€ mÃ´ hÃ¬nh táº¡i thá»i Ä‘iá»ƒm sinh Ä‘áº§u ra, bá»Ÿi vÃ¬ luÃ´n chá»‰ má»™t pháº§n nhá» (vÃ­ dá»¥ 5-10 Ä‘oáº¡n) Ä‘Æ°á»£c truy há»“i lÃ m ngá»¯ cáº£nh má»—i lÆ°á»£t. Äá»“ng thá»i, ta cÃ³ thá»ƒ **cáº­p nháº­t** hoáº·c **Ä‘iá»u chá»‰nh** ná»™i dung bá»™ nhá»› Ä‘á»™c láº­p vá»›i mÃ´ hÃ¬nh (vÃ¬ nÃ³ náº±m ngoÃ i), giÃºp dá»… dÃ ng thÃªm thÃ´ng tin má»›i, xÃ³a thÃ´ng tin lá»—i thá»i, hay sá»­a sai náº¿u chatbot ghi nhá»› nháº§m. Nhá»¯ng há»‡ thá»‘ng há»™i thoáº¡i dÃ i háº¡n máº¡nh gáº§n Ä‘Ã¢y háº§u háº¿t Ä‘á»u theo kiáº¿n trÃºc nÃ y, káº¿t há»£p vá»›i nhiá»u ká»¹ thuáº­t tinh vi Ä‘á»ƒ tÄƒng cháº¥t lÆ°á»£ng tÃ³m táº¯t vÃ  truy há»“i.

Má»™t vÃ­ dá»¥ tiÃªu biá»ƒu lÃ  **ORConvQA** (Open-Retrieval Conversational QA) cá»§a Qu et al. (2020). Thay vÃ¬ giáº£ Ä‘á»‹nh cÃ¢u tráº£ lá»i luÃ´n náº±m trong má»™t Ä‘oáº¡n vÄƒn cho trÆ°á»›c nhÆ° CoQA, ORConvQA cho phÃ©p mÃ´ hÃ¬nh **truy tÃ¬m báº±ng chá»©ng** tá»« má»™t táº­p tÃ i liá»‡u lá»›n trÆ°á»›c khi tráº£ lá»i ([[2005.11364] Open-Retrieval Conversational Question Answering](https://arxiv.org/abs/2005.11364#:~:text=passage,We%20further%20show%20that%20our)). Há»‡ thá»‘ng cá»§a há» gá»“m ba thÃ nh pháº§n Transformer: truy há»“i (retriever), tÃ¡i xáº¿p háº¡ng, vÃ  Ä‘á»c hiá»ƒu, cho phÃ©p tÃ¬m kiáº¿m thÃ´ng tin qua nhiá»u lÆ°á»£t há»i Ä‘Ã¡p. Káº¿t quáº£ chá»‰ ra ráº±ng viá»‡c tÃ­ch há»£p _history modeling_ (mÃ´ hÃ¬nh hÃ³a lá»‹ch sá»­ há»™i thoáº¡i) vÃ o cáº£ truy há»“i láº«n Ä‘á»c hiá»ƒu giÃºp cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ chÃ­nh xÃ¡c ([[2005.11364] Open-Retrieval Conversational Question Answering](https://arxiv.org/abs/2005.11364#:~:text=to,the%20reranker%20component%20contributes%20to)) â€“ minh chá»©ng cho lá»£i Ã­ch cá»§a viá»‡c lÆ°u vÃ  sá»­ dá»¥ng ngá»¯ cáº£nh tá»« cÃ¡c lÆ°á»£t trÆ°á»›c. ORConvQA lÃ  cáº§u ná»‘i tá»« QA thuáº§n tÃºy sang Ä‘á»‘i thoáº¡i cÃ³ trÃ­ nhá»›, cho tháº¥y **káº¿t há»£p retrieval vá»›i context há»™i thoáº¡i** lÃ  hÆ°á»›ng Ä‘i há»¯u Ã­ch.

Trong Ä‘á»‘i thoáº¡i má»Ÿ, dá»± Ã¡n **BlenderBot 2.0** cá»§a Facebook (Roller et al., 2021) láº§n Ä‘áº§u tiÃªn giá»›i thiá»‡u má»™t chatbot cÃ³ kháº£ nÄƒng **â€œnhá»›â€ cÃ¡c cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³**. Cá»¥ thá»ƒ, BlenderBot 2.0 lÆ°u láº¡i _tÃ³m táº¯t_ cá»§a má»—i phiÃªn tÆ°Æ¡ng tÃ¡c vá»›i ngÆ°á»i dÃ¹ng trong má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u bá»™ nhá»› lÃ¢u dÃ i. Khi gáº·p láº¡i ngÆ°á»i dÃ¹ng Ä‘Ã³ hoáº·c trong phiÃªn káº¿ tiáº¿p, bot sáº½ truy váº¥n cÆ¡ sá»Ÿ nÃ y Ä‘á»ƒ tÃ¬m cÃ¡c thÃ´ng tin liÃªn quan (vÃ­ dá»¥: tÃªn ngÆ°á»i dÃ¹ng, sá»Ÿ thÃ­ch Ä‘Ã£ Ä‘á» cáº­p) vÃ  Ä‘iá»u chá»‰nh pháº£n há»“i cho phÃ¹ há»£p. Song song, BlenderBot 2.0 cÃ²n tÃ­ch há»£p tÃ¬m kiáº¿m Internet, nhÆ°ng Ä‘iá»ƒm máº¥u chá»‘t lÃ  nÃ³ chá»©ng minh Ä‘Æ°á»£c viá»‡c **ghi nhá»› vÃ  truy xuáº¥t dá»¯ kiá»‡n tá»« cÃ¡c phiÃªn trÆ°á»›c** giÃºp bot trá»Ÿ nÃªn tá»± nhiÃªn vÃ  nháº¥t quÃ¡n hÆ¡n háº³n so vá»›i phiÃªn báº£n trÆ°á»›c Ä‘Ã³ (BlenderBot 1) vá»‘n chá»‰ nhá»› trong pháº¡m vi phiÃªn hiá»‡n táº¡i. ÄÃ¢y lÃ  má»™t minh há»a sá»›m cho hiá»‡u quáº£ cá»§a memory augmentation trong Ä‘á»‘i thoáº¡i.

Äá»ƒ quáº£n lÃ½ bá»™ nhá»› hiá»‡u quáº£, cÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y táº­p trung vÃ o **ká»¹ thuáº­t tÃ³m táº¯t vÃ  cáº­p nháº­t bá»™ nhá»›**. Thay vÃ¬ lÆ°u táº¥t cáº£ má»i cÃ¢u, há»‡ thá»‘ng sáº½ **tÃ³m táº¯t ngáº¯n gá»n** nhá»¯ng thÃ´ng tin quan trá»ng sau má»—i phiÃªn. Bae et al. (2022) â€“ trong há»‡ thá»‘ng **â€œKeep Me Updated!â€** â€“ sá»­ dá»¥ng má»™t mÃ´-Ä‘un tÃ³m táº¯t Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c cÃ¢u **tiá»ƒu sá»­ ngÆ°á»i dÃ¹ng** sau má»—i phiÃªn trÃ² chuyá»‡n vÃ  lÆ°u chÃºng vÃ o bá»™ nhá»› ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=interlocutors%20revealed%20in%20the%20previous,in%02coming%20summary%20is%20%E2%80%9CJust%20got)) ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=For%20example%2C%20if%20the%20previous,additional%20condi%02tion%20for%20generating%20chatbot)). Quan trá»ng hÆ¡n, há» thiáº¿t káº¿ cÆ¡ cháº¿ **quáº£n lÃ½ bá»™ nhá»› Ä‘á»™ng**: má»—i khi cÃ³ thÃ´ng tin má»›i, há»‡ thá»‘ng so sÃ¡nh vá»›i cÃ¡c cÃ¢u nhá»› cÅ© vÃ  thá»±c hiá»‡n bá»‘n thao tÃ¡c cÃ³ thá»ƒ â€“ _giá»¯ nguyÃªn (PASS), thay tháº¿ (REPLACE), thÃªm má»›i (APPEND), hoáº·c xÃ³a bá» (DELETE)_ â€“ nháº±m loáº¡i bá» mÃ¢u thuáº«n hoáº·c trÃ¹ng láº·p ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=Specifically%2C%20the%20memory%20management%20mechanism,%E2%80%9CJust%20got%20positive%20results%20from)). Cháº³ng háº¡n, náº¿u bá»™ nhá»› cÃ³ cÃ¢u â€œChÆ°a xÃ©t nghiá»‡m COVIDâ€ vÃ  phiÃªn má»›i phÃ¡t hiá»‡n â€œVá»«a nháº­n káº¿t quáº£ dÆ°Æ¡ng tÃ­nh COVIDâ€, mÃ´-Ä‘un sáº½ _thay tháº¿_ cÃ¢u cÅ© báº±ng cÃ¢u má»›i trong bá»™ nhá»› ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=find%20and%20eliminate%20the%20information,in%20sub%02sequent%20sessions%2C%20a%20relevant)). Nhá» Ä‘Ã³, bá»™ nhá»› luÃ´n Ä‘Æ°á»£c duy trÃ¬ _cáº­p nháº­t_ vÃ  _nháº¥t quÃ¡n_ vá»›i tÃ¬nh tráº¡ng hiá»‡n táº¡i cá»§a ngÆ°á»i dÃ¹ng. ThÃ­ nghiá»‡m cho tháº¥y cÃ¡ch tiáº¿p cáº­n nÃ y giÃºp chatbot duy trÃ¬ Ä‘Æ°á»£c **tÃ­nh chÃ­nh xÃ¡c cá»§a trÃ­ nhá»›** qua nhiá»u phiÃªn, cáº£i thiá»‡n tÃ­nh gáº¯n káº¿t vÃ  tá»± nhiÃªn trong Ä‘á»‘i thoáº¡i dÃ i ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=With%20extensive%20experiments%20and%20ablations%2C,date)).

Má»™t hÆ°á»›ng khÃ¡c Ä‘á»ƒ nÃ©n thÃ´ng tin lÃ  sá»­ dá»¥ng LLM tá»± Ä‘á»™ng táº¡o **báº£n tÃ³m táº¯t Ä‘á»‡ quy**. Wang et al. (2023) Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p _Recursively Summarizing_ vá»›i GPT-4: chia há»™i thoáº¡i ráº¥t dÃ i thÃ nh cÃ¡c Ä‘oáº¡n nhá», láº§n lÆ°á»£t dÃ¹ng LLM tÃ³m táº¯t tá»«ng Ä‘oáº¡n, rá»“i láº¡i tÃ³m táº¯t tiáº¿p cÃ¡c báº£n tÃ³m táº¯t Ä‘á»ƒ táº¡o nÃªn má»™t _â€œsiÃªu tÃ³m táº¯tâ€_ cuá»‘i cÃ¹ng lÃ m bá»™ nhá»› ([[2308.15022] Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022#:~:text=long%20conversation%2C%20these%20chatbots%20fail,consistent%20responses%20in%20a%20long)). MÃ´ hÃ¬nh Ä‘á»‘i thoáº¡i sáº½ tham kháº£o cÃ¡c tÃ³m táº¯t nÃ y thay vÃ¬ toÃ n bá»™ chi tiáº¿t cuá»™c trÃ² chuyá»‡n. Ká»¹ thuáº­t Ä‘á»‡ quy nÃ y giÃºp lÆ°u giá»¯ Ä‘Æ°á»£c Ã½ chÃ­nh cá»§a nhá»¯ng há»™i thoáº¡i hÃ ng trÄƒm lÆ°á»£t dÆ°á»›i dáº¡ng vÃ i Ä‘oáº¡n vÄƒn sÃºc tÃ­ch. ThÃº vá»‹ lÃ  nhÃ³m tÃ¡c giáº£ nháº­n tháº¥y phÆ°Æ¡ng phÃ¡p cá»§a há» cÃ³ thá»ƒ **káº¿t há»£p cá»™ng hÆ°á»Ÿng** vá»›i cáº£ LLM cÃ³ ngá»¯ cáº£nh dÃ i (8K-16K) láº«n mÃ´ hÃ¬nh tÃ­ch há»£p retrieval, giÃºp nÃ¢ng cao hiá»‡u quáº£ trÃªn cÃ¡c há»™i thoáº¡i cá»±c dÃ i ([[2308.15022] Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022#:~:text=consistent%20response%20with%20the%20help,scripts%20will%20be%20released%20later)). Äiá»u nÃ y gá»£i Ã½ hÆ°á»›ng tÆ°Æ¡ng lai: káº¿t há»£p giá»¯a tÃ³m táº¯t vÃ  truy há»“i má»™t cÃ¡ch thÃ­ch á»©ng.

Äá»‘i vá»›i pha **truy há»“i**, háº§u háº¿t cÃ¡c há»‡ thá»‘ng dÃ¹ng **embedding khÃ´ng gian**: lÆ°u cÃ¡c memory dÆ°á»›i dáº¡ng vector nhÃºng vÃ  sá»­ dá»¥ng _khoáº£ng cÃ¡ch ngá»¯ nghÄ©a_ Ä‘á»ƒ tÃ¬m kiáº¿m. Äá»™ chÃ­nh xÃ¡c truy há»“i phá»¥ thuá»™c nhiá»u vÃ o cÃ¡ch biá»ƒu diá»…n vÃ  tá»• chá»©c bá»™ nhá»›. Pan et al. (2024) trong cÃ´ng trÃ¬nh **SeCom** nháº¥n máº¡nh táº§m quan trá»ng cá»§a **â€œÄ‘Æ¡n vá»‹ bá»™ nhá»›â€**: há» so sÃ¡nh lÆ°u trá»¯ theo tá»«ng lÆ°á»£t thoáº¡i, theo tá»«ng phiÃªn, vÃ  theo Ä‘oáº¡n tÃ³m táº¯t, nháº­n tháº¥y má»—i cÃ¡ch cÃ³ Æ°u nhÆ°á»£c Ä‘iá»ƒm riÃªng ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=To%20deliver%20coherent%20and%20personalized,retrieval%20accuracy%20across%20different%20granularities)) ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=Building%20on%20these%20insights%2C%20we,as%20DialSeg711%2C%20TIAGE%2C%20and%20SuperDialSeg)). SeCom Ä‘á» xuáº¥t má»™t chiáº¿n lÆ°á»£c káº¿t há»£p: dÃ¹ng má»™t mÃ´ hÃ¬nh phÃ¢n Ä‘oáº¡n chá»§ Ä‘á» Ä‘á»ƒ chia há»™i thoáº¡i thÃ nh cÃ¡c **Ä‘oáº¡n sá»± kiá»‡n ngáº¯n**, lÆ°u má»—i Ä‘oáº¡n nhÆ° má»™t báº£n ghi bá»™ nhá»›, Ä‘á»“ng thá»i Ã¡p dá»¥ng ká»¹ thuáº­t **â€œnÃ©n thÃ´ng tin nhiá»…uâ€** Ä‘á»ƒ lá»c bá»›t pháº§n khÃ´ng liÃªn quan trong má»—i Ä‘oáº¡n trÆ°á»›c khi lÆ°u ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=Building%20on%20these%20insights%2C%20we,as%20DialSeg711%2C%20TIAGE%2C%20and%20SuperDialSeg)). Káº¿t quáº£, cÃ¡ch lÆ°u trá»¯ theo Ä‘oáº¡n chá»§ Ä‘á» giÃºp tÄƒng cháº¥t lÆ°á»£ng truy há»“i trÃªn cÃ¡c benchmark há»™i thoáº¡i dÃ i nhÆ° LOCOMO, vÃ¬ nÃ³ cÃ¢n báº±ng giá»¯a chi tiáº¿t vÃ  tá»•ng quÃ¡t. BÃªn cáº¡nh Ä‘Ã³, má»™t sá»‘ cáº£i tiáº¿n khÃ¡c gá»“m **truy há»“i theo thá»i gian** (Æ°u tiÃªn cÃ¡c sá»± kiá»‡n gáº§n Ä‘Ã¢y náº¿u cÃ¢u há»i chá»©a má»‘c thá»i gian â€“ xem Wu et al. 2023) hay **má»Ÿ rá»™ng truy váº¥n báº±ng tri thá»©c** (vÃ­ dá»¥ náº¿u há»i â€œanh áº¥yâ€ thÃ¬ truy váº¥n má»Ÿ rá»™ng â€œanh áº¥yâ€ = tÃªn cá»¥ thá»ƒ tá»« bá»™ nhá»›). Nhá»¯ng tá»‘i Æ°u nÃ y Ä‘Ã£ Ä‘Æ°á»£c tá»•ng káº¿t trong nghiÃªn cá»©u LongMemEval, Ä‘á» xuáº¥t khung â€œIndexing-Retrieval-Readingâ€ cho thiáº¿t káº¿ bá»™ nhá»›, trong Ä‘Ã³: **Ä‘Ã¡nh chá»‰ má»¥c** tá»‘i Æ°u báº±ng cÃ¡ch lÆ°u trá»¯ theo phiÃªn nhá» (session decomposition) vÃ  má»Ÿ rá»™ng khÃ³a báº±ng dá»¯ kiá»‡n, **truy há»“i** tá»‘i Æ°u báº±ng cÃ¡ch cÃ¢n nháº¯c ngá»¯ cáº£nh thá»i gian, vÃ  **Ä‘á»c** hiá»‡u quáº£ báº±ng cÃ¡ch káº¿t há»£p bá»™ nhá»› vÃ o input LLM ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=showing%20a%2030,term)).

Gáº§n Ä‘Ã¢y, xuáº¥t hiá»‡n nhá»¯ng há»‡ thá»‘ng trÃ­ nhá»› tiÃªn tiáº¿n táº­n dá»¥ng sá»©c máº¡nh LLM: vÃ­ dá»¥ **MemoryBank** (Zhong et al., 2023) vÃ  **THEANINE** (Ong et al., 2024). MemoryBank tÃ­ch há»£p má»™t **cÆ¡ cháº¿ cáº­p nháº­t bá»™ nhá»› láº¥y cáº£m há»©ng tá»« Ä‘Æ°á»ng cong lÃ£ng quÃªn cá»§a Ebbinghaus** â€“ nghÄ©a lÃ  mÃ´ phá»ng viá»‡c kÃ½ á»©c phai nháº¡t dáº§n theo thá»i gian náº¿u khÃ´ng nháº¯c láº¡i ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=personality%20over%20time%20by%20synthesizing,based%20chatbot%20named)). Cá»¥ thá»ƒ, MemoryBank cho phÃ©p AI â€œquÃªnâ€ bá»›t nhá»¯ng kÃ½ á»©c Ã­t quan trá»ng hoáº·c lÃ¢u khÃ´ng dÃ¹ng, vÃ  **cá»§ng cá»‘** nhá»¯ng kÃ½ á»©c hay Ä‘Æ°á»£c truy xuáº¥t, nhá» Ä‘Ã³ bá»™ nhá»› hoáº¡t Ä‘á»™ng hiá»‡u quáº£ vÃ  giá»‘ng ngÆ°á»i hÆ¡n ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=personality%20over%20time%20by%20synthesizing,based%20chatbot%20named)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=Memory%20Updating)). Há» triá»ƒn khai MemoryBank trÃªn má»™t chatbot báº¡n Ä‘á»“ng hÃ nh (SiliconFriend), cho tháº¥y bot cÃ³ thá»ƒ **tiáº¿p thu vÃ  thÃ­ch nghi vá»›i tÃ­nh cÃ¡ch ngÆ°á»i dÃ¹ng** qua thá»i gian, Ä‘á»“ng thá»i nhá»› Ä‘Æ°á»£c cÃ¡c sá»± kiá»‡n cá»‘t lÃµi trong quÃ¡ khá»© (vÃ­ dá»¥ sá»Ÿ thÃ­ch, má»¥c tiÃªu ngÆ°á»i dÃ¹ng) nhá» cÆ¡ cháº¿ nÃ y ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=psychological%20counseling%2C%20and%20secretarial%20assistance,the%20memory%2C%20thereby%20offering%20a)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=,memory%20works%20through%20repeated%20retrieval)). Trong khi Ä‘Ã³, THEANINE láº¡i chá»n cÃ¡ch **khÃ´ng xÃ³a bá» kÃ½ á»©c cÅ©**, thay vÃ o Ä‘Ã³ quáº£n lÃ½ má»™t **Ä‘á»“ thá»‹ kÃ½ á»©c theo dÃ²ng thá»i gian** ná»‘i cÃ¡c sá»± kiá»‡n theo quan há»‡ nhÃ¢n quáº£ vÃ  thá»i gian ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=to%20improve%20retrieval%20quality%2C%20we,human%20efforts%20when%20assessing%20agent)). Má»—i khi cáº§n táº¡o pháº£n há»“i, mÃ´ hÃ¬nh sáº½ láº§n theo _timeline_ cÃ¡c sá»± kiá»‡n liÃªn quan, táº¡o nÃªn má»™t ngá»¯ cáº£nh diá»…n giáº£i vÃ¬ sao ngÆ°á»i dÃ¹ng cÃ³ tráº¡ng thÃ¡i hiá»‡n táº¡i. CÃ¡ch nÃ y nháº¥n máº¡nh táº§m quan trá»ng cá»§a **ngá»¯ cáº£nh tiáº¿n hÃ³a**: vÃ­ dá»¥, thay vÃ¬ chá»‰ biáº¿t â€œngÆ°á»i dÃ¹ng thÃ­ch du lá»‹châ€, bot cÃ²n biáº¿t _lá»‹ch sá»­_ trÆ°á»›c Ä‘Ã¢y ngÆ°á»i dÃ¹ng Ä‘Ã£ tá»«ng _sá»£ Ä‘i mÃ¡y bay rá»“i sau Ä‘Ã³ má»›i thÃ­ch du lá»‹ch_ â€“ tá»« Ä‘Ã³ pháº£n há»“i tinh táº¿ hÆ¡n. THEANINE cho tháº¥y viá»‡c **liÃªn káº¿t cÃ¡c máº£nh memory** thÃ nh chuá»—i cÃ³ thá»ƒ giÃºp mÃ´ hÃ¬nh hiá»ƒu rÃµ sá»± thay Ä‘á»•i vÃ  nháº¥t quÃ¡n trong tÃ­nh cÃ¡ch ngÆ°á»i dÃ¹ng theo thá»i gian, mÃ  khÃ´ng cáº§n xÃ³a kÃ½ á»©c cÅ© (vá»‘n cÅ©ng mang thÃ´ng tin há»¯u Ã­ch vá» thay Ä‘á»•i hÃ nh vi) ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=constantly%20memorize%20perceived%20information%20and,Along)) ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=conversations,human%20efforts%20when%20assessing%20agent)).

Cuá»‘i cÃ¹ng, framework **LD-Agent** (Hao Li et al., 2024) Ä‘áº¡i diá»‡n cho xu hÆ°á»›ng tÃ­ch há»£p _Ä‘a thÃ nh pháº§n_: há»‡ thá»‘ng nÃ y chia tÃ¡c vá»¥ thÃ nh **3 mÃ´-Ä‘un** Ä‘á»™c láº­p â€“ (i) **nháº­n thá»©c sá»± kiá»‡n** (event perception) Ä‘á»ƒ tÃ³m táº¯t sá»± kiá»‡n chÃ­nh má»—i phiÃªn vÃ o bá»™ nhá»› dÃ i háº¡n, (ii) **trÃ­ch xuáº¥t persona** Ä‘á»™ng cho cáº£ ngÆ°á»i dÃ¹ng vÃ  chatbot, vÃ  (iii) **táº¡o pháº£n há»“i** (response generation) cÃ³ Ä‘iá»u kiá»‡n trÃªn ngá»¯ cáº£nh hiá»‡n táº¡i + bá»™ nhá»› sá»± kiá»‡n truy há»“i + persona Ä‘Ã£ nháº­n diá»‡n ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=the%20Long,Agent%20are)). Bá»™ nhá»› sá»± kiá»‡n cá»§a LD-Agent bao gá»“m hai pháº§n: **bá»™ nhá»› dÃ i háº¡n** chá»©a lá»‹ch sá»­ cÃ¡c sá»± kiá»‡n tÃ³m táº¯t qua nhiá»u phiÃªn (Ä‘Æ°á»£c lÆ°u vá»›i dáº¥u thá»i gian vÃ  phÃ¢n Ä‘oáº¡n theo chá»§ Ä‘á»), vÃ  **bá»™ nhá»› ngáº¯n háº¡n** cho phiÃªn hiá»‡n táº¡i (Ä‘áº£m báº£o thÃ´ng tin má»›i nháº¥t luÃ´n Ä‘Æ°á»£c chÃº trá»ng) ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=the%20Long,Agent%20are)) ([](https://openreview.net/pdf?id=lwCxVgVYoK#:~:text=200%20The%20event%20memory%20module,Specifically%2C%20this%20involves%20recording)). Khi pháº£n há»“i, há»‡ thá»‘ng dÃ¹ng má»™t cÆ¡ cháº¿ truy há»“i theo chá»§ Ä‘á» Ä‘á»ƒ láº¥y ra cÃ¡c sá»± kiá»‡n cÅ© liÃªn quan tá»« bá»™ nhá»› dÃ i háº¡n, káº¿t há»£p vá»›i ná»™i dung ngáº¯n háº¡n, cÃ¹ng vá»›i há»“ sÆ¡ persona Ä‘Ã£ cáº­p nháº­t, rá»“i Ä‘Æ°a vÃ o mÃ´-Ä‘un sinh. CÃ¡ch tiáº¿p cáº­n module hÃ³a nÃ y giÃºp dá»… dÃ ng tinh chá»‰nh tá»«ng pháº§n (vÃ­ dá»¥ thay mÃ´ hÃ¬nh tÃ³m táº¯t sá»± kiá»‡n khÃ¡c tá»‘t hÆ¡n, hoáº·c Ã¡p dá»¥ng ká»¹ thuáº­t LoRA Ä‘á»ƒ cáº­p nháº­t persona linh hoáº¡t), Ä‘á»“ng thá»i cho tháº¥y táº§m quan trá»ng cá»§a viá»‡c **quáº£n lÃ½ Ä‘á»“ng thá»i kiáº¿n thá»©c sá»± kiá»‡n vÃ  thÃ´ng tin cÃ¡ nhÃ¢n** cho Ä‘á»‘i thoáº¡i dÃ i háº¡n. CÃ¡c thÃ­ nghiá»‡m cá»§a LD-Agent chá»‰ ra ráº±ng viá»‡c tÃ­ch há»£p cáº£ hai loáº¡i bá»™ nhá»› (sá»± kiá»‡n + persona) giÃºp chatbot Ä‘áº¡t Ä‘á»™ tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c cao hÆ¡n rÃµ rá»‡t trÃªn nhiá»u benchmark khÃ¡c nhau ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=generation,various%20illustrative%20benchmarks%2C%20models%2C%20and)).

Tá»•ng káº¿t láº¡i, cÃ¡ch tiáº¿p cáº­n nÃ©n vÃ  truy há»“i ngá»¯ cáº£nh hiá»‡n lÃ  hÆ°á»›ng **Æ°u viá»‡t nháº¥t** Ä‘á»ƒ hiá»‡n thá»±c hÃ³a trÃ­ nhá»› dÃ i háº¡n trong Ä‘á»‘i thoáº¡i. NÃ³ táº­n dá»¥ng Ä‘Æ°á»£c sá»©c máº¡nh cá»§a cÃ¡c mÃ´ hÃ¬nh pretrained (báº±ng cÃ¡ch cung cáº¥p cho chÃºng â€œcontext má»Ÿ rá»™ngâ€ khi cáº§n), Ä‘á»“ng thá»i trÃ¡nh Ä‘Æ°á»£c cÃ¡c háº¡n cháº¿ vá» Ä‘á»™ dÃ i vÃ  quÃªn thÃ´ng tin do tá»± mÃ´ hÃ¬nh xá»­ lÃ½. CÃ¡c nghiÃªn cá»©u Ä‘ang tiáº¿p tá»¥c cáº£i tiáº¿n á»Ÿ cáº£ khÃ¢u tÃ³m táº¯t (Ä‘á»ƒ lÆ°u Ä‘Ãºng vÃ  Ä‘á»§ thÃ´ng tin cáº§n nhá»›) láº«n khÃ¢u truy há»“i (Ä‘á»ƒ tÃ¬m chÃ­nh xÃ¡c thÃ´ng tin khi cáº§n Ä‘áº¿n). Pháº§n tiáº¿p theo, chÃºng tÃ´i sáº½ so sÃ¡nh má»™t sá»‘ há»‡ thá»‘ng tiÃªu biá»ƒu thuá»™c hÆ°á»›ng nÃ y vÃ  cÃ¡c baseline liÃªn quan, trÆ°á»›c khi Ä‘i vÃ o Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ trÃªn cÃ¡c benchmark.

# So sÃ¡nh cÃ¡c há»‡ thá»‘ng tiÃªu biá»ƒu cÃ³ bá»™ nhá»› há»™i thoáº¡i

Äá»ƒ minh há»a cá»¥ thá»ƒ sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c hÆ°á»›ng tiáº¿p cáº­n vÃ  hiá»‡u quáº£ cá»§a trÃ­ nhá»› dÃ i háº¡n, báº£ng dÆ°á»›i Ä‘Ã¢y so sÃ¡nh **má»™t sá»‘ há»‡ thá»‘ng tiÃªu biá»ƒu** tá»« trÆ°á»›c Ä‘áº¿n nay:

- **MemNN (Memory Network, 2015)**: ÄÃ¢y lÃ  baseline kiá»ƒu (2) â€“ mÃ´ hÃ¬nh cÃ³ bá»™ nhá»› kháº£ vi. MemNN lÆ°u trá»¯ cÃ¡c phÃ¡t ngÃ´n trÆ°á»›c dÆ°á»›i dáº¡ng vector trong bá»™ nhá»› vÃ  sá»­ dá»¥ng attention Ä‘á»ƒ chá»n ra vector liÃªn quan nháº¥t khi tráº£ lá»i ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=,chaining%20multiple%20supporting%20sentences%20to)). MÃ´ hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng tá»‘t trÃªn cÃ¡c bÃ i toÃ¡n giáº£ láº­p ngáº¯n (nhÆ° bAbI) nhÆ°ng chÆ°a Ä‘Æ°á»£c chá»©ng minh hiá»‡u quáº£ trÃªn Ä‘á»‘i thoáº¡i má»Ÿ phá»©c táº¡p. **Æ¯u Ä‘iá»ƒm**: cÃ³ kháº£ nÄƒng suy luáº­n nhiá»u bÆ°á»›c nhá» Ä‘á»c nhiá»u Ã´ nhá»›; **NhÆ°á»£c Ä‘iá»ƒm**: khÃ³ huáº¥n luyá»‡n end-to-end, khÃ´ng tá»± Ä‘á»™ng cáº­p nháº­t khi thÃ´ng tin thay Ä‘á»•i (cáº§n ghi Ä‘Ã¨ thá»§ cÃ´ng).
    
- **Baseline khÃ´ng nhá»› (No Memory)**: ÄÃ¢y lÃ  há»‡ thá»‘ng kiá»ƒu tráº£ lá»i Ä‘á»™c láº­p tá»«ng lÆ°á»£t, vÃ­ dá»¥ DrQA hoáº·c cÃ¡c model seq2seq khÃ´ng cung cáº¥p lá»‹ch sá»­ vÃ o input. Há»‡ thá»‘ng nÃ y hoÃ n toÃ n _quÃªn_ má»i thá»© sau má»—i lÆ°á»£t, nÃªn **khÃ´ng thá»ƒ** tráº£ lá»i cÃ¡c cÃ¢u há»i phá»¥ thuá»™c ngá»¯ cáº£nh trÆ°á»›c (vd: â€œAnh áº¥yâ€ lÃ  ai?) vÃ  dá»… tráº£ lá»i láº·p láº¡i. Káº¿t quáº£ Ä‘á»‘i thoáº¡i thÆ°á»ng kÃ©m tá»± nhiÃªn vÃ  khÃ´ng duy trÃ¬ Ä‘Æ°á»£c máº¡ch thÃ´ng tin.
    
- **Keep Me Updated (Bae et al., 2022)**: Há»‡ thá»‘ng nÃ y thuá»™c hÆ°á»›ng (3) â€“ dÃ¹ng bá»™ nhá»› ngoÃ i vÄƒn báº£n vá»›i cáº­p nháº­t Ä‘á»™ng. NÃ³ tÃ³m táº¯t thÃ´ng tin ngÆ°á»i dÃ¹ng sau má»—i phiÃªn vÃ  thá»±c hiá»‡n cÃ¡c phÃ©p cáº­p nháº­t (thÃªm/xÃ³a/thay tháº¿) Ä‘á»ƒ bá»™ nhá»› luÃ´n nháº¥t quÃ¡n ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=Specifically%2C%20the%20memory%20management%20mechanism,%E2%80%9CJust%20got%20positive%20results%20from)). **Æ¯u Ä‘iá»ƒm**: Ä‘áº£m báº£o thÃ´ng tin má»›i nháº¥t luÃ´n Ä‘Æ°á»£c ghi nhá»›, trÃ¡nh mÃ¢u thuáº«n (nhá» chiáº¿n lÆ°á»£c cáº­p nháº­t) ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=find%20and%20eliminate%20the%20information,in%20sub%02sequent%20sessions%2C%20a%20relevant)); cho tháº¥y _cÃ ng nhiá»u phiÃªn_ thÃ¬ bot cÃ ng nhá»› tá»‘t hÆ¡n vÃ  tÆ°Æ¡ng tÃ¡c tá»± nhiÃªn hÆ¡n ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=With%20extensive%20experiments%20and%20ablations%2C,date)). **Háº¡n cháº¿**: chá»‰ lÆ°u thÃ´ng tin dÆ°á»›i dáº¡ng vÄƒn báº£n ngáº¯n nÃªn Ä‘Ã´i khi máº¥t chi tiáº¿t, vÃ  chÆ°a xá»­ lÃ½ tá»‘t trÆ°á»ng há»£p nhiá»u thÃ´ng tin khÃ¡c loáº¡i (vÃ¬ táº¥t cáº£ lÆ°u chung má»™t nÆ¡i).
    
- **LD-Agent (Hao Li et al., 2024)**: Äáº¡i diá»‡n tiÃªn tiáº¿n cho hÆ°á»›ng (3) vá»›i cáº¥u trÃºc module hÃ³a. LD-Agent cÃ³ **bá»™ nhá»› hai táº§ng** (dÃ i háº¡n + ngáº¯n háº¡n) vÃ  thÃªm **mÃ´-Ä‘un persona** riÃªng ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=the%20Long,Agent%20are)). Nhá» Ä‘Ã³, nÃ³ khÃ´ng chá»‰ nhá»› sá»± kiá»‡n mÃ  cÃ²n duy trÃ¬ Ä‘Æ°á»£c tÃ­nh cÃ¡ch, thÃ´ng tin nhÃ¢n kháº©u cá»§a cáº£ ngÆ°á»i dÃ¹ng vÃ  agent. **Æ¯u Ä‘iá»ƒm**: kiáº¿n trÃºc linh hoáº¡t, truy há»“i theo chá»§ Ä‘á» giÃºp tÃ¬m Ä‘Ãºng sá»± kiá»‡n; persona Ä‘á»™ng giÃºp Ä‘á»‘i thoáº¡i nháº¥t quÃ¡n vai; Ä‘áº¡t káº¿t quáº£ tá»‘t trÃªn nhiá»u tÃ¡c vá»¥ (há»i Ä‘Ã¡p, trÃ² chuyá»‡n nhiá»u chá»§ Ä‘á») ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=generation,various%20illustrative%20benchmarks%2C%20models%2C%20and)). **NhÆ°á»£c Ä‘iá»ƒm**: phá»©c táº¡p, cáº§n dá»¯ liá»‡u huáº¥n luyá»‡n phong phÃº (vÃ­ dá»¥ dá»¯ liá»‡u gÃ¡n nhÃ£n persona).
    
- **Theanine (NAACL 2025)**: MÃ´ hÃ¬nh nÃ y cÅ©ng thuá»™c (3) nhÆ°ng vá»›i cÃ¡ch quáº£n lÃ½ memory Ä‘áº·c biá»‡t (Ä‘á»“ thá»‹ timeline) ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=to%20improve%20retrieval%20quality%2C%20we,human%20efforts%20when%20assessing%20agent)). **Æ¯u**: khÃ´ng xÃ³a kÃ½ á»©c cÅ©, do Ä‘Ã³ sá»­ dá»¥ng Ä‘Æ°á»£c cáº£ bá»‘i cáº£nh lÃ¢u dÃ i Ä‘á»ƒ suy luáº­n sá»± thay Ä‘á»•i; dÃ¹ng LLM táº¡o _memory timeline_ giÃºp giáº£i thÃ­ch Ä‘Æ°á»£c máº¡ch sá»± kiá»‡n. Tuy nhiÃªn, do khÃ´ng xÃ³a nÃªn **thÃ¡ch thá»©c** lÃ  kiá»ƒm soÃ¡t kÃ­ch thÆ°á»›c bá»™ nhá»› vÃ  trÃ¡nh retrieval nháº§m tá»« nhá»¯ng kÃ½ á»©c quÃ¡ cÅ© khÃ´ng cÃ²n Ä‘Ãºng.
    
- **MemoryBank (AAAI 2023)**: Há»‡ thá»‘ng (3) vá»›i cÆ¡ cháº¿ quÃªn cÃ³ chá»n lá»c. **Æ¯u**: giá»‘ng nÃ£o ngÆ°á»i hÆ¡n â€“ tá»± Ä‘á»™ng lÃ m má» cÃ¡c memory Ã­t quan trá»ng, cá»§ng cá»‘ memory quan trá»ng ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=personality%20over%20time%20by%20synthesizing,based%20chatbot%20named)). NgoÃ i ra, MemoryBank lÆ°u trá»¯ Ä‘a dáº¡ng: _log há»™i thoáº¡i chi tiáº¿t, báº£n tÃ³m táº¯t sá»± kiá»‡n Ä‘á»‹nh ká»³, vÃ  há»“ sÆ¡ ngÆ°á»i dÃ¹ng_ (user portrait) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=Memory%20Storage%3A%20The%20Warehouse%20of,Memories)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=level%20overviews%20of%20daily%20events,tailor%20its%20responses%20over%20time)), do Ä‘Ã³ cung cáº¥p ngá»¯ cáº£nh ráº¥t phong phÃº cho mÃ´ hÃ¬nh. Káº¿t quáº£ cho tháº¥y chatbot tÃ­ch há»£p MemoryBank cÃ³ thá»ƒ **thá»ƒ hiá»‡n sá»± tháº¥u hiá»ƒu vÃ  ghi nhá»›** vÆ°á»£t trá»™i, nhÆ° nhá»› sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng qua nhiá»u tuáº§n lá»… ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=psychological%20counseling%2C%20and%20secretarial%20assistance,the%20memory%2C%20thereby%20offering%20a)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=,tailor%20its%20responses%20over%20time)). Äiá»ƒm cáº§n cáº£i tiáº¿n lÃ  Ä‘áº£m báº£o cÆ¡ cháº¿ quÃªn khÃ´ng vÃ´ tÃ¬nh loáº¡i bá» thÃ´ng tin cáº§n thiáº¿t náº¿u thá»i gian kÃ©o dÃ i (cÃ¢n báº±ng giá»¯a quÃªn vÃ  nhá»› Ä‘Ãºng).
    

NhÃ¬n chung, **xu hÆ°á»›ng phÃ¡t triá»ƒn** cho tháº¥y sá»± chuyá»ƒn dá»‹ch tá»« cÃ¡c mÃ´ hÃ¬nh khÃ´ng nhá»› hoáº·c nhá»› ngáº¯n háº¡n (BiDAF++, DrQA) sang cÃ¡c há»‡ thá»‘ng cÃ³ bá»™ nhá»› ngÃ y cÃ ng thÃ´ng minh hÆ¡n (Keep Me Updated, MemoryBank, Theanine, LD-Agent). Báº£ng so sÃ¡nh trÃªn nháº¥n máº¡nh vai trÃ² cá»§a cÃ¡c thÃ nh pháº§n nhÆ° **cáº­p nháº­t bá»™ nhá»›** (update), **cáº¥u trÃºc hÃ³a thÃ´ng tin** (theo sá»± kiá»‡n, theo persona), cÅ©ng nhÆ° nhá»¯ng phÆ°Æ¡ng phÃ¡p láº¥y cáº£m há»©ng tá»« tÃ¢m lÃ½ há»c (quÃªn cÃ³ chá»n lá»c) Ä‘á»ƒ nÃ¢ng cao cháº¥t lÆ°á»£ng tÆ°Æ¡ng tÃ¡c dÃ i háº¡n. Pháº§n tiáº¿p theo, chÃºng tÃ´i sáº½ giá»›i thiá»‡u cÃ¡c **benchmark vÃ  tiÃªu chÃ­ Ä‘Ã¡nh giÃ¡** Ä‘Æ°á»£c Ä‘á» xuáº¥t nháº±m Ä‘o lÆ°á»ng má»™t cÃ¡ch há»‡ thá»‘ng kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘á»‘i thoáº¡i nÃ y.

# Benchmark vÃ  tiÃªu chÃ­ Ä‘Ã¡nh giÃ¡ trÃ­ nhá»› trong há»™i thoáº¡i

Äá»ƒ Ä‘Ã¡nh giÃ¡ khÃ¡ch quan kháº£ nÄƒng ghi nhá»› vÃ  sá»­ dá»¥ng thÃ´ng tin dÃ i háº¡n, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ xÃ¢y dá»±ng má»™t sá»‘ **benchmark chuyÃªn biá»‡t** cÅ©ng nhÆ° sá»­ dá»¥ng cÃ¡c bá»™ dá»¯ liá»‡u há»™i thoáº¡i cÃ³ yáº¿u tá»‘ nhá»›. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c bá»™ dá»¯ liá»‡u vÃ  tiÃªu chÃ­ ná»•i báº­t:

- **LongMemEval (Wu et al., 2024)** â€“ ÄÃ¢y lÃ  má»™t bá»™ Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n Ä‘áº§u tiÃªn táº­p trung vÃ o **5 ká»¹ nÄƒng trÃ­ nhá»› lÃµi** cá»§a trá»£ lÃ½ chat ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,on%20memorizing%20information%20across%20sustained)). NÄƒm ká»¹ nÄƒng Ä‘Ã³ bao gá»“m: (1) **Nhá»› vÃ  trÃ­ch thÃ´ng tin** (Information Extraction) â€“ kiá»ƒm tra xem mÃ´ hÃ¬nh cÃ³ nhá»› chÃ­nh xÃ¡c cÃ¡c chi tiáº¿t Ä‘Æ°á»£c Ä‘á» cáº­p trÆ°á»›c Ä‘Ã³ hay khÃ´ng; (2) **Suy luáº­n Ä‘a phiÃªn** (Multi-session reasoning) â€“ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng káº¿t ná»‘i thÃ´ng tin qua nhiá»u phiÃªn trÃ² chuyá»‡n rá»i (vÃ­ dá»¥: ngÆ°á»i dÃ¹ng nÃ³i A á»Ÿ tuáº§n trÆ°á»›c vÃ  B á»Ÿ tuáº§n nÃ y, liá»‡u bot cÃ³ káº¿t há»£p A vÃ  B Ä‘á»ƒ tráº£ lá»i?); (3) **Suy luáº­n thá»i gian** (Temporal reasoning) â€“ kiá»ƒm tra hiá»ƒu biáº¿t vá» trÃ¬nh tá»± thá»i gian, nguyÃªn nhÃ¢n-káº¿t quáº£ theo thá»i gian (vÃ­ dá»¥ sá»± kiá»‡n X xáº£y ra sau Y thÃ¬ há»‡ quáº£ ra sao); (4) **Cáº­p nháº­t kiáº¿n thá»©c** (Knowledge updates) â€“ Ä‘Ã¡nh giÃ¡ viá»‡c bot cÃ³ sá»­ dá»¥ng thÃ´ng tin má»›i thay cho thÃ´ng tin cÅ© khi chÃºng mÃ¢u thuáº«n (giá»‘ng bÃ i toÃ¡n cáº­p nháº­t trÃ­ nhá»› COVID á»Ÿ trÃªn); (5) **Abstention (tá»« chá»‘i)** â€“ xem mÃ´ hÃ¬nh cÃ³ biáº¿t tá»« chá»‘i tráº£ lá»i khi khÃ´ng cháº¯c do thiáº¿u trÃ­ nhá»› hay khÃ´ng (trÃ¡nh trÆ°á»ng há»£p Ä‘oÃ¡n bá»«a/hallucinate) ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,term)). LongMemEval gá»“m 500 cÃ¢u há»i Ä‘Æ°á»£c gÃ i cáº©n tháº­n vÃ o cÃ¡c lá»‹ch sá»­ há»™i thoáº¡i dÃ i, má»—i cÃ¢u há»i tÆ°Æ¡ng á»©ng kiá»ƒm tra má»™t khÃ­a cáº¡nh trÃªn. Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y cÃ¡c chatbot hiá»‡n táº¡i (ká»ƒ cáº£ mÃ´ hÃ¬nh lá»›n vá»›i ngá»¯ cáº£nh dÃ i) **giáº£m hiá»‡u suáº¥t tá»›i ~30%** khi pháº£i ghi nhá»› thÃ´ng tin tráº£i dÃ i, so vá»›i cÃ¡c cÃ¢u há»i ngáº¯n háº¡n ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=meticulously%20curated%20questions%20embedded%20within,augmented%20key)). Äiá»u nÃ y kháº³ng Ä‘á»‹nh Ä‘á»™ khÃ³ cá»§a bÃ i toÃ¡n vÃ  sá»± cáº§n thiáº¿t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p memory augmentation. LongMemEval hiá»‡n Ä‘Æ°á»£c coi lÃ  thÆ°á»›c Ä‘o tiÃªu chuáº©n, khuyáº¿n khÃ­ch cÃ¡c nghiÃªn cá»©u tÆ°Æ¡ng lai cáº£i thiá»‡n cáº£ 5 ká»¹ nÄƒng ká»ƒ trÃªn Ä‘á»ƒ tiáº¿n tá»›i trá»£ lÃ½ Ä‘á»‘i thoáº¡i Ä‘Ã¡ng tin cáº­y hÆ¡n ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=interactions,term)).
    
- **LOCOMO (Maharana et al., 2024)** â€“ LÃ  viáº¿t táº¯t cá»§a _Long Conversation Model_, Ä‘Ã¢y Ä‘Æ°á»£c bÃ¡o cÃ¡o lÃ  bá»™ dá»¯ liá»‡u há»™i thoáº¡i _dÃ i nháº¥t_ hiá»‡n nay, vá»›i trung bÃ¬nh **300 lÆ°á»£t thoáº¡i (9k token)** má»—i há»™i thoáº¡i ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=%28i%29%20LOCOMO%C2%A0%28Maharana%20et%C2%A0al,on%20the%20recently%20released%20official)). LOCOMO mÃ´ phá»ng cÃ¡c cuá»™c trÃ² chuyá»‡n liÃªn tá»¥c, nhiá»u chá»§ Ä‘á», Ä‘Ã²i há»i mÃ´ hÃ¬nh pháº£i duy trÃ¬ tÆ°Æ¡ng tÃ¡c máº¡ch láº¡c trong thá»i gian ráº¥t dÃ i. Äá»ƒ Ä‘Ã¡nh giÃ¡, tÃ¡c giáº£ dÃ¹ng GPT-4 sinh ra cÃ¡c cÃ¢u há»i kiá»ƒm tra vá» ná»™i dung Ä‘Ã£ nÃ³i tá»« ráº¥t sá»›m trong phiÃªn, nháº±m xem bot cÃ³ nhá»› hay khÃ´ng ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=%28i%29%20LOCOMO%C2%A0%28Maharana%20et%C2%A0al,on%20the%20recently%20released%20official)). NgoÃ i ra, LOCOMO cÃ²n Ä‘o lÆ°á»ng má»©c Ä‘á»™ trÃ´i cháº£y vÃ  nháº¥t quÃ¡n qua thÆ°á»›c Ä‘o **GPT4Score** vÃ  cÃ¡c chá»‰ sá»‘ ngÃ´n ngá»¯ tá»± nhiÃªn (BLEU, ROUGE) cho pháº£n há»“i cá»§a mÃ´ hÃ¬nh ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=long,in%20performance%20improvements%20up%20to)) ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=Methods%20LOCOMO%20Long,44)). CÃ¹ng vá»›i LOCOMO, má»™t sá»‘ biáº¿n thá»ƒ nhÆ° **Long-MT-Bench+** cÅ©ng Ä‘Æ°á»£c dÃ¹ng â€“ Ä‘Ã¢y lÃ  má»Ÿ rá»™ng cá»§a bá»™ Ä‘Ã¡nh giÃ¡ Multi-Turn Dialogue (MT-Bench) dÃ nh riÃªng cho há»™i thoáº¡i dÃ i. CÃ¡c káº¿t quáº£ baseline trÃªn LOCOMO cho tháº¥y náº¿u mÃ´ hÃ¬nh chá»‰ dÃ¹ng lá»‹ch sá»­ ráº¥t ngáº¯n (hoáº·c khÃ´ng lá»‹ch sá»­) thÃ¬ Ä‘iá»ƒm sá»‘ tráº£ lá»i Ä‘Ãºng ráº¥t tháº¥p (~25-50), trong khi dÃ¹ng full history nÃ¢ng lÃªn ~54 ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=match%20at%20L389%20LOCOMO%20Zero,77%203%2C288)). Tuy nhiÃªn, dÃ¹ng full history phiáº¿n diá»‡n cÅ©ng gÃ¢y má»i model (13,000 token) vÃ  khÃ´ng nháº¥t thiáº¿t tá»‘i Æ°u. Do váº­y LOCOMO Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ thá»­ nghiá»‡m cÃ¡c chiáº¿n lÆ°á»£c nhá»›: thÃ­ dá»¥ SeCom trÃªn LOCOMO Ä‘áº¡t **GPT4Score ~69**, cao hÆ¡n háº³n so vá»›i mÃ´ hÃ¬nh khÃ´ng module nhá»› (~24) ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=LOCOMO%20Zero%20History%2024,77%203%2C288)) ([On Memory Construction and Retrieval for Personalized Conversational Agents](https://arxiv.org/html/2502.05589v2#:~:text=Methods%20LOCOMO%20Long,44)). Äiá»u nÃ y xÃ¡c nháº­n lá»£i Ã­ch rÃµ rá»‡t cá»§a memory Ä‘á»‘i vá»›i há»™i thoáº¡i siÃªu dÃ i.
    
- **CÃ¡c bá»™ dá»¯ liá»‡u personalized vÃ  multi-session**: TrÆ°á»›c khi cÃ³ cÃ¡c benchmark trÃªn, má»™t sá»‘ bá»™ dá»¯ liá»‡u há»™i thoáº¡i Ä‘Æ°á»£c táº¡o ra nháº±m kiá»ƒm tra má»™t pháº§n khÃ­a cáº¡nh cá»§a trÃ­ nhá»›. **Persona-Chat (Zhang et al., 2018)** cung cáº¥p cho má»—i nhÃ¢n váº­t má»™t há»“ sÆ¡ sá»Ÿ thÃ­ch (5 cÃ¢u mÃ´ táº£) vÃ  yÃªu cáº§u mÃ´ hÃ¬nh trÃ² chuyá»‡n giá»¯ Ä‘Ãºng persona nÃ y. ÄÃ¢y lÃ  kiá»ƒm tra kháº£ nÄƒng **nhá»› thÃ´ng tin há»“ sÆ¡ tÄ©nh** â€“ gáº§n vá»›i memory ngáº¯n háº¡n (vÃ¬ persona khÃ´ng Ä‘á»•i). **MuTual (Cui et al., 2020)** vÃ  **DSTC7,8** cung cáº¥p cÃ¡c Ä‘oáº¡n há»™i thoáº¡i yÃªu cáº§u suy luáº­n logic giá»¯a cÃ¡c lÆ°á»£t â€“ giÃ¡n tiáº¿p Ä‘Ã²i há»i nhá»› ná»™i dung trÆ°á»›c. **QuAC, CoQA (2018)** nhÆ° Ä‘Ã£ Ä‘á» cáº­p, Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng tráº£ lá»i dá»±a vÃ o nhiá»u lÆ°á»£t há»i trÆ°á»›c (context co-reference). Tuy nhiÃªn, cÃ¡c dataset nÃ y thÆ°á»ng chá»‰ kÃ©o dÃ i tá»‘i Ä‘a vÃ i chá»¥c lÆ°á»£t trong má»™t phiÃªn, vÃ  khÃ´ng Ä‘Ã¡nh giÃ¡ xuyÃªn phiÃªn hay cáº­p nháº­t. Gáº§n Ä‘Ã¢y, má»™t sá»‘ dataset hÆ°á»›ng Ä‘áº¿n **Ä‘a phiÃªn**: vÃ­ dá»¥ **MSC (Multi-Session Chat)** (Xu et al., 2022) ná»‘i 2-3 phiÃªn PersonaChat láº¡i Ä‘á»ƒ xem bot cÃ³ nhá»› thÃ´ng tin giá»¯a cÃ¡c phiÃªn; hay **CareCall-Mem** (Bae et al., 2022) â€“ dá»¯ liá»‡u tiáº¿ng HÃ n mÃ  nhÃ³m Keep Me Updated xÃ¢y dá»±ng â€“ gá»“m 5 phiÃªn trÃ² chuyá»‡n giá»¯a bot vÃ  má»™t ngÆ°á»i dÃ¹ng hÆ° cáº¥u vá»›i cÃ¡c thÃ´ng tin cÃ¡ nhÃ¢n thay Ä‘á»•i theo thá»i gian (sá»©c khá»e, thÃ³i quen) ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=3,We%20extend%20this)) ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=single,Sessions%207%2C665%20Session%201%202%2C812)). CÃ¡c dataset nÃ y phá»¥c vá»¥ huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trong bá»‘i cáº£nh **thÃ´ng tin ngÆ°á»i dÃ¹ng thay Ä‘á»•i**: vÃ­ dá»¥ phiÃªn 1 nÃ³i â€œghÃ©t váº­n Ä‘á»™ngâ€, phiÃªn 3 láº¡i nÃ³i â€œÄ‘ang há»c bÆ¡iâ€ thÃ¬ bot pháº£i hiá»ƒu sá»Ÿ thÃ­ch Ä‘Ã£ thay Ä‘á»•i. TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡ gá»“m Ä‘á»™ tá»± nhiÃªn, tÃ­nh gáº¯n káº¿t, vÃ  quan trá»ng lÃ  **Ä‘á»™ chÃ­nh xÃ¡c cá»§a thÃ´ng tin** mÃ  bot nÃ³i ra so vá»›i há»“ sÆ¡ thá»±c táº¿ (trÃ¡nh nháº§m thÃ´ng tin cÅ©).
    
- **TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡**: Dá»±a trÃªn cÃ¡c benchmark trÃªn, ta cÃ³ thá»ƒ liá»‡t kÃª nhá»¯ng tiÃªu chÃ­ chÃ­nh Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng trÃ­ nhá»› dÃ i háº¡n cá»§a há»‡ thá»‘ng há»™i thoáº¡i:
    
    - _ChÃ­nh xÃ¡c thÃ´ng tin Ä‘Ã£ nhá»› (Memory Recall)_: Kiá»ƒm tra tá»‰ lá»‡ thÃ´ng tin Ä‘Ãºng Ä‘Æ°á»£c bot nháº¯c láº¡i khi cáº§n. VÃ­ dá»¥, user Ä‘Ã£ nÃ³i há» sinh nÄƒm 1990, sau 10 lÆ°á»£t bot Ä‘á» cáº­p láº¡i Ä‘Ãºng nÄƒm sinh hay khÃ´ng. TiÃªu chÃ­ nÃ y Ä‘o báº±ng cÃ¢u há»i trá»±c tiáº¿p (nhÆ° LongMemEval) hoáº·c so khá»›p vá»›i log quÃ¡ khá»©.
        
    - _Pháº£n há»“i nháº¥t quÃ¡n, khÃ´ng áº£o giÃ¡c (Consistency & No-hallucination)_: ÄÃ¡nh giÃ¡ xem bot cÃ³ mÃ¢u thuáº«n vá»›i chÃ­nh nÃ³ hoáº·c vá»›i thá»±c táº¿ Ä‘Ã£ biáº¿t khÃ´ng, vÃ  cÃ³ bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong bá»™ nhá»› khÃ´ng. Náº¿u bot _quÃªn_ má»™t chi tiáº¿t vÃ  tá»± cháº¿ ra, Ä‘Ã³ lÃ  Ä‘iá»ƒm trá»« lá»›n. ThÆ°á»›c Ä‘o cÃ³ thá»ƒ báº±ng kiá»ƒm tra logic (vÃ­ dá»¥ Persona-Chat yÃªu cáº§u khÃ´ng nÃ³i sai persona), hoáº·c nhá» Ä‘Ã¡nh giÃ¡ cá»§a mÃ´ hÃ¬nh/human xem cÃ¢u tráº£ lá»i cÃ³ cÄƒn cá»© quÃ¡ khá»© hay khÃ´ng.
        
    - _Cáº­p nháº­t kiáº¿n thá»©c ká»‹p thá»i (Knowledge Update Accuracy)_: Khi ngÆ°á»i dÃ¹ng cung cáº¥p thÃ´ng tin má»›i hoáº·c Ä‘Ã­nh chÃ­nh, bot cÃ³ pháº£n Ã¡nh Ä‘Ãºng sá»± thay Ä‘á»•i trong cÃ¡c lÆ°á»£t sau khÃ´ng. TiÃªu chÃ­ nÃ y thÆ°á»ng Ä‘Ã¡nh giÃ¡ theo ká»‹ch báº£n: vÃ­ dá»¥ nhÆ° bÃ i toÃ¡n COVID test á»Ÿ trÃªn â€“ sau khi user bÃ¡o dÆ°Æ¡ng tÃ­nh, bot pháº£i quÃªn thÃ´ng tin â€œchÆ°a xÃ©t nghiá»‡mâ€ trÆ°á»›c Ä‘Ã³. CÃ³ thá»ƒ Ä‘o báº±ng truy váº¥n sau update xem bot tráº£ lá»i dá»±a trÃªn thÃ´ng tin nÃ o.
        
    - _Suy luáº­n theo dÃ²ng thá»i gian (Temporal Reasoning)_: Bot cÃ³ hiá»ƒu má»‘i quan há»‡ thá»i gian giá»¯a cÃ¡c sá»± kiá»‡n trong trÃ­ nhá»› khÃ´ng. VÃ­ dá»¥, user nÃ³i â€œnÄƒm 2020 tÃ´i tá»‘t nghiá»‡pâ€, sau Ä‘Ã³ há»i â€œ2 nÄƒm sau tÃ´i lÃ m gÃ¬â€ â€“ bot pháº£i biáº¿t 2 nÄƒm sau 2020 lÃ  2022 vÃ  tÃ¬m trong memory xem 2022 cÃ³ sá»± kiá»‡n gÃ¬ (hoáº·c tráº£ lá»i chÆ°a biáº¿t náº¿u khÃ´ng cÃ³). Kháº£ nÄƒng nÃ y thÆ°á»ng Ä‘o báº±ng cÃ¡c cÃ¢u há»i yÃªu cáº§u káº¿t há»£p má»‘c thá»i gian (nhÆ° trong LongMemEval) ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,on%20memorizing%20information%20across%20sustained)).
        
    - _Kháº£ nÄƒng tá»« chá»‘i khi khÃ´ng nhá»› (Abstention)_: Má»™t há»‡ thá»‘ng tá»‘t cáº§n biáº¿t giá»›i háº¡n trÃ­ nhá»› cá»§a mÃ¬nh, tá»©c lÃ  náº¿u thÃ´ng tin khÃ´ng cÃ³ trong bá»™ nhá»› thÃ¬ nÃªn xin lá»—i hoáº·c tá»« chá»‘i hÆ¡n lÃ  bá»‹a. TiÃªu chÃ­ nÃ y Ä‘Ã¡nh giÃ¡ tá»· lá»‡ bot **khÃ´ng Ä‘oÃ¡n bá»«a**. LongMemEval Ä‘Æ°a ra cÃ¡c tÃ¬nh huá»‘ng mÃ  cÃ¢u há»i ngoÃ i pháº¡m vi nhá»¯ng gÃ¬ Ä‘Ã£ nÃ³i, yÃªu cáº§u bot pháº£i pháº£n há»“i kiá»ƒu â€œTÃ´i khÃ´ng nhá»› rÃµâ€¦â€ thay vÃ¬ cung cáº¥p thÃ´ng tin sai ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,term)).
        

NgoÃ i ra, cÃ¡c tiÃªu chÃ­ tá»•ng quan nhÆ° **Ä‘á»™ hÃ i lÃ²ng ngÆ°á»i dÃ¹ng, Ä‘á»™ tá»± nhiÃªn cá»§a há»™i thoáº¡i, Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ cá»§a giÃ¡m kháº£o** cÅ©ng ráº¥t quan trá»ng, nhÆ°ng chÃºng chá»‹u áº£nh hÆ°á»Ÿng nhiá»u yáº¿u tá»‘ ngoÃ i trÃ­ nhá»› (nhÆ° ká»¹ nÄƒng ngÃ´n ngá»¯ chung cá»§a mÃ´ hÃ¬nh). Do Ä‘Ã³, cÃ¡c benchmark chuyÃªn biá»‡t cá»‘ gáº¯ng cÃ´ láº­p áº£nh hÆ°á»Ÿng cá»§a trÃ­ nhá»› Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cÃ´ng báº±ng giá»¯a cÃ¡c giáº£i phÃ¡p.

# HÆ°á»›ng má»Ÿ rá»™ng vÃ  káº¿t luáº­n

**TrÃ­ nhá»› dÃ i háº¡n cho há»‡ thá»‘ng Ä‘á»‘i thoáº¡i** váº«n lÃ  má»™t bÃ i toÃ¡n má»Ÿ vá»›i nhiá»u hÆ°á»›ng nghiÃªn cá»©u tiá»m nÄƒng. Dá»±a trÃªn cÃ¡c xu hÆ°á»›ng hiá»‡n táº¡i, cÃ³ thá»ƒ gá»£i Ã½ má»™t sá»‘ hÆ°á»›ng phÃ¡t triá»ƒn chÃ­nh sau:

- **Káº¿t há»£p cháº·t cháº½ giá»¯a truy há»“i vÃ  cáº­p nháº­t tri thá»©c**: Hiá»‡n nay, retrieval augmented generation (RAG) Ä‘Ã£ phá»• biáº¿n trong QA má»Ÿ, nhÆ°ng thÆ°á»ng vá»›i _knowledge base_ tÄ©nh. Má»Ÿ rá»™ng hÆ¡n, ta cÃ³ thá»ƒ tÃ­ch há»£p RAG vÃ o Ä‘á»‘i thoáº¡i sao cho **kho tri thá»©c Ä‘Æ°á»£c cáº­p nháº­t liÃªn tá»¥c trong quÃ¡ trÃ¬nh trÃ² chuyá»‡n**. VÃ­ dá»¥, khi ngÆ°á»i dÃ¹ng cung cáº¥p má»™t thÃ´ng tin má»›i, há»‡ thá»‘ng ngay láº­p tá»©c thÃªm nÃ³ vÃ o _bá»™ nhá»› tri thá»©c_ vÃ  cÃ¡c lÆ°á»£t sau truy há»“i cÃ³ thá»ƒ láº¥y ra. Äiá»u nÃ y Ä‘Ã²i há»i giáº£i quyáº¿t bÃ i toÃ¡n Ä‘á»“ng bá»™ giá»¯a thÃ nh pháº§n ghi nhá»› vÃ  thÃ nh pháº§n tÃ¬m kiáº¿m. Má»™t hÆ°á»›ng lÃ  phÃ¡t triá»ƒn cÃ¡c phÆ°Æ¡ng phÃ¡p **index Ä‘á»™ng**: cáº­p nháº­t chá»‰ má»¥c bá»™ nhá»› theo thá»i gian thá»±c, hoáº·c sá»­ dá»¥ng mÃ´ hÃ¬nh há»c tÄƒng cÆ°á»ng Ä‘á»ƒ quyáº¿t Ä‘á»‹nh khi nÃ o cáº§n _re-index_.
    
- **Truy há»“i thÃ­ch á»©ng vÃ  cÃ³ hÆ°á»›ng dáº«n**: Thay vÃ¬ luÃ´n truy há»“i má»™t cÃ¡ch mÃ¡y mÃ³c top-k Ä‘oáº¡n giá»‘ng nhÆ° hiá»‡n nay, mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c cÃ¡ch **Ä‘áº·t truy váº¥n thÃ´ng minh** hoáº·c **chá»n lá»c** tÃ¹y tÃ¬nh huá»‘ng. Cháº³ng háº¡n, náº¿u cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng ráº¥t rÃµ rÃ ng (nhÆ° há»i tÃªn Ä‘Ã£ cho trÆ°á»›c Ä‘Ã³), má»™t truy váº¥n tháº³ng sáº½ hiá»‡u quáº£; nhÆ°ng náº¿u cÃ¢u há»i mÆ¡ há»“, mÃ´ hÃ¬nh cÃ³ thá»ƒ tá»± sinh ra má»™t truy váº¥n rÃµ hÆ¡n dá»±a trÃªn ngá»¯ cáº£nh â€“ tÆ°Æ¡ng tá»± ká»¹ thuáº­t _query rewriting_ ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=Query%20Rewriting)). NgoÃ i ra, mÃ´ hÃ¬nh nÃªn há»c _khi nÃ o_ thÃ¬ cáº§n truy há»“i: Ä‘Ã´i khi, cÃ¢u há»i hiá»‡n táº¡i khÃ´ng liÃªn quan gÃ¬ Ä‘áº¿n quÃ¡ khá»©, viá»‡c truy há»“i chá»‰ thÃªm nhiá»…u. CÃ³ thá»ƒ dÃ¹ng má»™t module phá»¥ (nhÆ° má»™t classifier) Ä‘á»ƒ quyáº¿t Ä‘á»‹nh cÃ³ truy há»“i memory khÃ´ng á»Ÿ má»—i lÆ°á»£t. Má»™t Ã½ tÆ°á»Ÿng khÃ¡c lÃ  cho chÃ­nh LLM **hÆ°á»›ng dáº«n viá»‡c truy há»“i**: vÃ­ dá»¥ trÆ°á»›c khi tráº£ lá»i, mÃ´ hÃ¬nh tá»± suy luáº­n "Äá»ƒ tráº£ lá»i, tÃ´i cáº§n nhá»› X", sau Ä‘Ã³ dÃ¹ng suy luáº­n nÃ y lÃ m chÃ¬a khÃ³a tÃ¬m kiáº¿m bá»™ nhá»›. ÄÃ¢y lÃ  má»™t dáº¡ng _chain-of-thought for retrieval_ Ä‘áº§y há»©a háº¹n.
    
- **Sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ phá»¥ trá»£ cho quáº£n lÃ½ trÃ­ nhá»›**: Thay vÃ¬ cÃ¡c rule cá»©ng (nhÆ° 4 thao tÃ¡c cá»§a Keep Me Updated), ta cÃ³ thá»ƒ dÃ¹ng má»™t LLM nhá» hoáº·c cÃ¡c prompt Ä‘áº·c biá»‡t cho chÃ­nh LLM lá»›n Ä‘á»ƒ quáº£n lÃ½ memory. VÃ­ dá»¥, cÃ³ thá»ƒ triá»ƒn khai má»™t _â€œMemory Manager Agentâ€_ cháº¡y song song: agent nÃ y dÃ¹ng LLM Ä‘á»ƒ Ä‘á»‹nh ká»³ Ä‘á»c lá»‹ch sá»­ vÃ  viáº¿t tÃ³m táº¯t, lÆ°u vÃ o vector DB; khi cáº§n thÃ¬ há»— trá»£ truy váº¥n vector DB vÃ  cung cáº¥p káº¿t quáº£ cho LLM chÃ­nh. CÃ¡ch tiáº¿p cáº­n kiáº¿n trÃºc agent nÃ y Ä‘Ã£ Ä‘Æ°á»£c Park et al. (2023) thá»­ nghiá»‡m trong **Generative Agents**, nÆ¡i nhiá»u agent LLM tÆ°Æ¡ng tÃ¡c vá»›i nhau vÃ  cÃ³ bá»™ nhá»› sá»± kiá»‡n Ä‘Æ°á»£c ghi láº¡i vÃ  suy diá»…n báº±ng LLM. Má»™t á»©ng dá»¥ng khÃ¡c lÃ  dÃ¹ng LLM Ä‘á»ƒ **Ä‘Ã¡nh giÃ¡ vÃ  chá»‰nh sá»­a** memory: vÃ­ dá»¥ dÃ¹ng GPT-4 Ä‘á»c toÃ n bá»™ memory log vÃ  phÃ¡t hiá»‡n mÃ¢u thuáº«n hoáº·c lá»—i Ä‘á»ƒ sá»­a (má»™t dáº¡ng reviewer). NhÃ¬n chung, táº­n dá»¥ng kháº£ nÄƒng ngÃ´n ngá»¯ Ä‘a nÄƒng cá»§a LLM cho viá»‡c quáº£n trá»‹ trÃ­ nhá»› cÃ³ thá»ƒ Ä‘em láº¡i linh hoáº¡t hÆ¡n so vá»›i cÃ¡ch lÃ m thuáº§n heuristic.
    
- **Má»Ÿ rá»™ng sang Ä‘a mÃ´ hÃ¬nh vÃ  tri thá»©c tháº¿ giá»›i**: TrÃ­ nhá»› há»™i thoáº¡i khÃ´ng chá»‰ gá»“m lá»i thoáº¡i â€“ trong nhiá»u á»©ng dá»¥ng, nÃ³ cáº§n nhá»› cáº£ cÃ¡c **thÃ´ng tin thá»‹ giÃ¡c, cáº£m biáº¿n, hay tri thá»©c ngoÃ i**. HÆ°á»›ng má»Ÿ lÃ  tÃ­ch há»£p **bá»™ nhá»› chung cho Ä‘a mÃ´ hÃ¬nh**: vÃ­ dá»¥ má»™t robot trá»£ lÃ½ nhÃ  thÃ´ng minh cáº§n nhá»› hÃ´m qua camera tháº¥y gÃ¬, ai Ä‘Ã£ ghÃ© thÄƒm, Ä‘á»“ váº­t Ä‘áº·t á»Ÿ Ä‘Ã¢u... cÃ¹ng vá»›i há»™i thoáº¡i vá»›i chá»§ nhÃ . Äiá»u nÃ y Ä‘áº·t ra bÃ i toÃ¡n lÆ°u trá»¯ vÃ  truy há»“i cÃ¡c **Ä‘áº¡i diá»‡n Ä‘a mÃ´ hÃ¬nh** (hÃ¬nh áº£nh, Ã¢m thanh) bÃªn cáº¡nh vÄƒn báº£n. TÆ°Æ¡ng tá»±, káº¿t há»£p **knowledge graph** hoáº·c cÆ¡ sá»Ÿ tri thá»©c vÃ o memory: vÃ­ dá»¥ khi ngÆ°á»i dÃ¹ng nÃ³i sá»Ÿ thÃ­ch, bot cÃ³ thá»ƒ lÆ°u vÃ o má»™t _knowledge graph node_ vá» ngÆ°á»i dÃ¹ng, liÃªn káº¿t vá»›i cÃ¡c node hoáº¡t Ä‘á»™ng tÆ°Æ¡ng á»©ng. Viá»‡c káº¿t há»£p cáº¥u trÃºc tri thá»©c cÃ³ thá»ƒ giÃºp bot suy luáº­n logic vÃ  nháº¥t quÃ¡n hÆ¡n (trÃ¡nh mÃ¢u thuáº«n thá»±c táº¿). Má»™t hÆ°á»›ng lÃ  má»—i khi memory update, Ä‘á»“ng thá»i cáº­p nháº­t knowledge graph, vÃ  dÃ¹ng graph embedding Ä‘á»ƒ há»— trá»£ retrieval song song.
    
- **ÄÃ¡nh giÃ¡ vÃ  giáº£m thiá»ƒu nhiá»…u do trÃ­ nhá»› sai**: Khi tÃ­ch há»£p bá»™ nhá»›, má»™t nguy cÆ¡ lÃ  _nhá»› sai hoáº·c nhá»› mÆ¡ há»“_ cÃ³ thá»ƒ dáº«n Ä‘áº¿n pháº£n há»“i sai (hallucination do memory). Do Ä‘Ã³, cáº§n cÆ¡ cháº¿ **Ä‘Ã¡nh giÃ¡ Ä‘á»™ tin cáº­y cá»§a memory**. Má»™t hÆ°á»›ng lÃ  kÃ¨m theo má»—i máº©u memory má»™t Ä‘á»™ tin cáº­y (confidence score) vÃ  thá»i gian, Ä‘á»ƒ mÃ´ hÃ¬nh Æ°u tiÃªn dÃ¹ng thÃ´ng tin má»›i vÃ  cÃ³ Ä‘á»™ tin cáº­y cao. Náº¿u memory quÃ¡ cÅ©, mÃ´ hÃ¬nh cÃ³ thá»ƒ cáº£nh bÃ¡o. Má»™t hÆ°á»›ng khÃ¡c lÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh **phÃ¡t hiá»‡n mÃ¢u thuáº«n** giá»¯a memory vÃ  message hiá»‡n táº¡i: náº¿u phÃ¡t hiá»‡n user nÃ³i Ä‘iá»u trÃ¡i ngÆ°á»£c háº³n vá»›i memory cÅ©, cÃ³ thá»ƒ kÃ­ch hoáº¡t má»™t _quy trÃ¬nh xÃ¡c minh_, há»i láº¡i ngÆ°á»i dÃ¹ng Ä‘á»ƒ cháº¯c cháº¯n trÆ°á»›c khi cáº­p nháº­t.
    

TÃ³m láº¡i, **há»‡ thá»‘ng Ä‘á»‘i thoáº¡i tÃ­ch há»£p trÃ­ nhá»› dÃ i háº¡n** Ä‘ang dáº§n trá»Ÿ nÃªn kháº£ thi nhá» cÃ¡c tiáº¿n bá»™ trong cáº£ mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n láº«n ká»¹ thuáº­t quáº£n lÃ½ tri thá»©c. Tá»« nhá»¯ng mÃ´ hÃ¬nh QA Ä‘Æ¡n lÆ°á»£t Ä‘Æ¡n giáº£n, chÃºng ta Ä‘Ã£ chá»©ng kiáº¿n sá»± ra Ä‘á»i cá»§a cÃ¡c chatbot cÃ³ kháº£ nÄƒng ghi nhá»› hÃ ng trÄƒm lÆ°á»£t thoáº¡i, cÃ¡ nhÃ¢n hÃ³a theo ngÆ°á»i dÃ¹ng, vÃ  cáº­p nháº­t hiá»ƒu biáº¿t theo thá»i gian. DÃ¹ váº«n cÃ²n nhá»¯ng thÃ¡ch thá»©c vá» tá»‘i Æ°u vÃ  Ä‘á»™ tin cáº­y, hÆ°á»›ng nghiÃªn cá»©u nÃ y há»©a háº¹n Ä‘em láº¡i cÃ¡c trá»£ lÃ½ áº£o **nhá»› lÃ¢u, hiá»ƒu sÃ¢u vÃ  pháº£n há»“i tá»± nhiÃªn** hÆ¡n â€“ má»™t bÆ°á»›c tiáº¿n lá»›n tá»›i **AI Ä‘á»‘i thoáº¡i mang tÃ­nh cÃ¡ nhÃ¢n vÃ  Ä‘Ã¡ng tin cáº­y** trong tÆ°Æ¡ng lai gáº§n. CÃ¡c nghiÃªn cá»©u má»›i nhÆ° LongMemEval Ä‘ang táº¡o ná»n táº£ng Ä‘á»ƒ **Ä‘Ã¡nh giÃ¡ cÃ³ há»‡ thá»‘ng** cÃ¡c tiáº¿n bá»™, cÃ²n cÃ¡c Ã½ tÆ°á»Ÿng káº¿t há»£p memory vÃ  LLM (MemoryBank, THEANINE, LD-Agent) Ä‘ang má»Ÿ Ä‘Æ°á»ng cho tháº¿ há»‡ mÃ´ hÃ¬nh há»™i thoáº¡i thÃ´ng minh káº¿ tiáº¿p. ChÃºng ta cÃ³ thá»ƒ ká»³ vá»ng trong tÆ°Æ¡ng lai, sá»± káº¿t há»£p giá»¯a **cá»­a sá»• ngá»¯ cáº£nh lá»›n** vÃ  **bá»™ nhá»› ngoÃ i linh hoáº¡t** sáº½ giÃºp xÃ³a nhÃ²a ranh giá»›i vá» trÃ­ nhá»› trong Ä‘á»‘i thoáº¡i, cho phÃ©p cÃ¡c há»‡ thá»‘ng AI trÃ² chuyá»‡n má»™t cÃ¡ch máº¡ch láº¡c vÃ  hiá»ƒu biáº¿t qua _nhiá»u thÃ¡ng, nhiá»u nÄƒm_ tÆ°Æ¡ng tÃ¡c vá»›i con ngÆ°á»i.

**TÃ i liá»‡u tham kháº£o:**

1. Wu, D. _et al._ (2024). _LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory_. **ICLR 2025 (preprint)** ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=capabilities%20in%20sustained%20interactions%20remain,term)) ([[2410.10813] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://arxiv.org/abs/2410.10813#:~:text=meticulously%20curated%20questions%20embedded%20within,augmented%20key)).
    
2. Qu, C. _et al._ (2020). _Open-Retrieval Conversational Question Answering_. **SIGIR 2020** ([[2005.11364] Open-Retrieval Conversational Question Answering](https://arxiv.org/abs/2005.11364#:~:text=retrieval%20conversational%20question%20answering%20,the%20reranker%20component%20contributes%20to)).
    
3. Bae, S. _et al._ (2022). _Keep Me Updated! Memory Management in Long-term Conversations_. **Findings of EMNLP 2022** ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=Specifically%2C%20the%20memory%20management%20mechanism,%E2%80%9CJust%20got%20positive%20results%20from)) ([](https://aclanthology.org/2022.findings-emnlp.276.pdf#:~:text=find%20and%20eliminate%20the%20information,in%20sub%02sequent%20sessions%2C%20a%20relevant)).
    
4. Li, H. _et al._ (2025). _Hello Again! LLM-powered Personalized Agent for Long-term Dialogue (LD-Agent)_. **NAACL 2025 (to appear)** ([[2406.05925] Hello Again! LLM-powered Personalized Agent for Long-term Dialogue](https://arxiv.org/abs/2406.05925#:~:text=the%20Long,Agent%20are)).
    
5. Zhong, W. _et al._ (2023). _MemoryBank: Enhancing Large Language Models with Long-Term Memory_. **arXiv:2305.10250** ([[2305.10250] MemoryBank: Enhancing Large Language Models with Long-Term Memory](https://ar5iv.labs.arxiv.org/html/2305.10250#:~:text=personality%20over%20time%20by%20synthesizing,based%20chatbot%20named)) ([Augmenting LLMs with Retrieval, Tools, and Long-term Memory | by Alaa Dania Adimi | InfinitGraph | Mar, 2025 | Medium](https://medium.com/@ja_adimi/augmenting-llms-with-retrieval-tools-and-long-term-memory-b9e1e6b2fc28#:~:text=Memory%20Updating)).
    
6. Ong, K.T. _et al._ (2025). _THEANINE: Timeline-based Memory Management for Lifelong Dialogue Agents_. **NAACL 2025 (to appear)** ([[2406.10996] Towards Lifelong Dialogue Agents via Timeline-based Memory Management](https://arxiv.org/abs/2406.10996#:~:text=to%20improve%20retrieval%20quality%2C%20we,human%20efforts%20when%20assessing%20agent)).
    
7. Weston, J. _et al._ (2015). _Memory Networks_. **ICLR 2015** ([[1410.3916] Memory Networks](https://arxiv.org/abs/1410.3916#:~:text=,chaining%20multiple%20supporting%20sentences%20to)).
    
8. Graves, A. _et al._ (2016). _Hybrid computing using a neural network with dynamic external memory (DNC)_. **Nature 538, 471â€“476 (2016)** ([Differentiable neural computer - Wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer#:~:text=DNC%20indirectly%20takes%20inspiration%20from,by%20finding%20a%20%2052)).
    
9. Seo, M. _et al._ (2017). _Bidirectional Attention Flow for Machine Comprehension (BiDAF)_. **ICLR 2017** ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=4,representation%20generated%20when%20answering%20previous)).
    
10. Chen, D. _et al._ (2017). _Reading Wikipedia to Answer Open-Domain Questions (DrQA)_. **ACL 2017** ([BERT with History Answer Embedding for Conversational Question Answering](https://arxiv.org/pdf/1905.05412#:~:text=,JASIS%2C%2038%3A389%E2%80%93404%2C%201987)).