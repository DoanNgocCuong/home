### **ğŸ“„ Äá»“ Ã¡n nghiÃªn cá»©u: LONG TERM MEMORY, MEMORY-AUGMENTED AI AGENTS**

ğŸ“ **TÃ¡c giáº£:** (TÃªn cá»§a báº¡n)  
ğŸ« **ÄÆ¡n vá»‹ nghiÃªn cá»©u:** (TÃªn trÆ°á»ng Ä‘áº¡i há»c / viá»‡n nghiÃªn cá»©u)  
ğŸ“… **NgÃ y thá»±c hiá»‡n:** (NgÃ y báº¯t Ä‘áº§u nghiÃªn cá»©u)

---

## **ğŸ“Œ 1. Giá»›i thiá»‡u (Introduction)**

### **1.1. Äáº·t váº¥n Ä‘á»: 

CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) nhÆ° GPT, BERT, LLaMA hay PaLM Ä‘Ã£ chá»©ng tá» nÄƒng lá»±c ná»•i báº­t trong viá»‡c hiá»ƒu vÃ  táº¡o ra ngÃ´n ngá»¯ tá»± nhiÃªn, Ä‘áº·c biá»‡t khi Ä‘Æ°á»£c triá»ƒn khai dÆ°á»›i dáº¡ng há»‡ thá»‘ng há»™i thoáº¡i hoáº·c trá»£ lÃ½ AI. Tuy nhiÃªn, trong mÃ´i trÆ°á»ng á»©ng dá»¥ng thá»±c táº¿, nháº¥t lÃ  táº¡i cÃ¡c doanh nghiá»‡p, nhá»¯ng mÃ´ hÃ¬nh nÃ y pháº£i Ä‘á»‘i máº·t vá»›i má»™t bÃ i toÃ¡n Ä‘Ã¡ng ká»ƒ liÃªn quan Ä‘áº¿n viá»‡c duy trÃ¬ ngá»¯ cáº£nh, cáº­p nháº­t dá»¯ liá»‡u Ä‘á»™ng vÃ  tÆ°Æ¡ng tÃ¡c lÃ¢u dÃ i cÃ¹ng ngÆ°á»i dÃ¹ng. Má»—i khi lÆ°á»£ng thÃ´ng tin cá»§a cÃ¡c cuá»™c há»™i thoáº¡i trá»Ÿ nÃªn Ä‘á»“ sá»™, viá»‡c Ä‘Æ°a táº¥t cáº£ ná»™i dung vÃ o pháº§n ngá»¯ cáº£nh (prompt) trá»Ÿ nÃªn báº¥t kháº£ thi do giá»›i háº¡n vá» máº·t ká»¹ thuáº­t cÅ©ng nhÆ° chi phÃ­ tÃ­nh toÃ¡n. BÃªn cáº¡nh Ä‘Ã³, dá»¯ liá»‡u táº¡i cÃ¡c doanh nghiá»‡p khÃ´ng tÄ©nh mÃ  luÃ´n Ä‘Æ°á»£c Ä‘iá»u chá»‰nh vÃ  bá»• sung, Ä‘Ã²i há»i má»™t cÆ¡ cháº¿ thÆ°á»ng xuyÃªn cáº­p nháº­t Ä‘á»ƒ trÃ¡nh tÃ¬nh tráº¡ng mÃ´ hÃ¬nh sá»­ dá»¥ng thÃ´ng tin cÅ© hoáº·c láº¡c háº­u.

Váº¥n Ä‘á» cá»§a nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ nÃ y cÃ ng trá»Ÿ nÃªn phá»©c táº¡p hÆ¡n khi ngÆ°á»i dÃ¹ng mong Ä‘á»£i kháº£ nÄƒng â€œnhá»›â€ cÃ¡c luá»“ng há»™i thoáº¡i kÃ©o dÃ i nhiá»u phiÃªn, tháº­m chÃ­ nhiá»u thÃ¡ng. Trong cÃ¡c tÃ¬nh huá»‘ng thá»±c táº¿, nhÆ° khi tÆ°Æ¡ng tÃ¡c vá»›i khÃ¡ch hÃ ng, viá»‡c chatbot quÃªn máº¥t nhá»¯ng trao Ä‘á»•i trÆ°á»›c Ä‘Ã³ khiáº¿n ngÆ°á»i dÃ¹ng pháº£i láº·p láº¡i thÃ´ng tin vÃ  gÃ¢y nÃªn sá»± khÃ³ chá»‹u khÃ´ng nhá». Náº¿u nhÆ° trÆ°á»›c Ä‘Ã¢y, cÃ¡c giáº£i phÃ¡p RAG (Retrieval-Augmented Generation) chá»§ yáº¿u táº­p trung vÃ o cÃ¡c táº­p dá»¯ liá»‡u tÄ©nh Ä‘á»ƒ giáº£m thiá»ƒu sai sÃ³t vÃ  trÃ¡nh tÃ¬nh tráº¡ng mÃ´ hÃ¬nh â€œbá»‹aâ€ thÃ´ng tin, thÃ¬ nay, nhu cáº§u quáº£n lÃ½ luá»“ng há»™i thoáº¡i Ä‘á»™ng vÃ  dá»¯ liá»‡u doanh nghiá»‡p liÃªn tá»¥c thay Ä‘á»•i láº¡i Ä‘Ã²i há»i má»™t cÆ¡ cháº¿ linh hoáº¡t hÆ¡n. RAG truyá»n thá»‘ng khÃ´ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ thÆ°á»ng xuyÃªn cÃ¡c cáº­p nháº­t, thÃªm bá»›t hay xÃ³a bá» ná»™i dung trong cÆ¡ sá»Ÿ dá»¯ liá»‡u, vÃ¬ Ä‘a sá»‘ táº­p trung vÃ o vÄƒn báº£n â€œtÄ©nhâ€ nhÆ° cáº©m nang, FAQ hoáº·c hÆ°á»›ng dáº«n.

KhÃ³ khÄƒn khÃ¡c náº£y sinh khi nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ nÃ y trá»Ÿ nÃªn quÃ¡ táº£i do pháº£i náº¡p toÃ n bá»™ lá»‹ch sá»­ cá»§a nhiá»u phiÃªn há»™i thoáº¡i, dáº«n Ä‘áº¿n chi phÃ­ tÃ­nh toÃ¡n tÄƒng vá»t vÃ  Ä‘á»™ trá»… xá»­ lÃ½ cÅ©ng khÃ´ng cÃ²n Ä‘Ã¡p á»©ng Ä‘Æ°á»£c yÃªu cáº§u thá»±c táº¿. Tá»« gÃ³c Ä‘á»™ ngÆ°á»i dÃ¹ng, viá»‡c cÃ³ má»™t chatbot thÃ´ng minh vÃ  nhanh nháº¡y lÃ  Æ°u tiÃªn hÃ ng Ä‘áº§u, nhÆ°ng náº¿u há»‡ thá»‘ng máº¥t quÃ¡ nhiá»u thá»i gian Ä‘á»ƒ suy xÃ©t hoáº·c tráº£ lá»i khÃ´ng chÃ­nh xÃ¡c do rá»‘i loáº¡n thÃ´ng tin, tráº£i nghiá»‡m tÆ°Æ¡ng tÃ¡c sáº½ bá»‹ áº£nh hÆ°á»Ÿng náº·ng ná». Äá»‘i vá»›i doanh nghiá»‡p, lÆ°á»£ng há»™i thoáº¡i cÃ³ thá»ƒ lÃªn Ä‘áº¿n hÃ ng triá»‡u dÃ²ng, káº¿t há»£p vá»›i nhiá»u loáº¡i dá»¯ liá»‡u khÃ¡c nhÆ° há»“ sÆ¡ khÃ¡ch hÃ ng, thá»‘ng kÃª kinh doanh hay cÃ¡c bÃ¡o cÃ¡o ná»™i bá»™. Sá»± hiá»‡n diá»‡n cá»§a má»™t kiáº¿n trÃºc cÃ³ thá»ƒ trÃ­ch xuáº¥t pháº§n thÃ´ng tin cáº§n thiáº¿t Ä‘á»ƒ Ä‘Æ°a vÃ o ngá»¯ cáº£nh vÃ  bá» qua nhá»¯ng yáº¿u tá»‘ khÃ´ng cÃ²n há»¯u Ã­ch lÃ  Ä‘iá»u báº¯t buá»™c, nháº±m tá»‘i Æ°u cáº£ chi phÃ­ vÃ  kháº£ nÄƒng váº­n hÃ nh.

BÃ i toÃ¡n â€œlÃ£ng quÃªnâ€ hay â€œÆ°u tiÃªn thÃ´ng tinâ€ Ä‘áº·t ra yÃªu cáº§u Ä‘áº·c biá»‡t vá» cÃ¡ch tá»• chá»©c vÃ  gÃ¡n nhÃ£n dá»¯ liá»‡u. Náº¿u giá»¯ láº¡i táº¥t cáº£ thÃ¬ há»‡ thá»‘ng bá»‹ quÃ¡ táº£i, cÃ²n náº¿u xÃ³a bá»›t má»™t cÃ¡ch tÃ¹y tiá»‡n, mÃ´ hÃ¬nh cÃ³ thá»ƒ bá» lá»¡ nhá»¯ng chi tiáº¿t quan trá»ng vá»‘n dÄ© cáº§n thiáº¿t Ä‘á»ƒ suy luáº­n chÃ­nh xÃ¡c. HÆ¡n ná»¯a, má»—i khi cÃ³ xung Ä‘á»™t thÃ´ng tin hoáº·c thay Ä‘á»•i vá» dá»¯ liá»‡u, quÃ¡ trÃ¬nh cáº­p nháº­t sao cho mÃ´ hÃ¬nh khÃ´ng tráº£ lá»i dá»±a trÃªn nhá»¯ng gÃ¬ Ä‘Ã£ lá»—i thá»i láº¡i trá»Ÿ thÃ nh má»™t thÃ¡ch thá»©c. Tá»« Ä‘Ã³, cÃ¢u há»i trung tÃ¢m Ä‘Æ°á»£c Ä‘áº·t ra lÃ  lÃ m tháº¿ nÃ o Ä‘á»ƒ thiáº¿t káº¿ má»™t cÆ¡ cháº¿ â€œbá»™ nhá»› ngoÃ iâ€ cho mÃ´ hÃ¬nh ngÃ´n ngá»¯, cÃ³ kháº£ nÄƒng quáº£n lÃ½ luá»“ng há»™i thoáº¡i dÃ i háº¡n, tiáº¿p nháº­n vÃ  loáº¡i bá» thÃ´ng tin linh hoáº¡t, Ä‘á»“ng thá»i báº£o Ä‘áº£m tá»‘c Ä‘á»™ vÃ  cháº¥t lÆ°á»£ng tráº£ lá»i khÃ´ng suy giáº£m.

Táº¥t cáº£ cÃ¡c yáº¿u tá»‘ vá»«a Ä‘á» cáº­p nháº¥n máº¡nh nhu cáº§u nghiÃªn cá»©u vÃ  phÃ¡t triá»ƒn má»™t lá»›p â€œbá»™ nhá»› dÃ i háº¡nâ€ (long-term memory layer) cÃ³ thá»ƒ bá»• sung cho LLM, giÃºp lÆ°u trá»¯, truy xuáº¥t vÃ  cáº­p nháº­t thÃ´ng tin hiá»‡u quáº£ trong mÃ´i trÆ°á»ng há»™i thoáº¡i nhiá»u phiÃªn vÃ  dá»¯ liá»‡u doanh nghiá»‡p liÃªn tá»¥c phÃ¡t sinh. Giáº£i phÃ¡p lÃ½ tÆ°á»Ÿng cáº§n cho phÃ©p tÃ¡ch biá»‡t quÃ¡ trÃ¬nh lÆ°u trá»¯ vÃ  trÃ­ch xuáº¥t khá»i mÃ´ hÃ¬nh ngÃ´n ngá»¯, Ä‘á»“ng thá»i duy trÃ¬ tÃ­nh chÃ­nh xÃ¡c trong viá»‡c truy váº¥n nhá»¯ng Ä‘oáº¡n thÃ´ng tin quan trá»ng má»—i khi cáº§n dÃ¹ng Ä‘áº¿n. Má»¥c tiÃªu lÃ  nÃ¢ng cao cháº¥t lÆ°á»£ng há»™i thoáº¡i, giáº£m thiá»ƒu tÃ¬nh tráº¡ng láº·p láº¡i cÃ¢u há»i, háº¡n cháº¿ chi phÃ­ vá» token vÃ  thá»i gian, qua Ä‘Ã³ Ä‘Ã¡p á»©ng tá»‘t hÆ¡n yÃªu cáº§u triá»ƒn khai thá»±c táº¿ trong doanh nghiá»‡p.

### **1.2. CÃ¡c giáº£i phÃ¡p hiá»‡n táº¡i vÃ  háº¡n cháº¿
- .... PhÃ¢n Ä‘oáº¡n dá»±a trÃªn ...

**Trá»£ lÃ½ há»™i thoáº¡i cÃ¡ nhÃ¢n hÃ³a cÃ³ trÃ­ nhá»› (Memory-based Personalized Dialogue Agents)**

Sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c trá»£ lÃ½ há»™i thoáº¡i cÃ¡ nhÃ¢n hÃ³a cÃ³ trÃ­ nhá»› Ä‘Ã£ nÃ¢ng cao Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng tÆ°Æ¡ng tÃ¡c dÃ i háº¡n, báº±ng cÃ¡ch cho phÃ©p há»‡ thá»‘ng **lÆ°u giá»¯ vÃ  sá»­ dá»¥ng láº¡i thÃ´ng tin tá»« cÃ¡c cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³** (Bae et al., 2022).

Nhá»¯ng phÆ°Æ¡ng phÃ¡p ban Ä‘áº§u, cháº³ng háº¡n nhÆ° **CoMemNN** (Pei et al., 2021), giá»›i thiá»‡u cÃ¡c cÆ¡ cháº¿ Ä‘á»ƒ **tá»«ng bÆ°á»›c xÃ¢y dá»±ng há»“ sÆ¡ ngÆ°á»i dÃ¹ng** trong quÃ¡ trÃ¬nh Ä‘á»‘i thoáº¡i.

Tuy nhiÃªn, viá»‡c thu tháº­p dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n Ä‘á»§ lá»›n Ä‘á»ƒ huáº¥n luyá»‡n má»™t há»‡ thá»‘ng cÃ¡ nhÃ¢n hÃ³a lÃ¢u dÃ i lÃ  **ráº¥t khÃ³** (Tseng et al., 2024).

Gáº§n Ä‘Ã¢y, cÃ¡c nghiÃªn cá»©u táº­p trung vÃ o viá»‡c **káº¿t há»£p LLM vá»›i module bá»™ nhá»›**. VÃ­ dá»¥:

| Bae et al., "Keep Me Updated!"                     | 2022 | [https://aclanthology.org/2022.findings-emnlp.276](https://aclanthology.org/2022.findings-emnlp.276) |
| -------------------------------------------------- | ---- | ---------------------------------------------------------------------------------------------------- |
| Pei et al., "Cooperative Memory Network (CoMemNN)" | 2021 | [https://doi.org/10.1145/3442381.3449843](https://doi.org/10.1145/3442381.3449843)                   |

## ğŸ”— **Link cÃ¡c bÃ i bÃ¡o Ä‘Æ°á»£c trÃ­ch dáº«n:**

| Paper                                              | NÄƒm  | Link                                                                                                 |
| -------------------------------------------------- | ---- | ---------------------------------------------------------------------------------------------------- |
| Bae et al., "Keep Me Updated!"                     | 2022 | [https://aclanthology.org/2022.findings-emnlp.276](https://aclanthology.org/2022.findings-emnlp.276) |
| Pei et al., "Cooperative Memory Network (CoMemNN)" | 2021 | [https://doi.org/10.1145/3442381.3449843](https://doi.org/10.1145/3442381.3449843)                   |
| Tseng et al., "Two Tales of Persona in LLMs"       | 2024 | [https://aclanthology.org/2024.findings-emnlp.969](https://aclanthology.org/2024.findings-emnlp.969) |
| Li et al., "LD-Agent"                              | 2024 | [https://arxiv.org/abs/2406.05925](https://arxiv.org/abs/2406.05925)                                 |
| Zhong et al., "MemoryBank"                         | 2024 | [https://doi.org/10.1609/aaai.v38i17.29946](https://doi.org/10.1609/aaai.v38i17.29946)               |
| Kim et al., "Theanine"                             | 2024 | [https://arxiv.org/abs/2406.10996](https://arxiv.org/abs/2406.10996)                                 |

---

Náº¿u Quá»‘c muá»‘n, mÃ¬nh cÃ³ thá»ƒ tá»•ng há»£p Ä‘oáº¡n nÃ y thÃ nh **má»™t pháº§n â€œRelated Workâ€ hoÃ n chá»‰nh cho research paper** hoáº·c váº½ sÆ¡ Ä‘á»“ so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p. Quá»‘c muá»‘n chá»n hÆ°á»›ng nÃ o?
### **1.3. Má»¥c tiÃªu vÃ  Ä‘á»‹nh hÆ°á»›ng giáº£i phÃ¡p

Äá»“ Ã¡n hÆ°á»›ng Ä‘áº¿n má»¥c tiÃªu sau:
1. TÃ¬m hiá»ƒu vÃ  nghiÃªn cá»©u cÃ¡c phÆ°Æ¡ng phÃ¡p, ká»¹ thuáº­t phÃ¢n Ä‘oáº¡n khÃ¡c nhau,
tá»« Ä‘Ã³ Ä‘Æ°a ra Ä‘Ã¡nh giÃ¡ vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p káº¿t há»£p Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cÃ¡c
phÆ°Æ¡ng phÃ¡p cÅ©ng nhÆ° háº¡n cháº¿, nhÆ°á»£c Ä‘iá»ƒm phÃ¹ há»£p vá»›i tÃ¬nh huá»‘ng Ä‘áº·t ra vá» tÃ i
nguyÃªn sá»­ dá»¥ng.
2. ÄÆ°a ra Ä‘á» xuáº¥t cáº£i tiáº¿n vÃ  hiá»‡u quáº£ cÃ³ kháº£ nÄƒng lÃ m nÃ¢ng cao hiá»‡u quáº£ trong
mÃ´ Ä‘un truy xuáº¥t.
3. CÃ i Ä‘áº·t há»‡ thá»‘ng vÃ  thá»­ nghiá»‡m vá»›i cÃ¡c ká»‹ch báº£n khÃ¡c nhau. NÃªu vÃ  phÃ¢n tÃ­ch
Ä‘Æ°á»£c nhá»¯ng káº¿t quáº£ thá»±c nghiá»‡m, so sÃ¡nh Ä‘Ã¡nh giÃ¡ giá»¯a cÃ¡c phÆ°Æ¡ng phÃ¡p vÃ  Ä‘Æ°a
ra cÃ¡c Æ°u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm cá»§a cÃ¡c ká»¹ thuáº­t thá»­ nghiá»‡m.
TrÃªn cÆ¡ sá»Ÿ cÃ¡c phÃ¢n tÃ­ch vÃ  Ä‘Ã¡nh giÃ¡ á»Ÿ pháº§n 0.2 vÃ  Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng má»¥c tiÃªu
Ä‘Ã£ nÃªu phÃ­a trÃªn, Ä‘á»“ Ã¡n nÃ y sáº½ táº­p trung vÃ o cÃ¡c khÃ­a cáº¡nh:
4. Thá»±c hiá»‡n káº¿t há»£p phÆ°Æ¡ng phÃ¡p phÃ¢n Ä‘oáº¡n cá»• Ä‘iá»ƒn vá»›i phÆ°Æ¡ng phÃ¡p phÃ¢n
Ä‘oáº¡n sá»­ dá»¥ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng phÃ¢n Ä‘oáº¡n.
4
5. Thiáº¿t káº¿ luá»“ng truy xuáº¥t hiá»‡u quáº£ báº±ng cÃ¡ch káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p truy
xuáº¥t vÃ  sá»­ dá»¥ng thÃªm mÃ´-Ä‘un xáº¿p háº¡ng Ä‘á»ƒ cáº£i thiá»‡n thÃªm cháº¥t lÆ°á»£ng truy xuáº¥t.

Váº¥n Ä‘á» vá»›i phÆ°Æ¡ng phÃ¡p cÅ© (LongMemEval gá»‘c)

1. **Granularity chÆ°a tá»‘i Æ°u**:
    
    - Viá»‡c trÃ­ch xuáº¥t `summary`, `fact`, `keyphrase` tá»« **toÃ n bá»™ session** hoáº·c **round riÃªng láº»** cÃ³ thá»ƒ gáº·p tÃ¬nh tráº¡ng:
        - Äoáº¡n quÃ¡ **ngáº¯n** (khÃ´ng Ä‘á»§ ngá»¯ cáº£nh Ä‘á»ƒ LLM trÃ­ch xuáº¥t meaningful facts).
        - Äoáº¡n quÃ¡ **dÃ i** (gÃ¢y nhiá»…u thÃ´ng tin, LLM khÃ´ng thá»ƒ tÃ³m táº¯t chÃ­nh xÃ¡c, dá»… máº¥t detail).
    - KhÃ´ng cÃ³ cÃ¡ch kiá»ƒm soÃ¡t má»©c Ä‘á»™ coherence hoáº·c topic shift trong session dÃ i.
2. **Chá»‰ dÃ¹ng 1 loáº¡i key duy nháº¥t cho indexing**:
    
    - `K = V + fact` hoáº·c `K = V + summary` lÃ  tá»‘t, nhÆ°ng má»—i loáº¡i key cÃ³ Ä‘iá»ƒm máº¡nh khÃ¡c nhau:
        - `summary`: tá»‘t cho match semantic tá»•ng thá»ƒ.
        - `keyphrase`: báº¯t cá»¥ thá»ƒ keyword.
        - `fact`: truy xuáº¥t chÃ­nh xÃ¡c cÃ¡c entity, sá»‘ liá»‡u, má»‘c thá»i gian.
    - KhÃ´ng táº­n dá»¥ng Ä‘Æ°á»£c hiá»‡u á»©ng **ensemble giá»¯a cÃ¡c loáº¡i key**.
3. **Lacking structure in indexing**:
    
    - Indexing hiá»‡n táº¡i lÃ  **flat** â†’ khÃ´ng táº­n dá»¥ng tÃ­nh cháº¥t "táº§ng" cá»§a vÄƒn báº£n há»™i thoáº¡i: Ä‘oáº¡n â€“ session â€“ timeline.
    - Thiáº¿u kháº£ nÄƒng Ä‘iá»u hÆ°á»›ng mÆ°á»£t mÃ  giá»¯a cÃ¡c má»©c khÃ¡i quÃ¡t (coarse) vÃ  chi tiáº¿t (fine).

---

## âœ… Giáº£i phÃ¡p Ä‘á» xuáº¥t: Káº¿t há»£p **LLMs + Raptor + Multi-Key Embedding + Hierarchical Indexing**

### **1. Conversation-Aware Chunking trÆ°á»›c khi Extract**

#### âœ‚ï¸ 1.1. LLM-based Chunking

- DÃ¹ng LLM Ä‘á»ƒ phÃ¢n chia session thÃ nh cÃ¡c Ä‘oáº¡n nhá» (chunk) theo chuyá»ƒn chá»§ Ä‘á», má»¥c Ä‘Ã­ch cÃ¢u há»i, hoáº·c hÃ nh vi ngÆ°á»i dÃ¹ng.
- Lá»£i Ã­ch:
    - TÃ¡ch Ä‘Æ°á»£c cÃ¡c segment theo topic.
    - Giá»¯ Ä‘Æ°á»£c coherence bÃªn trong má»—i chunk.

#### ğŸ§± 1.2. Raptor Chunking

- DÃ¹ng **RAPTOR (recursive abstractive chunking)** Ä‘á»ƒ táº¡o cÃ¢y phÃ¢n cáº¥p cho tá»«ng session.
- Má»—i node lÃ  má»™t chunk hoáº·c summary cá»§a chunk con â†’ cÃ³ thá»ƒ phá»¥c vá»¥ **hierarchical retrieval**.


=> **Sau Ä‘Ã³ ta thu Ä‘Æ°á»£c: thay vÃ¬ K = Session + fact thÃ¬ cá»§a ta lÃ : K = Session1.i + Fact (Vá»›i i lÃ  Session 1.i Ä‘Æ°á»£c Chunking nhá» ra tá»« Session to ban Ä‘áº§u).** 

![[Pasted image 20250322071142.png]]
---

### **2. Embedding: Flatten & Index**

#### ğŸ§¾ 2.1 Raptor Flat Embedding

- ÄÆ°a tá»«ng chunk (ban Ä‘áº§u + LLM-chunked + summary chunk) vÃ o embedding encoder.
- Táº¡o index **dáº¡ng pháº³ng (flat)**, cÃ³ thá»ƒ dÃ¹ng Reranker Ä‘á»ƒ chá»n top-K chunk cÃ³ kháº£ nÄƒng cao nháº¥t.

#### ğŸ§  2.2 Hierarchical Indexing (2-phase Retrieval)

**Pha 1: Coarse Retrieval**

- Embed summary / keyphrase cá»§a chunk.
- DÃ¹ng query Ä‘á»ƒ so sÃ¡nh, chá»n Top-K chunk liÃªn quan.

**Pha 2: Fine Retrieval**

- Vá»›i má»—i chunk Ä‘Ã£ chá»n á»Ÿ coarse stage â†’ Ä‘i sÃ¢u vÃ o level fine:
    - Embed láº¡i cÃ¡c cÃ¢u gá»‘c / facts / sub-chunks.
    - Láº¥y top-Kâ€™ fine-grained memory units.

=> **Cuá»‘i cÃ¹ng Ä‘Æ°a vÃ o LLM Ä‘á»ƒ Ä‘á»c vÃ  tráº£ lá»i (Reading stage).**

---

### **3. Multi-Key Embedding cho Indexing**

- Vá»›i má»—i chunk â†’ táº¡o vÃ  embed song song:
    - `K1 = V + summary`
    - `K2 = V + fact`
    - `K3 = V + keyphrase`
- Káº¿t há»£p káº¿t quáº£ truy há»“i tá»« cÃ¡c luá»“ng (voting / weighted fusion / union-rerank).
- LÃ½ do:
    - Summary báº¯t ngá»¯ nghÄ©a chung.
    - Fact giÃºp reasoning logic.
    - Keyphrase giÃºp match keyword trong truy váº¥n cá»¥ thá»ƒ.

---

## ğŸ” Tá»•ng pipeline cáº£i tiáº¿n

```
Session
  â†“
Conversation-aware Chunking (LLMs Chunk + Raptor Chunk)
  â†“
Chunk-level Value â†’ Extract (summary, fact, keyphrase)
  â†“
Embed:
   - K1 = V + summary
   - K2 = V + fact
   - K3 = V + keyphrase
  â†“
Phase 1: Coarse Retrieval tá»« summary/keyphrase
  â†“
Phase 2: Fine Retrieval tá»« facts
  â†“
Reading Strategy: CoN + JSON (Chain-of-Note)
  â†“
Answer
```

---

## ğŸ“ˆ Lá»£i Ã­ch chÃ­nh

- âœ… Giáº£m nhiá»…u, tá»‘i Æ°u granularity nhá» LLM chunking.
- âœ… Táº­n dá»¥ng tá»‘t semantic, factual vÃ  keyword-level cues qua multi-key.
- âœ… TÄƒng kháº£ nÄƒng truy há»“i Ä‘Ãºng vá»›i hierarchical search.
- âœ… PhÃ¹ há»£p Ä‘á»ƒ scale cho 500+ sessions nhÆ° LongMemEvalM.

---

Náº¿u báº¡n cáº§n mÃ¬nh mÃ´ phá»ng pipeline nÃ y báº±ng sÆ¡ Ä‘á»“ (sequence hoáº·c flowchart), hoáº·c viáº¿t pháº§n giáº£i thÃ­ch thÃ nh vÄƒn academic Ä‘á»ƒ ghÃ©p vÃ o má»™t paper / research blog, mÃ¬nh cÅ©ng lÃ m Ä‘Æ°á»£c ngay nhÃ©!

---
DÆ°á»›i Ä‘Ã¢y lÃ  toÃ n bá»™ **ká»‹ch báº£n thá»­ nghiá»‡m** (experimental settings) tÆ°Æ¡ng á»©ng vá»›i tá»«ng Ã½ tÆ°á»Ÿng cáº£i tiáº¿n mÃ  Quá»‘c Ä‘á» xuáº¥t â€” Ä‘Æ°á»£c tá»• chá»©c theo dáº¡ng **ma tráº­n thÃ­ nghiá»‡m** Ä‘á»ƒ cÃ³ thá»ƒ dá»… dÃ ng triá»ƒn khai thá»±c nghiá»‡m, Ä‘Ã¡nh giÃ¡ tá»«ng thÃ nh pháº§n vÃ  káº¿t há»£p cá»§a pipeline.

---

## ğŸ¯ **Má»¤C TIÃŠU THá»¬ NGHIá»†M**

> Kiá»ƒm chá»©ng cÃ¡c cáº£i tiáº¿n vá» chunking, indexing, embedding, retrieval vÃ  reading strategy nháº±m cáº£i thiá»‡n hiá»‡u quáº£ cá»§a há»‡ thá»‘ng long-term memory QA (vÃ­ dá»¥ trÃªn benchmark nhÆ° LONGMEMEVAL).

---

## ğŸ§ª **Ká»ŠCH Báº¢N THá»¬ NGHIá»†M CHÃNH**

### ğŸ”¹ **I. Chunking Strategy**

|MÃ£|TÃªn phÆ°Æ¡ng phÃ¡p|MÃ´ táº£|
|---|---|---|
|C1|No Chunking (baseline)|DÃ¹ng cáº£ session hoáº·c round lÃ m value trá»±c tiáº¿p|
|C2|LLM-based Chunking|PhÃ¢n chia Ä‘oáº¡n theo chá»§ Ä‘á»/ngá»¯ nghÄ©a báº±ng LLM|
|C3|RAPTOR Chunking|Chunking dáº¡ng cÃ¢y phÃ¢n cáº¥p theo RAPTOR|
|C4|LLM + RAPTOR Hybrid|Chunk theo LLM â†’ dÃ¹ng RAPTOR Ä‘á»ƒ tÃ³m táº¯t tá»«ng chunk|

---

### ğŸ”¹ **II. Value Representation**

|MÃ£|Dáº¡ng value Ä‘áº§u vÃ o|MÃ´ táº£|
|---|---|---|
|V1|Full Session|KhÃ´ng chia nhá», Ä‘á»ƒ nguyÃªn session|
|V2|Round-based|Má»—i round lÃ  má»™t value|
|V3|Chunked|Chunk theo chiáº¿n lÆ°á»£c C2, C3, C4|
|V4|Summary|TÃ³m táº¯t cá»§a chunk hoáº·c session|
|V5|Fact|Fact trÃ­ch tá»« chunk/session|

---

### ğŸ”¹ **III. Key Design (Indexing)**

|MÃ£|TÃªn thiáº¿t káº¿ key|MÃ´ táº£|
|---|---|---|
|K1|K = V|DÃ¹ng raw value lÃ m key|
|K2|K = fact|Key lÃ  facts Ä‘Ã£ trÃ­ch|
|K3|K = summary|Key lÃ  summary|
|K4|K = V + fact|Ná»‘i fact vÃ o value Ä‘á»ƒ táº¡o key|
|K5|K = V + summary|Ná»‘i summary vÃ o value|
|K6|K = V + fact + summary + keyphrase|Multi-key (concat táº¥t cáº£)|
|K7|Multi-path index|Táº¡o nhiá»u loáº¡i key riÃªng biá»‡t, embed Ä‘á»™c láº­p|

---

### ğŸ”¹ **IV. Retrieval Strategy**

|MÃ£|PhÆ°Æ¡ng phÃ¡p truy há»“i|MÃ´ táº£|
|---|---|---|
|R1|Flat Retrieval|Retrieval Ä‘Æ¡n lá»›p, cosine / FAISS|
|R2|Coarse â†’ Fine Retrieval (2-phase)|Truy xuáº¥t 2 pha: summary â†’ fact|
|R3|Flat + Reranker|Retrieval sÆ¡ cáº¥p rá»“i rerank báº±ng LLM|
|R4|Multi-path Fusion|Truy há»“i theo tá»«ng key, rá»“i há»£p káº¿t quáº£ (voting / union)|

---

### ğŸ”¹ **V. Reading Strategy**

|MÃ£|Ká»¹ thuáº­t Ä‘á»c káº¿t quáº£|MÃ´ táº£|
|---|---|---|
|RS1|Direct Answer|ÄÆ°a chunk vÃ o, yÃªu cáº§u LLM tráº£ lá»i tháº³ng|
|RS2|Chain-of-Note (CoN)|TrÃ­ch info trÆ°á»›c rá»“i reasoning sau|
|RS3|JSON + CoN|ÄÆ°a input dáº¡ng JSON cÃ³ cáº¥u trÃºc, dÃ¹ng CoN|
|RS4|CoT + CoN|Káº¿t há»£p chain-of-thought reasoning vá»›i CoN|

---

## âœ… **Káº¾T Há»¢P THá»¬ NGHIá»†M Gá»¢I Ã (FULL COMBO)**

|#|Chunking|Value|Key|Retrieval|Reading|
|---|---|---|---|---|---|
|1|C1|V1|K1|R1|RS1|
|2|C2|V3|K4|R2|RS3|
|3|C3|V3|K6|R2|RS2|
|4|C4|V3|K7|R4|RS3|
|5|C3|V5|K2|R3|RS1|
|6|C2|V3|K5|R1|RS2|
|7|C4|V3|K6|R3|RS4|

> ğŸ’¡ _Báº¡n cÃ³ thá»ƒ chá»n 3-5 combo Ä‘á»ƒ thá»­ nghiá»‡m A/B, Ä‘o cÃ¡c metric nhÆ°:_
> 
> - **QA Accuracy (top-K)**
> - **Recall@K, NDCG@K**
> - **Inference time / latency**
> - **Token count (input to reader)**

---

## ğŸ“¦ **Dá»¯ liá»‡u sá»­ dá»¥ng**

- Dataset chÃ­nh: `LONGMEMEVAL-S` vÃ  `LONGMEMEVAL-M`
- CÃ³ thá»ƒ má»Ÿ rá»™ng thá»­ nghiá»‡m vá»›i `MemoryBank`, `PerLTQA` Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ generalize.

---

Náº¿u báº¡n cáº§n mÃ¬nh giÃºp viáº¿t script pipeline cháº¡y thá»­ nghiá»‡m hoáº·c dá»±ng YAML config cho tá»«ng scenario Ä‘á»ƒ quáº£n lÃ½ cháº¡y báº±ng `Hydra`/`WandB` hoáº·c há»‡ thá»‘ng ML pipeline khÃ¡c thÃ¬ cá»© nÃ³i nhÃ©. CÅ©ng cÃ³ thá»ƒ dá»±ng láº¡i báº£ng nÃ y dÆ°á»›i dáº¡ng Notion template hoáº·c Google Sheet Ä‘á»ƒ dá»… quáº£n lÃ½.


---
# SAU KHI HIá»‚U Gá»C, BÃ€I BÃO Gá»C, QUAY Láº I LANGGRAPH THÃŒ THáº¤Y á»’. RA LANGGRAPH NÃ“ Äá»€ XUáº¤T KIáº¾N TRÃšC THÃ”I, CÃ’N CÆ  Báº¢N THÃŒ LÃ€ BÃ€I BÃO ÄANG SEARCH NÃ€Y 

**LangGraph** lÃ  má»™t **framework** Ä‘Æ°á»£c giá»›i thiá»‡u trong khÃ³a há»c "Long-Term Agentic Memory with LangGraph" do Harrison Chase, Co-Founder vÃ  CEO cá»§a LangChain, giáº£ng dáº¡y. KhÃ³a há»c nÃ y hÆ°á»›ng dáº«n cÃ¡ch xÃ¢y dá»±ng má»™t **agent** vá»›i kháº£ nÄƒng **ghi nhá»› dÃ i háº¡n**, cá»¥ thá»ƒ lÃ  trong viá»‡c quáº£n lÃ½ email cÃ¡ nhÃ¢n.

**Äiá»ƒm má»›i mÃ  LangGraph Ä‘á» cáº­p Ä‘áº¿n**:

1. **TÃ­ch há»£p ba loáº¡i memory trong agent**:
    
    - **Semantic Memory**: LÆ°u trá»¯ cÃ¡c **facts** vá» ngÆ°á»i dÃ¹ng, nhÆ° sá»Ÿ thÃ­ch, thÃ³i quen, Ä‘á»ƒ sá»­ dá»¥ng trong cÃ¡c tÆ°Æ¡ng tÃ¡c sau nÃ y.
    - **Episodic Memory**: Ghi nhá»› cÃ¡c **tÃ¬nh huá»‘ng cá»¥ thá»ƒ** Ä‘Ã£ xáº£y ra trong quÃ¡ khá»©, giÃºp agent hiá»ƒu ngá»¯ cáº£nh vÃ  cáº£i thiá»‡n pháº£n há»“i.
    - **Procedural Memory**: LÆ°u trá»¯ cÃ¡c **hÆ°á»›ng dáº«n vÃ  quy trÃ¬nh** mÃ  agent cáº§n tuÃ¢n theo, giÃºp tá»‘i Æ°u hÃ³a hÃ nh vi dá»±a trÃªn pháº£n há»“i.


---
https://github.com/DoanNgocCuong/MiniProj_RAG3_RAG6_LegalChatbot_16032025

---
# Hiá»ƒu sÃ¢u hÆ¡n vá» Datase: 


LongMemEval lÃ  má»™t bá»™ dá»¯ liá»‡u toÃ n diá»‡n, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n. Bá»™ dá»¯ liá»‡u nÃ y bao gá»“m 500 cÃ¢u há»i cháº¥t lÆ°á»£ng cao, táº­p trung vÃ o nÄƒm kháº£ nÄƒng cá»‘t lÃµi:îˆ†

1. **TrÃ­ch xuáº¥t thÃ´ng tin (Information Extraction):** Kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin cá»¥ thá»ƒ tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c dÃ i, bao gá»“m cáº£ chi tiáº¿t do ngÆ°á»i dÃ¹ng hoáº·c trá»£ lÃ½ cung cáº¥p.îˆ†
    
2. **LÃ½ luáº­n Ä‘a phiÃªn (Multi-Session Reasoning):** Kháº£ nÄƒng tá»•ng há»£p thÃ´ng tin tá»« nhiá»u phiÃªn trÃ² chuyá»‡n Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i phá»©c táº¡p yÃªu cáº§u sá»± tá»•ng há»£p vÃ  so sÃ¡nh.îˆ†
    
3. **Cáº­p nháº­t kiáº¿n thá»©c (Knowledge Updates):** Kháº£ nÄƒng nháº­n biáº¿t vÃ  cáº­p nháº­t thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng theo thá»i gian.îˆ†
    
4. **LÃ½ luáº­n thá»i gian (Temporal Reasoning):** Nháº­n thá»©c vá» cÃ¡c khÃ­a cáº¡nh thá»i gian cá»§a thÃ´ng tin ngÆ°á»i dÃ¹ng, bao gá»“m cáº£ thá»i gian Ä‘Æ°á»£c Ä‘á» cáº­p rÃµ rÃ ng vÃ  siÃªu dá»¯ liá»‡u thá»i gian trong cÃ¡c tÆ°Æ¡ng tÃ¡c.îˆ†
    
5. **Tá»« chá»‘i tráº£ lá»i (Abstention):** Kháº£ nÄƒng tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.îˆ†
    

Láº¥y cáº£m há»©ng tá»« bÃ i kiá»ƒm tra "tÃ¬m kim trong Ä‘á»‘ng cá» khÃ´", LongMemEval sá»­ dá»¥ng má»™t quy trÃ¬nh kiá»ƒm soÃ¡t thuá»™c tÃ­nh Ä‘á»ƒ táº¡o ra lá»‹ch sá»­ trÃ² chuyá»‡n máº¡ch láº¡c, cÃ³ thá»ƒ má»Ÿ rá»™ng vÃ  Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u thá»i gian cho má»—i cÃ¢u há»i. Há»‡ thá»‘ng trÃ² chuyá»‡n cáº§n phÃ¢n tÃ­ch cÃ¡c tÆ°Æ¡ng tÃ¡c Ä‘á»™ng Ä‘á»ƒ ghi nhá»› vÃ  tráº£ lá»i cÃ¢u há»i sau khi táº¥t cáº£ cÃ¡c phiÃªn tÆ°Æ¡ng tÃ¡c Ä‘Ã£ diá»…n ra.îˆ†

**Cáº¥u trÃºc Bá»™ Dá»¯ Liá»‡u:**

Bá»™ dá»¯ liá»‡u bao gá»“m ba tá»‡p chÃ­nh:îˆ†

1. **longmemeval_s.json:** Má»—i lá»‹ch sá»­ trÃ² chuyá»‡n tiÃªu thá»¥ khoáº£ng 115.000 token (~40 phiÃªn lá»‹ch sá»­).îˆ†
    
2. **longmemeval_m.json:** Má»—i lá»‹ch sá»­ trÃ² chuyá»‡n chá»©a khoáº£ng 500 phiÃªn.îˆ†
    
3. **longmemeval_oracle.json:** Chá»‰ bao gá»“m cÃ¡c phiÃªn chá»©a báº±ng chá»©ng cáº§n thiáº¿t.îˆ†
    

Má»—i tá»‡p chá»©a 500 trÆ°á»ng há»£p Ä‘Ã¡nh giÃ¡, má»—i trÆ°á»ng há»£p bao gá»“m cÃ¡c trÆ°á»ng:îˆ†

- **question_id:** ID duy nháº¥t cho má»—i cÃ¢u há»i.îˆ†
    
- **question_type:** Loáº¡i cÃ¢u há»i, nhÆ° single-session-user, single-session-assistant, single-session-preference, temporal-reasoning, knowledge-update, vÃ  multi-session. Náº¿u question_id káº¿t thÃºc báº±ng _abs, Ä‘Ã³ lÃ  cÃ¢u há»i tá»« chá»‘i tráº£ lá»i.îˆ†
    
- **question:** Ná»™i dung cÃ¢u há»i.îˆ†
    
- **answer:** CÃ¢u tráº£ lá»i mong Ä‘á»£i tá»« mÃ´ hÃ¬nh.îˆ†
    
- **question_date:** NgÃ y cá»§a cÃ¢u há»i.îˆ†
    
- **haystack_session_ids:** Danh sÃ¡ch ID cá»§a cÃ¡c phiÃªn lá»‹ch sá»­ (sáº¯p xáº¿p theo thá»i gian).îˆ†
    
- **haystack_dates:** Danh sÃ¡ch cÃ¡c má»‘c thá»i gian cá»§a cÃ¡c phiÃªn lá»‹ch sá»­.îˆ†
    
- **haystack_sessions:** Danh sÃ¡ch ná»™i dung thá»±c táº¿ cá»§a cÃ¡c phiÃªn trÃ² chuyá»‡n giá»¯a ngÆ°á»i dÃ¹ng vÃ  trá»£ lÃ½. Má»—i phiÃªn lÃ  má»™t danh sÃ¡ch cÃ¡c lÆ°á»£t trao Ä‘á»•i, má»—i lÆ°á»£t cÃ³ Ä‘á»‹nh dáº¡ng {"role": user/assistant, "content": ná»™i dung tin nháº¯n}. Äá»‘i vá»›i cÃ¡c lÆ°á»£t chá»©a báº±ng chá»©ng cáº§n thiáº¿t, cÃ³ thÃªm trÆ°á»ng has_answer: true.îˆ†
    
- **answer_session_ids:** Danh sÃ¡ch ID cá»§a cÃ¡c phiÃªn chá»©a báº±ng chá»©ng, dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a viá»‡c nhá»› láº¡i á»Ÿ cáº¥p Ä‘á»™ phiÃªn.îˆ†
    

**Thiáº¿t láº­p MÃ´i TrÆ°á»ng:**

Äá»ƒ sá»­ dá»¥ng bá»™ dá»¯ liá»‡u, báº¡n cÃ³ thá»ƒ táº£i xuá»‘ng tá»« [Hugging Face](https://huggingface.co/datasets/xiaowu0162/longmemeval) vÃ  giáº£i nÃ©n vÃ o thÆ° má»¥c `data/`. Khuyáº¿n nghá»‹ sá»­ dá»¥ng mÃ´i trÆ°á»ng conda Ä‘á»ƒ cÃ i Ä‘áº·t cÃ¡c yÃªu cáº§u cáº§n thiáº¿t:îˆ†

```bash
conda create -n longmemeval python=3.9
conda activate longmemeval
pip install -r requirements-full.txt
```

îˆ†

**ÄÃ¡nh GiÃ¡ Há»‡ Thá»‘ng:**

Äá»ƒ kiá»ƒm tra há»‡ thá»‘ng cá»§a báº¡n trÃªn LongMemEval, báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c táº­p lá»‡nh Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c cung cáº¥p. LÆ°u Ä‘áº§u ra cá»§a há»‡ thá»‘ng vÃ o tá»‡p JSONL vá»›i má»—i dÃ²ng chá»©a hai trÆ°á»ng: `question_id` vÃ  `hypothesis`. Sau Ä‘Ã³, cháº¡y táº­p lá»‡nh Ä‘Ã¡nh giÃ¡:îˆ†

```bash
export OPENAI_API_KEY=YOUR_API_KEY
cd src/evaluation
python3 evaluate_qa.py gpt-4o your_hypothesis_file ../../data/longmemeval_oracle.json
```

îˆ†

Táº­p lá»‡nh nÃ y sáº½ lÆ°u nháº­t kÃ½ Ä‘Ã¡nh giÃ¡ vÃ o tá»‡p `[your_hypothesis_file].log`. Báº¡n cÃ³ thá»ƒ tá»•ng há»£p cÃ¡c Ä‘iá»ƒm sá»‘ tá»« nháº­t kÃ½ báº±ng lá»‡nh:îˆ†

```bash
python3 print_qa_metrics.py gpt-4o your_hypothesis_file.log ../../data/longmemeval_oracle.json
```

îˆ†

**Táº¡o Lá»‹ch Sá»­ TrÃ² Chuyá»‡n TÃ¹y Chá»‰nh:**

LongMemEval há»— trá»£ biÃªn soáº¡n lá»‹ch sá»­ trÃ² chuyá»‡n vá»›i Ä‘á»™ dÃ i tÃ¹y Ã½ cho má»—i trÆ°á»ng há»£p cÃ¢u há»i, cho phÃ©p báº¡n dá»… dÃ ng tÄƒng Ä‘á»™ khÃ³. Äá»ƒ táº¡o lá»‹ch sá»­ tÃ¹y chá»‰nh, báº¡n cÃ³ thá»ƒ lÃ m theo Ä‘á»‹nh dáº¡ng trong `2_questions` vÃ  `6_session_cache` Ä‘á»ƒ táº¡o cÃ¢u há»i vÃ  cÃ¡c phiÃªn báº±ng chá»©ng, sau Ä‘Ã³ cháº¡y táº­p lá»‡nh `sample_haystack_and_timestamp.py` vá»›i cÃ¡c tham sá»‘ phÃ¹ há»£p.îˆ†

**Cháº¡y Thá»­ Nghiá»‡m Há»‡ Thá»‘ng Ghi Nhá»›:**

ChÃºng tÃ´i cung cáº¥p mÃ£ thá»­ nghiá»‡m cho viá»‡c truy xuáº¥t bá»™ nhá»› vÃ  táº¡o cÃ¢u tráº£ lá»i cÃ³ há»— trá»£ truy xuáº¥t dÆ°á»›i cÃ¡c thÆ° má»¥c `src/retrieval


---
Long-TermMemoryMethods Toequipchatassistantswithlong-termmemorycapabilities, three major techniques are commonly explored. The first approach involves directly adapting LLMs to process extensive history information as long-context inputs (Beltagy et al., 2020; Kitaev et al., 2020; Fu et al., 2024; An et al., 2024). While this method avoids the need for complex architectures, it is inefficient and susceptible to the â€œlost-in-the-middleâ€ phenomenon, where the ability of LLMs to utilize contextual information weakens as the input length grows (Shi et al., 2023; Liu et al., 2024). A second line of research integrates differentiable memory modules into language models, proposing specialized architectural designs and training strategies to enhance memory capabilities (Weston et al., 2014; Wu et al., 2022; Zhong et al., 2022; Wang et al., 2023). Lastly, several studies approach long-term memory from the perspective of context compression, developing techniques 3 Published as a conference paper at ICLR 2025 to condense lengthy histories into compact representations, whether in the form of LLM internal representations (Mu et al., 2023; Chevalier et al., 2023), discrete tokens (Jiang et al., 2023; Xu et al., 2024), or retrievable text segments via retrieval-augmented generation (RAG, Shi et al. (2024); Wang et al. (2023); Sarthi et al. (2024); Chen et al. (2023a); GutiÂ´ errez et al. (2024)). Although LONGMEMEVAL can evaluate any memory system, we will take an online context compression perspective, where each history interaction session is sequentially processed, stored, and accessed on-demand through indexing and retrieval mechanisms (Â§4). This formulation aligns with current literature (Zhong et al., 2024; GutiÂ´ errez et al., 2024) and commercial systems (OpenAI, 2024; Coze, 2024). Its plug-and-play nature also facilitates the integration into existing chat assistant systems


DÆ°á»›i Ä‘Ã¢y lÃ  báº£n dá»‹ch tiáº¿ng Viá»‡t Ä‘oáº¡n vÄƒn báº¡n cung cáº¥p:

---

### **CÃ¡c phÆ°Æ¡ng phÃ¡p trÃ­ nhá»› dÃ i háº¡n (Long-Term Memory Methods)**

Äá»ƒ trang bá»‹ kháº£ nÄƒng ghi nhá»› dÃ i háº¡n cho cÃ¡c trá»£ lÃ½ há»™i thoáº¡i, hiá»‡n cÃ³ ba ká»¹ thuáº­t chÃ­nh thÆ°á»ng Ä‘Æ°á»£c nghiÃªn cá»©u:

1. **PhÆ°Æ¡ng phÃ¡p thá»© nháº¥t** lÃ  Ä‘iá»u chá»‰nh trá»±c tiáº¿p cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLMs) Ä‘á»ƒ xá»­ lÃ½ lÆ°á»£ng lá»›n thÃ´ng tin lá»‹ch sá»­ dÆ°á»›i dáº¡ng Ä‘áº§u vÃ o dÃ i (long-context input)  
    _(Beltagy et al., 2020; Kitaev et al., 2020; Fu et al., 2024; An et al., 2024)_.  
    PhÆ°Æ¡ng phÃ¡p nÃ y giÃºp trÃ¡nh viá»‡c pháº£i thiáº¿t káº¿ kiáº¿n trÃºc phá»©c táº¡p,  
    tuy nhiÃªn láº¡i **kÃ©m hiá»‡u quáº£** vÃ  dá»… gáº·p hiá»‡n tÆ°á»£ng **"máº¥t thÃ´ng tin á»Ÿ giá»¯a" (lost-in-the-middle)** â€“  
    khi mÃ  kháº£ nÄƒng cá»§a LLM trong viá»‡c táº­n dá»¥ng thÃ´ng tin ngá»¯ cáº£nh suy giáº£m theo Ä‘á»™ dÃ i Ä‘áº§u vÃ o tÄƒng lÃªn _(Shi et al., 2023; Liu et al., 2024)_.
    
2. **HÆ°á»›ng nghiÃªn cá»©u thá»© hai** lÃ  tÃ­ch há»£p cÃ¡c **module bá»™ nhá»› phÃ¢n biá»‡t Ä‘Æ°á»£c (differentiable memory modules)** vÃ o trong mÃ´ hÃ¬nh ngÃ´n ngá»¯.  
    CÃ¡c nghiÃªn cá»©u nÃ y Ä‘á» xuáº¥t cÃ¡c thiáº¿t káº¿ kiáº¿n trÃºc chuyÃªn biá»‡t vÃ  chiáº¿n lÆ°á»£c huáº¥n luyá»‡n nháº±m tÄƒng cÆ°á»ng kháº£ nÄƒng ghi nhá»› cá»§a mÃ´ hÃ¬nh  
    _(Weston et al., 2014; Wu et al., 2022; Zhong et al., 2022; Wang et al., 2023)_.
    
3. **Cuá»‘i cÃ¹ng**, nhiá»u nghiÃªn cá»©u tiáº¿p cáº­n trÃ­ nhá»› dÃ i háº¡n tá»« gÃ³c Ä‘á»™ **nÃ©n ngá»¯ cáº£nh (context compression)**, phÃ¡t triá»ƒn cÃ¡c ká»¹ thuáº­t nháº±m **tinh gá»n lá»‹ch sá»­ há»™i thoáº¡i dÃ i** thÃ nh cÃ¡c biá»ƒu diá»…n nhá» gá»n hÆ¡n â€“  
    cÃ³ thá»ƒ dÆ°á»›i dáº¡ng biá»ƒu diá»…n ná»™i táº¡i trong LLM _(Mu et al., 2023; Chevalier et al., 2023)_,  
    cÃ¡c token rá»i ráº¡c _(Jiang et al., 2023; Xu et al., 2024)_,  
    hoáº·c cÃ¡c Ä‘oáº¡n vÄƒn báº£n cÃ³ thá»ƒ truy xuáº¥t Ä‘Æ°á»£c thÃ´ng qua ká»¹ thuáº­t sinh cÃ³ há»— trá»£ truy há»“i (Retrieval-Augmented Generation - RAG)  
    _(Shi et al., 2024; Wang et al., 2023; Sarthi et al., 2024; Chen et al., 2023a; GutiÃ©rrez et al., 2024)_.
    

Máº·c dÃ¹ **LONGMEMEVAL** cÃ³ thá»ƒ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ báº¥t ká»³ há»‡ thá»‘ng trÃ­ nhá»› nÃ o,  
trong bÃ i nÃ y chÃºng tÃ´i chá»n cÃ¡ch tiáº¿p cáº­n theo hÆ°á»›ng **nÃ©n ngá»¯ cáº£nh trá»±c tuyáº¿n (online context compression)**,  
nÆ¡i mÃ  má»—i phiÃªn tÆ°Æ¡ng tÃ¡c trong lá»‹ch sá»­ sáº½ Ä‘Æ°á»£c **xá»­ lÃ½ tuáº§n tá»±, lÆ°u trá»¯ vÃ  truy xuáº¥t theo yÃªu cáº§u** thÃ´ng qua cÃ¡c cÆ¡ cháº¿ Ä‘Ã¡nh chá»‰ má»¥c (indexing) vÃ  truy há»“i (retrieval) (Â§4).

CÃ¡ch tiáº¿p cáº­n nÃ y phÃ¹ há»£p vá»›i cÃ¡c cÃ´ng trÃ¬nh hiá»‡n táº¡i _(Zhong et al., 2024; GutiÃ©rrez et al., 2024)_  
cÅ©ng nhÆ° cÃ¡c há»‡ thá»‘ng thÆ°Æ¡ng máº¡i nhÆ° **OpenAI (2024)** vÃ  **Coze (2024)**.  
Äáº·c biá»‡t, nhá» vÃ o tÃ­nh **"plug-and-play"** (cáº¯m vÃ o lÃ  cháº¡y), phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ dá»… dÃ ng tÃ­ch há»£p vÃ o cÃ¡c há»‡ thá»‘ng trá»£ lÃ½ há»™i thoáº¡i hiá»‡n cÃ³.

---

Náº¿u báº¡n muá»‘n mÃ¬nh tÃ³m láº¡i thÃ nh báº£ng so sÃ¡nh 3 hÆ°á»›ng tiáº¿p cáº­n hoáº·c biá»ƒu Ä‘á»“ sÆ¡ Ä‘á»“ hÃ³a thÃ¬ mÃ¬nh cÃ³ thá»ƒ váº½ liá»n nhÃ©!

## 1.4 ÄÃ³ng gÃ³p cá»§a Ä‘á»“ Ã¡n
Äá»“ Ã¡n nÃ y cÃ³ 2 Ä‘Ã³ng gÃ³p chÃ­nh nhÆ° sau:
1. Äá»“ Ã¡n Ä‘á» xuáº¥t giáº£i phÃ¡p káº¿t há»£p cÃ¡c ká»¹ thuáº­t phÃ¢n Ä‘oáº¡n khÃ¡c nhau nháº±m tÄƒng
hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng truy xuáº¥t thÃ´ng tin.
2. Thá»±c hiá»‡n thá»­ nghiá»‡m káº¿t há»£p cÃ¡c ká»¹ thuáº­t truy xuáº¥t nháº±m cáº£i thiá»‡n káº¿t quáº£
Ä‘áº§u ra.
## 1.5 Bá»‘ cá»¥c Ä‘á»“ Ã¡n
ToÃ n bá»™ bÃ¡o cÃ¡o Ä‘á»“ Ã¡n tá»‘t nghiá»‡p Ä‘Æ°á»£c triá»ƒn khai trong 5 chÆ°Æ¡ng. CÃ¡c chÆ°Æ¡ng
cÃ²n láº¡i cá»§a bÃ¡o cÃ¡o cÃ³ ná»™i dung nhÆ° sau.
ChÆ°Æ¡ng 2 Ä‘á» cáº­p Ä‘áº¿n cÃ¡c ná»™i dung lÃ½ thuyáº¿t nháº±m phá»¥c vá»¥ viá»‡c nghiÃªn cá»©u, xÃ¢y
dá»±ng thá»­ nghiá»‡m vÃ  Ä‘Ã¡nh giÃ¡ giáº£i phÃ¡p Ä‘á» xuáº¥t. Trong chÆ°Æ¡ng nÃ y, tÃ´i sáº½ trÃ¬nh bÃ y
tá»•ng quan vá» mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n, cÃ¡c á»©ng dá»¥ng, háº¡n cháº¿ vÃ  má»™t sá»‘ dÃ²ng mÃ´
hÃ¬nh ngÃ´n ngá»¯ lá»›n phá»• biáº¿n. Ká»¹ thuáº­t RAG vá»›i cÃ¡c thÃ nh pháº§n vÃ  cÃ¡c giáº£i phÃ¡p
hiá»‡n cÃ³ cÅ©ng sáº½ Ä‘Æ°á»£c phÃ¢n tÃ­ch chi tiáº¿t á»Ÿ chÆ°Æ¡ng nÃ y.
ChÆ°Æ¡ng 3 trÃ¬nh bÃ y chi tiáº¿t vá» giáº£i phÃ¡p Ä‘á» xuáº¥t. TrÆ°á»›c háº¿t, tÃ´i mÃ´ táº£ tá»•ng quan
vá» luá»“ng xá»­ lÃ½, sau Ä‘Ã³ lÃ  Ä‘i sÃ¢u vÃ o tá»«ng mÃ´-Ä‘un. Trong mÃ´-Ä‘un phÃ¢n Ä‘oáº¡n, tÃ´i
trÃ¬nh bÃ y hai ká»¹ thuáº­t phÃ¢n Ä‘oáº¡n tÃ´i láº¥y lÃ m Ã½ tÆ°á»Ÿng Ä‘Ã³ lÃ  phÃ¢n Ä‘oáº¡n sá»­ dá»¥ng mÃ´
hÃ¬nh ngÃ´n ngá»¯ lá»›n vÃ  RAPTOR. Sau Ä‘Ã³, tÃ´i Ä‘á» xuáº¥t viá»‡c káº¿t há»£p hai ká»¹ thuáº­t nÃ y
Ä‘á»ƒ bá»• trá»£ cho nhau. Trong mÃ´-Ä‘un truy xuáº¥t, tÃ´i trÃ¬nh bÃ y viá»‡c káº¿t há»£p hai ká»¹ thuáº­t
Ä‘Ã³ lÃ : i) tÃ¬m kiáº¿m má»©c ngá»¯ nghÄ©a vÃ  ii) tÃ¬m kiáº¿m má»©c tá»« vá»±ng nháº±m cáº£i thiá»‡n má»©c
Ä‘á»™ phÃ¹ há»£p cá»§a cÃ¡c tÃ i liá»‡u tÃ¬m kiáº¿m Ä‘Æ°á»£c.
ChÆ°Æ¡ng 4 trÃ¬nh bÃ y cá»¥ thá»ƒ vá» cÃ¡c ká»‹ch báº£n thá»­ nghiá»‡m, thÃ´ng sá»‘ cáº¥u hÃ¬nh thá»­
nghiá»‡m, káº¿t quáº£ thá»±c nghiá»‡m vÃ  cÃ¡c Ä‘Ã¡nh giÃ¡, nháº­n xÃ©t vá» cÃ¡c phÆ°Æ¡ng phÃ¡p thá»­
nghiá»‡m. Trong chÆ°Æ¡ng nÃ y, tÃ´i sá»­ dá»¥ng má»™t sá»‘ Ä‘á»™ Ä‘o tá»± Ä‘á»™ng thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng
cho há»i Ä‘Ã¡p vÃ  Ä‘Ã¡nh giÃ¡ báº±ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. Nhá»¯ng nháº­n xÃ©t vÃ  Ä‘Ã¡nh giÃ¡
hiá»‡u nÄƒng cá»§a phÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p tham chiáº¿u cÅ©ng Ä‘Æ°á»£c
trÃ¬nh bÃ y táº¡i chÆ°Æ¡ng nÃ y.
ChÆ°Æ¡ng 5 lÃ  chÆ°Æ¡ng cuá»‘i cÃ¹ng. Trong chÆ°Æ¡ng nÃ y, tÃ´i nÃªu ra káº¿t luáº­n vá» phÆ°Æ¡ng
phÃ¡p Ä‘á» xuáº¥t, nhá»¯ng Æ°u Ä‘iá»ƒm cÅ©ng nhÆ° nhá»¯ng háº¡n cháº¿ cÃ²n tá»“n táº¡i cÅ©ng nhÆ° Ä‘á» ra
cÃ¡c hÆ°á»›ng phÃ¡t triá»ƒn trong tÆ°Æ¡ng lai.

---

## **ğŸ“Œ 2. Tá»•ng quan nghiÃªn cá»©u (Related Work)**

### **2.1. Háº¡n cháº¿ cá»§a LLMs vá» trÃ­ nhá»›**

- LLMs hiá»‡n nay **chá»‰ cÃ³ trÃ­ nhá»› ngáº¯n háº¡n**, bá»‹ giá»›i háº¡n bá»Ÿi context window (128K tokens vá»›i GPT-4-turbo, 1M tokens vá»›i Claude 3). - 2M ráº¥t to
- CÃ¡c mÃ´ hÃ¬nh khÃ´ng thá»ƒ duy trÃ¬ bá»‘i cáº£nh há»™i thoáº¡i **qua nhiá»u phiÃªn lÃ m viá»‡c**.

### **2.2. CÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n táº¡i**

#### **(1) LLMs lÆ°u trá»¯ ngáº¯n háº¡n



#### **(2) Retrieval-Augmented Generation (RAG)**

- **Æ¯u Ä‘iá»ƒm**: LLM cÃ³ thá»ƒ truy xuáº¥t dá»¯ liá»‡u tá»« nguá»“n ngoÃ i khi cáº§n.
- **NhÆ°á»£c Ä‘iá»ƒm**: KhÃ´ng nhá»› thÃ´ng tin theo thá»i gian, chá»‰ hoáº¡t Ä‘á»™ng khi cÃ³ truy váº¥n tÃ¬m kiáº¿m.

#### **(3) CÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y**

- OpenAI Ä‘ang phÃ¡t triá»ƒn **tÃ¡c nhÃ¢n cÃ³ trÃ­ nhá»›** nhÆ°ng chÆ°a cÃ´ng bá»‘ chi tiáº¿t.
- Meta AI thá»­ nghiá»‡m chatbot cÃ³ kháº£ nÄƒng **nhá»› sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng** nhÆ°ng gáº·p thÃ¡ch thá»©c vá» quyá»n riÃªng tÆ°.
![[Pasted image 20250322054143.png]]

ğŸ“Œ **Äiá»ƒm khÃ¡c biá»‡t cá»§a nghiÃªn cá»©u nÃ y:**  
âœ… Äá» xuáº¥t mÃ´ hÃ¬nh **Memory-Augmented AI** tá»‘i Æ°u hÆ¡n, cÃ³ thá»ƒ **há»c há»i theo thá»i gian mÃ  khÃ´ng bá»‹ quÃ¡ táº£i dá»¯ liá»‡u**.  
âœ… Káº¿t há»£p giá»¯a **Memory-Augmented Learning & RAG** Ä‘á»ƒ tá»‘i Æ°u hÃ³a bá»™ nhá»›.

---

## **ğŸ“Œ 3. PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u (Methodology)**

### **3.1. Kiáº¿n trÃºc Ä‘á» xuáº¥t**

MÃ´ hÃ¬nh **Memory-Augmented AI Agent** gá»“m cÃ¡c thÃ nh pháº§n chÃ­nh:  
1ï¸âƒ£ **Short-Term Memory (STM)**: LÆ°u trá»¯ thÃ´ng tin trong pháº¡m vi cá»­a sá»• ngá»¯ cáº£nh hiá»‡n táº¡i.  
2ï¸âƒ£ **Long-Term Memory (LTM)**: LÆ°u trá»¯ thÃ´ng tin quan trá»ng vÃ o **Vector Database**.  
3ï¸âƒ£ **Memory Management Algorithm**: Quyáº¿t Ä‘á»‹nh **nÃªn nhá»› gÃ¬, quÃªn gÃ¬**.  (lÆ°u táº¥t thÃ¬ bá»‹ phÃ¬ng bá»™ nhá»›? )
-bá»:  TrÃ­ nhá»› vá» sá»Ÿ thÃ­ch 
- bá»: TrÃ­ nhá»› vá» cÃ¡c sá»± kiá»‡n Ä‘Ã£ qua 
- TrÃ­ nhá»› vá» cÃ¡c lá»‹ch sáº¯p tá»›i
- 
4ï¸âƒ£ **Knowledge Update Mechanism**: Cáº­p nháº­t vÃ  quÃªn thÃ´ng tin cÅ© khi cáº§n.
- Cáº­p nháº­t dá»±a trÃªn thá»i gian (User ngÃ y xÆ°a thÃ­ch chÆ¡i Ä‘Ã¡ bÃ³ng.Gáº«y chÃ¢n => Hiá»‡n táº¡i thÃ¬ khÃ´ng). 

- 
ğŸ“Œ **MÃ´ hÃ¬nh sá»­ dá»¥ng cÃ¡c cÃ´ng nghá»‡:**

- **LLM (GPT-4, Claude 3, Llama 2)**.
- **Vector Database (FAISS, Pinecone, Weaviate)** Ä‘á»ƒ lÆ°u trÃ­ nhá»› dÃ i háº¡n.
- **LangChain / LlamaIndex** Ä‘á»ƒ quáº£n lÃ½ truy xuáº¥t thÃ´ng tin.

---

## **ğŸ“Œ 4. Thá»±c nghiá»‡m & Káº¿t quáº£ (Experiments & Results)**

### **4.1. Thiáº¿t láº­p thá»­ nghiá»‡m**

**BÃ i toÃ¡n:** So sÃ¡nh hiá»‡u suáº¥t giá»¯a **Memory-Augmented AI Agent** vÃ  **LLM thÃ´ng thÆ°á»ng** trong há»™i thoáº¡i dÃ i háº¡n.

ğŸ”¹ **Dá»¯ liá»‡u thá»­ nghiá»‡m:**

- **Táº­p há»™i thoáº¡i thá»±c táº¿** (chÄƒm sÃ³c khÃ¡ch hÃ ng, trá»£ lÃ½ áº£o).
- **Táº­p há»™i thoáº¡i tá»•ng há»£p** (há»™i thoáº¡i kÃ©o dÃ i > 10,000 tokens).
## 4. Thá»±c nghiá»‡m vÃ  Ä‘Ã¡nh giÃ¡

### 4.1 Deep Memory Retrieval (DMR)

- **DMR** (giá»›i thiá»‡u trong MemGPT) cÃ³ 500 cuá»™c há»™i thoáº¡i nhiá»u phiÃªn (multi-session).
- Zep Ä‘áº¡t **94.8%** Ä‘á»™ chÃ­nh xÃ¡c khi dÃ¹ng GPT-4-turbo (vÃ  98.2% khi dÃ¹ng má»™t biáº¿n thá»ƒ GPT-4o-mini), nhá»‰nh hÆ¡n so vá»›i MemGPT (93.4%).
- Tuy nhiÃªn, bá»™ DMR chá»‰ cÃ³ há»™i thoáº¡i khÃ¡ ngáº¯n (khoáº£ng 60 tin nháº¯n má»—i cuá»™c), chÆ°a thá»±c sá»± kiá»ƒm tra kháº£ nÄƒng â€œsiÃªu dÃ i háº¡nâ€.

### 4.2 LongMemEval (LME)

- **LongMemEval** cÃ³ cÃ¡c Ä‘oáº¡n há»™i thoáº¡i dÃ i hÆ¡n nhiá»u (trung bÃ¬nh 115.000 tokens), mÃ´ phá»ng tÃ¬nh huá»‘ng doanh nghiá»‡p thá»±c táº¿ phá»©c táº¡p.

CÃ¡c há»‡ thá»‘ng trá»£ lÃ½ trÃ² chuyá»‡n ngÃ´n ngá»¯ lá»›n gáº§n Ä‘Ã¢y (LLM) cÃ³ cÃ¡c thÃ nh pháº§n bá»™ nhá»› tÃ­ch há»£p Ä‘á»ƒ theo dÃµi lá»‹ch sá»­ trÃ² chuyá»‡n cÃ³ sá»± há»— trá»£ cá»§a ngÆ°á»i dÃ¹ng, cho phÃ©p cÃ¡c pháº£n há»“i chÃ­nh xÃ¡c vÃ  cÃ¡ nhÃ¢n hÃ³a hÆ¡n. Tuy nhiÃªn, kháº£ nÄƒng bá»™ nhá»› dÃ i háº¡n cá»§a há» trong cÃ¡c tÆ°Æ¡ng tÃ¡c bá»n vá»¯ng váº«n chÆ°a Ä‘Æ°á»£c khai thÃ¡c. BÃ i viáº¿t nÃ y giá»›i thiá»‡u Longmemeval, má»™t Ä‘iá»ƒm chuáº©n toÃ n diá»‡n Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ nÄƒm kháº£ nÄƒng bá»™ nhá»› dÃ i háº¡n cá»‘t lÃµi cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n: trÃ­ch xuáº¥t thÃ´ng tin, lÃ½ luáº­n Ä‘a phiÃªn, lÃ½ luáº­n thá»i gian, cáº­p nháº­t kiáº¿n â€‹â€‹thá»©c vÃ  kiÃªng khem. Vá»›i 500 cÃ¢u há»i Ä‘Æ°á»£c quáº£n lÃ½ tá»‰ má»‰ Ä‘Æ°á»£c nhÃºng trong lá»‹ch sá»­ trÃ² chuyá»‡n há»— trá»£ ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ má»Ÿ rá»™ng, Longmemeval Ä‘Æ°a ra má»™t thÃ¡ch thá»©c Ä‘Ã¡ng ká»ƒ Ä‘á»‘i vá»›i cÃ¡c há»‡ thá»‘ng bá»™ nhá»› dÃ i háº¡n hiá»‡n cÃ³, vá»›i cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n thÆ°Æ¡ng máº¡i vÃ  LLM bá»‘i cáº£nh dÃ i cho tháº¥y Ä‘á»™ chÃ­nh xÃ¡c giáº£m 30% khi ghi nhá»› thÃ´ng tin qua cÃ¡c tÆ°Æ¡ng tÃ¡c Ä‘Æ°á»£c duy trÃ¬. Sau Ä‘Ã³, chÃºng tÃ´i trÃ¬nh bÃ y má»™t khung thá»‘ng nháº¥t phÃ¢n chia thiáº¿t káº¿ bá»™ nhá»› dÃ i háº¡n thÃ nh bá»‘n lá»±a chá»n thiáº¿t káº¿ trÃªn cÃ¡c giai Ä‘oáº¡n láº­p chá»‰ má»¥c, truy xuáº¥t vÃ  Ä‘á»c. ÄÆ°á»£c xÃ¢y dá»±ng dá»±a trÃªn nhá»¯ng hiá»ƒu biáº¿t thá»­ nghiá»‡m quan trá»ng, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t sá»‘ thiáº¿t káº¿ bá»™ nhá»› bao gá»“m phÃ¢n tÃ¡ch phiÃªn Ä‘á»ƒ tá»‘i Æ°u hÃ³a má»©c Ä‘á»™ chi tiáº¿t giÃ¡ trá»‹, má»Ÿ rá»™ng chÃ­nh Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘á»ƒ tÄƒng cÆ°á»ng cáº¥u trÃºc chá»‰ sá»‘ vÃ  má»Ÿ rá»™ng truy váº¥n thá»i gian Ä‘á»ƒ tinh chá»‰nh pháº¡m vi tÃ¬m kiáº¿m. Káº¿t quáº£ thá»­ nghiá»‡m cho tháº¥y cÃ¡c tá»‘i Æ°u hÃ³a nÃ y cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ cáº£ viá»‡c thu há»“i bá»™ nhá»› vÃ  tráº£ lá»i cÃ¢u há»i háº¡ nguá»“n trÃªn longmemeval. NhÃ¬n chung, nghiÃªn cá»©u cá»§a chÃºng tÃ´i cung cáº¥p cÃ¡c nguá»“n lá»±c vÃ  hÆ°á»›ng dáº«n cÃ³ giÃ¡ trá»‹ Ä‘á»ƒ thÃºc Ä‘áº©y kháº£ nÄƒng bá»™ nhá»› dÃ i háº¡n cá»§a cÃ¡c trá»£ lÃ½ trÃ² chuyá»‡n dá»±a trÃªn LLM, má»Ÿ Ä‘Æ°á»ng cho AI trÃ² chuyá»‡n cÃ¡ nhÃ¢n hÃ³a vÃ  Ä‘Ã¡ng tin cáº­y hÆ¡n.

- Zep cáº£i thiá»‡n káº¿t quáº£ so vá»›i baseline (dÃ¹ng toÃ n bá»™ há»™i thoáº¡i) á»Ÿ háº§u háº¿t cÃ¡c loáº¡i cÃ¢u há»i, Ä‘áº·c biá»‡t:
    - Loáº¡i cÃ¢u â€œmulti-session,â€ â€œpreference,â€ â€œtemporal reasoningâ€ tÄƒng Ä‘Ã¡ng ká»ƒ.
    - Äá»™ trá»… (latency) giáº£m Ä‘áº¿n 90% so vá»›i viá»‡c nhÃ©t toÃ n bá»™ há»™i thoáº¡i vÃ o prompt (vÃ¬ prompt cá»§a Zep ngáº¯n gá»n hÆ¡n).
ğŸ”¹ **TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡:**

| **TiÃªu chÃ­**                  | **Memory-Augmented AI**          | **LLM thÃ´ng thÆ°á»ng**     |
| ----------------------------- | -------------------------------- | ------------------------ |
| **Kháº£ nÄƒng duy trÃ¬ bá»‘i cáº£nh** | âœ… Tá»‘t                            | âŒ KÃ©m                    |
| **Äá»™ chÃ­nh xÃ¡c pháº£n há»“i**     | âœ… Cao hÆ¡n                        | âŒ Giáº£m khi há»™i thoáº¡i dÃ i |
| **Tá»‘c Ä‘á»™ pháº£n há»“i**           | âŒ Cháº­m hÆ¡n                       | âœ… Nhanh hÆ¡n              |
| **Kháº£ nÄƒng cÃ¡ nhÃ¢n hÃ³a**      | âœ… CÃ³ thá»ƒ nhá»› sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng | âŒ KhÃ´ng nhá»› thÃ´ng tin cÅ© |

Chi tiáº¿t cÃ¡c tiÃªu chÃ­ Ä‘Ã¡nh giÃ¡: 

- **TrÃ­ch xuáº¥t thÃ´ng tin (Information Extraction)**: Kháº£ nÄƒng nhá»› láº¡i thÃ´ng tin cá»¥ thá»ƒ tá»« lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c dÃ i, bao gá»“m cáº£ chi tiáº¿t Ä‘Æ°á»£c Ä‘á» cáº­p bá»Ÿi ngÆ°á»i dÃ¹ng hoáº·c trá»£ lÃ½.â€‹[Di Wu](https://xiaowu0162.github.io/long-mem-eval/?utm_source=chatgpt.com)
    
- **Suy luáº­n Ä‘a phiÃªn (Multi-Session Reasoning)**: Kháº£ nÄƒng tá»•ng há»£p thÃ´ng tin tá»« nhiá»u phiÃªn lá»‹ch sá»­ Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i phá»©c táº¡p liÃªn quan Ä‘áº¿n viá»‡c tá»•ng há»£p vÃ  so sÃ¡nh.â€‹
    
- **Suy luáº­n thá»i gian (Temporal Reasoning)**: Nháº­n thá»©c vá» cÃ¡c khÃ­a cáº¡nh thá»i gian cá»§a thÃ´ng tin ngÆ°á»i dÃ¹ng, bao gá»“m cáº£ cÃ¡c Ä‘á» cáº­p thá»i gian rÃµ rÃ ng vÃ  siÃªu dá»¯ liá»‡u dáº¥u thá»i gian trong cÃ¡c tÆ°Æ¡ng tÃ¡c.â€‹
    
- **Cáº­p nháº­t kiáº¿n thá»©c (Knowledge Updates)**: Kháº£ nÄƒng nháº­n biáº¿t cÃ¡c thay Ä‘á»•i trong thÃ´ng tin cÃ¡ nhÃ¢n cá»§a ngÆ°á»i dÃ¹ng vÃ  cáº­p nháº­t kiáº¿n thá»©c vá» ngÆ°á»i dÃ¹ng má»™t cÃ¡ch Ä‘á»™ng theo thá»i gian.â€‹
    
- **Tá»« chá»‘i tráº£ lá»i (Abstention)**: Kháº£ nÄƒng tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n thÃ´ng tin khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c, tá»©c lÃ  thÃ´ng tin khÃ´ng Ä‘Æ°á»£c nháº¯c Ä‘áº¿n trong lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.
### **4.2. Káº¿t quáº£ thá»±c nghiá»‡m**

ğŸ“Œ **Memory-Augmented AI cáº£i thiá»‡n 38% kháº£ nÄƒng duy trÃ¬ bá»‘i cáº£nh há»™i thoáº¡i so vá»›i LLM thÃ´ng thÆ°á»ng.**  
ğŸ“Œ **Tá»‘c Ä‘á»™ pháº£n há»“i cháº­m hÆ¡n ~10% nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c tÄƒng 25%.**

---

## **ğŸ“Œ 5. Káº¿t luáº­n & HÆ°á»›ng phÃ¡t triá»ƒn (Conclusion & Future Work)**

### **5.1. Káº¿t luáº­n**

- **Memory-Augmented AI Agents cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ kháº£ nÄƒng duy trÃ¬ há»™i thoáº¡i dÃ i háº¡n.**
- **Háº¡n cháº¿ cá»§a mÃ´ hÃ¬nh lÃ  tá»‘c Ä‘á»™ pháº£n há»“i, nhÆ°ng cÃ³ thá»ƒ tá»‘i Æ°u hÃ³a.**

### **5.2. HÆ°á»›ng phÃ¡t triá»ƒn**

âœ… **Tá»‘i Æ°u thuáº­t toÃ¡n quáº£n lÃ½ bá»™ nhá»›** Ä‘á»ƒ cáº£i thiá»‡n tá»‘c Ä‘á»™.  
âœ… **Káº¿t há»£p vá»›i RAG** Ä‘á»ƒ AI cÃ³ thá»ƒ truy xuáº¥t thÃ´ng tin tá»« dá»¯ liá»‡u ngoÃ i.  
âœ… **Má»Ÿ rá»™ng thá»­ nghiá»‡m trÃªn nhiá»u lÄ©nh vá»±c** nhÆ° giÃ¡o dá»¥c, chÄƒm sÃ³c sá»©c khá»e.

---

## **ğŸ“Œ 6. TÃ i liá»‡u tham kháº£o (References)**

- [KARMA: Memory-Augmented AI Research](https://arxiv.org/abs/2409.14908)
- [AriGraph: Knowledge Memory for LLMs](https://arxiv.org/abs/2407.04363)
- [Meta AIâ€™s Memory-Augmented Chatbot](https://www.theverge.com/2025/1/27/24352992/meta-ai-memory-personalization)
