1. [Welcome to the ü§ó AI Agents Course - Hugging Face Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction)
2. https://docs.livekit.io/agents/
3. [The Top 10 arXiv Papers About AI Agents (especially Voice AI Agents) | Deepgram](https://deepgram.com/learn/top-arxiv-papers-about-ai-agents-and-voice-ai-agents)
4. https://www.automationanywhere.com/company/blog/automation-ai/exploring-ai-agents-types-capabilities-and-real-world-applications

---

M·ªôt s·ªë paper ƒë·∫ßu T3/2025: 

- [KARMA: Memory-Augmented AI Research](https://arxiv.org/abs/2409.14908)
- [AriGraph: Knowledge Memory for LLMs](https://arxiv.org/abs/2407.04363)
- [Meta AI‚Äôs Memory-Augmented Chatbot](https://www.theverge.com/2025/1/27/24352992/meta-ai-memory-personalization)

---
M·ªôt s·ªë paper 12/3/2025: 
- [[2404.13501] A Survey on the Memory Mechanism of Large Language Model based Agents](https://arxiv.org/abs/2404.13501)
- [[2501.13956] Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956)
- [[2502.12110] A-MEM: Agentic Memory for LLM Agents](https://arxiv.org/abs/2502.12110)


G·∫ßn ƒë√¢y, c√≥ nhi·ªÅu nghi√™n c·ª©u t·∫≠p trung v√†o vi·ªác c·∫£i thi·ªán c∆° ch·∫ø qu·∫£n l√Ω b·ªô nh·ªõ trong c√°c t√°c nh√¢n AI:ÓàÜ

- **A-MEM: Agentic Memory for LLM Agents:** Nghi√™n c·ª©u n√†y ƒë·ªÅ xu·∫•t m·ªôt h·ªá th·ªëng b·ªô nh·ªõ t√°c nh√¢n cho c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn, cho ph√©p t·ªï ch·ª©c b·ªô nh·ªõ m·ªôt c√°ch linh ho·∫°t v√† hi·ªáu qu·∫£ h∆°n. H·ªá th·ªëng n√†y t·∫°o ra c√°c m·∫°ng l∆∞·ªõi ki·∫øn th·ª©c li√™n k·∫øt th√¥ng qua vi·ªác l·∫≠p ch·ªâ m·ª•c v√† li√™n k·∫øt ƒë·ªông, d·ª±a tr√™n nguy√™n t·∫Øc c·ªßa ph∆∞∆°ng ph√°p Zettelkasten. ÓàÄciteÓàÇturn0search5ÓàÅÓàÜ
    
- **Zep: A Temporal Knowledge Graph Architecture for Agent Memory:** Zep l√† m·ªôt ki·∫øn tr√∫c ƒë·ªì th·ªã tri th·ª©c t·∫°m th·ªùi ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng ghi nh·ªõ c·ªßa c√°c t√°c nh√¢n AI. N√≥ v∆∞·ª£t tr·ªôi h∆°n so v·ªõi c√°c h·ªá th·ªëng hi·ªán c√≥ nh∆∞ MemGPT trong vi·ªác truy xu·∫•t b·ªô nh·ªõ s√¢u v√† ph√π h·ª£p v·ªõi c√°c ·ª©ng d·ª•ng th·ª±c t·∫ø trong doanh nghi·ªáp. ÓàÄciteÓàÇturn0search7ÓàÅÓàÜ
    
- **A Survey on the Memory Mechanism of Large Language Model based Agents:** B√†i kh·∫£o s√°t n√†y t·∫≠p trung v√†o c∆° ch·∫ø b·ªô nh·ªõ c·ªßa c√°c t√°c nh√¢n d·ª±a tr√™n m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn, nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa kh·∫£ nƒÉng t·ª± ti·∫øn h√≥a v√† t∆∞∆°ng t√°c d√†i h·∫°n gi·ªØa t√°c nh√¢n v√† m√¥i tr∆∞·ªùng. ÓàÄciteÓàÇturn0search1ÓàÅÓàÜ
    

T·ªïng k·∫øt, c·∫£ LangGraph v√† Mem0 ƒë·ªÅu cung c·∫•p c√°c gi·∫£i ph√°p linh ho·∫°t cho vi·ªác qu·∫£n l√Ω b·ªô nh·ªõ trong c√°c t√°c nh√¢n AI, gi√∫p c·∫£i thi·ªán kh·∫£ nƒÉng t∆∞∆°ng t√°c v√† c√° nh√¢n h√≥a. C√°c nghi√™n c·ª©u m·ªõi nh·∫•t ti·∫øp t·ª•c m·ªü r·ªông kh·∫£ nƒÉng c·ªßa b·ªô nh·ªõ trong AI, h∆∞·ªõng t·ªõi vi·ªác t·∫°o ra c√°c t√°c nh√¢n AI th√¥ng minh v√† hi·ªáu qu·∫£ h∆°n.ÓàÜ



---
# Deep Research: Ngu·ªìn 12/3/2025

```
Citations

[

langchain-ai.github.io

Memory

Short-term memory, or thread -scoped memory, can be recalled at any time from within a single conversational thread with a user. LangGraph manages short- term memory as a part of your agent's 51. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Short,the%20start%20of%20each%20step)[

langchain-ai.github.io

Memory

LangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=LangGraph%20manages%20short,maintaining%20separation%20between%20different%20threads)[

langchain-ai.github.io

Memory

Short-term memory, or thread -scoped memory, can be recalled at any time from within a single conversational thread with a user. LangGraph manages short- term memory as a part of your agent's 51. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Short,the%20start%20of%20each%20step)[

langchain-ai.github.io

Memory

Long conversations pose a challenge to today's LLMs. The full history may not even fit inside an LLM's context window, resulting in an irrecoverable error. Even if your LLM technically supports the full context length, most LLMs still perform poorly over long contexts. They get "distracted" by stale or off-topic content, all while suffering from slower response times and higher costs.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Long%20conversations%20pose%20a%20challenge,response%20times%20and%20higher%20costs)[

langchain-ai.github.io

Memory

Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Chat%20models%20accept%20context%20using,remove%20or%20forget%20stale%20information)[

langchain-ai.github.io

Memory

Summarizing past conversations¬∂

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Summarizing%20past%20conversations%C2%B6)[

langchain-ai.github.io

Memory

def summarize_conversation(state: State):

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=def%20summarize_conversation)[

langchain-ai.github.io

Memory

Long-term memory is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores (reference doc) to let you save and recall long-term memories.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Long,term%20memories)[

langchain-ai.github.io

Memory

Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Long,term%20memory%2C%20which%20is%20thread)[

langchain-ai.github.io

Memory

LangGraph stores long-term memories as JSON documents in a store ( 54). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a filename). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters. See the example below for an example.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=LangGraph%20stores%20long,example%20below%20for%20an%20example)[

langchain-ai.github.io

Memory

# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use. store = InMemoryStore(index={"embed": embed, "dims": 2}) user_id = "my-user" application_context = "chitchat" namespace = (user_id, application_context) store.put( namespace, "a-memory",

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=,memory)[

langchain-ai.github.io

Memory

user_id = "my-user" application_context = "chitchat" namespace = (user_id, application_context) store.put( namespace, "a-memory", { "rules": [ "User likes short, direct language",

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=user_id%20%3D%20%22my,User%20likes%20short%2C%20direct%20language)[

langchain-ai.github.io

Memory

# get the "memory" by ID item = store.get(namespace, "a-memory") # search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity items = store.search( namespace, filter={"my-key": "my-value"}, query="language preferences" )

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=,value%22%7D%2C%20query%3D%22language%20preferences%22)[

langchain-ai.github.io

Memory

Long-term memory is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores ( 54) to let you save and recall long-term memories.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Long,term%20memories)[

langchain-ai.github.io

Memory

Different applications require various types of memory. Although the analogy isn't perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Different%20applications%20require%20various%20types,those%20used%20in%20AI%20agents)[

langchain-ai.github.io

Memory

Memory Type What is Stored Human Example Agent Example Semantic Facts Things I learned in school Facts about a user Episodic Experiences Things I did Past agent actions Procedural Instructions Instincts or motor skills Agent system prompt

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Memory%20Type%20What%20is%20Stored,motor%20skills%20Agent%20system%20prompt)[

langchain-ai.github.io

Memory

Semantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Semantic%20memory%2C%20both%20in%20humans,or%20concepts%20from%20past%20interactions)[

langchain-ai.github.io

Memory

# Profile¬∂

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=)[

langchain-ai.github.io

Memory

Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you're less likely to lose information over time. It's easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=Alternatively%2C%20memories%20can%20be%20a,lead%20to%20higher%20recall%20downstream)[

langchain-ai.github.io

Memory

pairs you've selected to represent your domain.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=pairs%20you%27ve%20selected%20to%20represent,your%20domain)[

langchain-ai.github.io

Memory

However, this shifts some complexity memory updating. The model must now delete or update existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior.

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=However%2C%20this%20shifts%20some%20complexity,help%20you%20tune%20the%20behavior)[

langchain-ai.github.io

Memory

When do you want to update memories?

](https://langchain-ai.github.io/langgraph/concepts/memory/#:~:text=When%20do%20you%20want%20to,update%20memories)[

![Favicon](https://www.google.com/s2/favicons?domain=https://redis.io&sz=32)redis.io

Smarter memory management for AI agents with Mem0 and Redis - Redis

We‚Äôre happy to announce the integration of Mem0 with Redis, a powerful combination that enhances the capabilities of AI agents by providing efficient and scalable memory management. Mem0 is a self-improving memory layer for LLM applications and AI agents, enabling personalized AI experiences that save costs and delight users. Key features of Mem0 include:

](https://redis.io/blog/smarter-memory-management-for-ai-agents-with-mem0-and-redis/#:~:text=We%E2%80%99re%20happy%20to%20announce%20the,Key%20features%20of%20Mem0%20include)[

![Favicon](https://www.google.com/s2/favicons?domain=https://redis.io&sz=32)redis.io

Smarter memory management for AI agents with Mem0 and Redis - Redis

* Optimize AI responses: Deliver more accurate and personalized AI outputs by using historical context and user preferences. * Easy integration: Seamlessly enhance your existing AI solutions with Mem0‚Äôs memory layer, compatible with OpenAI, Claude, and more.

](https://redis.io/blog/smarter-memory-management-for-ai-agents-with-mem0-and-redis/#:~:text=,with%20OpenAI%2C%20Claude%2C%20and%20more)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.graphlit.com&sz=32)graphlit.com

Survey of AI Agent Memory Frameworks - Graphlit

Short-Term (Context Window) Memory: ‚Äì Letta: Utilizes an in-context memory design, showing messages and system prompts within a configurable token limit. Also features core memory blocks that remain visible in the prompt window, and a recall memory for recently accessed data. ‚Äì Mem0.ai: Offers personalization by storing conversation history and user preferences in memory. Provides short-term memory in chat contexts, enhanced by local or remote vector stores. ‚Äì Zep: Maintains session-based interactions, storing conversation transcripts as

](https://www.graphlit.com/blog/survey-of-ai-agent-memory-frameworks#:~:text=Short,interactions%2C%20storing%20conversation%20transcripts%20as)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.graphlit.com&sz=32)graphlit.com

Survey of AI Agent Memory Frameworks - Graphlit

sets remain accessible. ‚Äì Mem0.ai: Supports persistent memory stores keyed to users, sessions, or projects. Offers batch operations (batch-delete, batch-update) and advanced filtering or searching. Embedding integration allows for semantic retrieval across large data sets. ‚Äì Zep: Relies on a knowledge graph or a memory store to retain facts, messages, and metadata over multiple sessions. Provides robust cloud features (like classification and advanced search) for session data. ‚Äì CrewAI: Long-term knowledge resides in a specialized entity memory layer where

](https://www.graphlit.com/blog/survey-of-ai-agent-memory-frameworks#:~:text=sets%20remain%20accessible,specialized%20entity%20memory%20layer%20where)[

![Favicon](https://www.google.com/s2/favicons?domain=https://docs.mem0.ai&sz=32)docs.mem0.ai

Memory Types - Mem0

How Mem0 Implements Long-Term Memory

](https://docs.mem0.ai/core-concepts/memory-types#:~:text=How%20Mem0%20Implements%20Long)[

![Favicon](https://www.google.com/s2/favicons?domain=https://docs.mem0.ai&sz=32)docs.mem0.ai

Memory Types - Mem0

Mem0‚Äôs long-term memory system builds on these foundations by:

](https://docs.mem0.ai/core-concepts/memory-types#:~:text=Mem0%E2%80%99s%20long,on%20these%20foundations%20by)[

![Favicon](https://www.google.com/s2/favicons?domain=https://microsoft.github.io&sz=32)microsoft.github.io

Mem0: Long-Term Memory and Personalization for Agents | AutoGen 0.2

Comprehensive Memory Management Manage long-term, short-term, semantic, and episodic memories Self-Improving Memory Adaptive system that learns from user interactions Cross-Platform Consistency Unified user experience across various AI platforms Ô∏è Centralized Memory Control Effortless storage, updating, and deletion of memories Simplified Development API-first approach for streamlined integration Activity Dashboard

](https://microsoft.github.io/autogen/0.2/docs/ecosystem/mem0/#:~:text=Comprehensive%20Memory%20Management%20Manage%20long,for%20streamlined%20integration%20Activity%20Dashboard)[

![Favicon](https://www.google.com/s2/favicons?domain=https://microsoft.github.io&sz=32)microsoft.github.io

Mem0: Long-Term Memory and Personalization for Agents | AutoGen 0.2

* Short-term Memory: Manage temporary information within a single interaction * Semantic Memory: Organize and retrieve conceptual knowledge * Episodic Memory: Store and recall specific events or experiences * Self-Improving System: Continuously refine understanding based on user

](https://microsoft.github.io/autogen/0.2/docs/ecosystem/mem0/#:~:text=%2A%20Short,refine%20understanding%20based%20on%20user)[

![Favicon](https://www.google.com/s2/favicons?domain=https://redis.io&sz=32)redis.io

Smarter memory management for AI agents with Mem0 and Redis - Redis

* Improve future conversations: Build smarter AI that learns from every interaction and delivers context-rich responses without repetitive questions. * Save money: Cut LLM costs by up to 80% with intelligent data filtering, sending only the most relevant information to AI models. * Optimize AI responses: Deliver more accurate and personalized AI outputs by using historical context and user preferences. * Easy integration: Seamlessly enhance your existing AI solutions with Mem0‚Äôs memory layer, compatible with OpenAI, Claude, and more.

](https://redis.io/blog/smarter-memory-management-for-ai-agents-with-mem0-and-redis/#:~:text=,with%20OpenAI%2C%20Claude%2C%20and%20more)[

news.ycombinator.com

Show HN: Mem0 ‚Äì open-source Memory Layer for AI apps | Hacker News

Hi this looks interesting. From your description it looks like mem0 remembers details and context of previous chats but not the actual text of chats. Is this a correct assumption?

](https://news.ycombinator.com/item?id=41447317#:~:text=Hi%20this%20looks%20interesting,Is%20this%20a%20correct%20assumption)[

news.ycombinator.com

Show HN: Mem0 ‚Äì open-source Memory Layer for AI apps | Hacker News

3. Content management: Claude has minimum length requirements for caching (1024 characters for Sonnet, 2048 for Haiku). Mem0 can handle information of any length, from short facts to longer contexts. 4. Customization: Developers have greater control over Mem0's memory management, including options for prioritizing or deprioritizing information based on relevance or time. Claude's caching system offers less direct control. 5. Information retrieval: Mem0 is designed for more precise and targeted information retrieval, while Claude's cache works with broader contextual blocks.

](https://news.ycombinator.com/item?id=41447317#:~:text=3,works%20with%20broader%20contextual%20blocks)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.graphlit.com&sz=32)graphlit.com

Survey of AI Agent Memory Frameworks - Graphlit

Mem0.ai‚Äôs Platform and SDKs ‚Äì Offers both managed and open-source versions with Python, JavaScript, and cURL examples. ‚Äì Includes memory search, advanced filtering (logical AND/OR, metadata queries), and structured batching for memory creation or deletion. ‚Äì Supports integration with frameworks like LangChain, MultiOn, CrewAI, LlamaIndex, or custom solutions.

](https://www.graphlit.com/blog/survey-of-ai-agent-memory-frameworks#:~:text=Mem0,CrewAI%2C%20LlamaIndex%2C%20or%20custom%20solutions)[

news.ycombinator.com

Show HN: Mem0 ‚Äì open-source Memory Layer for AI apps | Hacker News

As mentioned in the post, we use a hybrid datastore approach that handles these cases effectively and that's where the graph aspect comes into picture.

](https://news.ycombinator.com/item?id=41447317#:~:text=As%20mentioned%20in%20the%20post%2C,graph%20aspect%20comes%20into%20picture)[

![Favicon](https://www.google.com/s2/favicons?domain=https://redis.io&sz=32)redis.io

Smarter memory management for AI agents with Mem0 and Redis - Redis

Redis stands out as the top data platform for managing long-term memory in AI agents‚Äîhere‚Äôs why:

](https://redis.io/blog/smarter-memory-management-for-ai-agents-with-mem0-and-redis/#:~:text=Redis%20stands%20out%20as%20the,memory%20in%20AI%20agents%E2%80%94here%E2%80%99s%20why)[

news.ycombinator.com

Show HN: Mem0 ‚Äì open-source Memory Layer for AI apps | Hacker News

1. Purpose and duration: Claude's cache is designed for short-term memory, clearing every 5 minutes. In contrast, Mem0 is built for long-term information storage, retaining data indefinitely unless instructed otherwise. 2. Flexibility and control: Mem0 offers more flexibility, allowing developers to update, delete, or modify stored information as needed. Claude's cache is more static - new information creates additional entries rather than updating existing ones. 3. Content management: Claude has minimum length requirements for caching (1024 characters for Sonnet, 2048 for Haiku). Mem0 can handle information of any length, from short facts to longer contexts. 4. Customization: Developers have

](https://news.ycombinator.com/item?id=41447317#:~:text=1,Customization%3A%20Developers%20have)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.graphlit.com&sz=32)graphlit.com

Survey of AI Agent Memory Frameworks - Graphlit

Mem0.ai‚Äôs Platform and SDKs ‚Äì Offers both managed and open-source versions with Python, JavaScript, and cURL examples. ‚Äì Includes memory search, advanced filtering (logical AND/OR, metadata queries), and structured batching for memory creation or deletion. ‚Äì Supports integration with frameworks like LangChain, MultiOn, CrewAI, LlamaIndex, or custom solutions.

](https://www.graphlit.com/blog/survey-of-ai-agent-memory-frameworks#:~:text=Mem0,CrewAI%2C%20LlamaIndex%2C%20or%20custom%20solutions)[

![Favicon](https://www.google.com/s2/favicons?domain=https://microsoft.github.io&sz=32)microsoft.github.io

Mem0: Long-Term Memory and Personalization for Agents | AutoGen 0.2

Mem0 Platform provides a smart, self-improving memory layer for Large Language Models (LLMs), enabling developers to create personalized AI experiences that evolve with each user interaction.

](https://microsoft.github.io/autogen/0.2/docs/ecosystem/mem0/#:~:text=Mem0%20Platform%20provides%20a%20smart%2C,evolve%20with%20each%20user%20interaction)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.graphlit.com&sz=32)graphlit.com

Survey of AI Agent Memory Frameworks - Graphlit

sets remain accessible. ‚Äì Mem0.ai: Supports persistent memory stores keyed to users, sessions, or projects. Offers batch operations (batch-delete, batch-update) and advanced filtering or searching. Embedding integration allows for semantic retrieval across large data sets.

](https://www.graphlit.com/blog/survey-of-ai-agent-memory-frameworks#:~:text=sets%20remain%20accessible,retrieval%20across%20large%20data%20sets)[

news.ycombinator.com

Show HN: Mem0 ‚Äì open-source Memory Layer for AI apps | Hacker News

designed for more precise and targeted information retrieval, while Claude's cache works with broader contextual blocks.

](https://news.ycombinator.com/item?id=41447317#:~:text=designed%20for%20more%20precise%20and,works%20with%20broader%20contextual%20blocks)[

![Favicon](https://www.google.com/s2/favicons?domain=https://redis.io&sz=32)redis.io

Smarter memory management for AI agents with Mem0 and Redis - Redis

1. Fast performance: Redis‚Äô in-memory architecture delivers microsecond-level read and write operations, which is critical for apps where memory retrieval times can significantly impact user experience. 2. Fastest and fully featured vector search: Redis provides an in-built, fully

](https://redis.io/blog/smarter-memory-management-for-ai-agents-with-mem0-and-redis/#:~:text=1.%20Fast%20performance%3A%20Redis%E2%80%99%20in,built%2C%20fully)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.anthropic.com&sz=32)anthropic.com

Introducing 100K Context Windows \ Anthropic

We‚Äôve expanded Claude‚Äôs context window from 9K to 100K tokens, corresponding to around 75,000 words! This means businesses can now submit hundreds of pages of materials for Claude to digest and analyze, and conversations with Claude can go on for hours or even days.

](https://www.anthropic.com/news/100k-context-windows#:~:text=We%E2%80%99ve%20expanded%20Claude%E2%80%99s%20context%20window,for%20hours%20or%20even%20days)[

![Favicon](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)arxiv.org

Personalized LLM Response Generation with Parameterized User Memory Injection

this paradigm suffers from the long-context window limitation of LLM (Liu et¬†al., 2024 ). Memory-based approaches provide a solution by leveraging a memory to store user historical content. When a new user query comes, a retriever will first retrieve relevant user information from the memory to prompt LLM produce personalized responses (Dalvi et¬†al., 30; Madaan et¬†al., 2022 ; Lewis et¬†al., 32; Zhang et¬†al., 2023 ). Unfortunately, they are limited in capturing fine-grained information due to the nature of similarity comparison retrieval process (Zhang et¬†al., 33). Additionally, user historical content can be complex and noisy, posing

](https://arxiv.org/html/2404.03565v2#:~:text=this%20paradigm%20suffers%20from%20the,be%20complex%20and%20noisy%2C%20posing)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.lukew.com&sz=32)lukew.com

LukeW | Generative Agents

* In the center of the architecture that powers generative agents is a memory stream that maintains a record of agents' experiences in natural language. * From the memory stream, records are retrieved as relevant to the agents' cognitive processes. A retrieval function that takes the agent's current situation as input and returns a subset of the memory stream to pass to a LLM, which then generates the final output behavior of the agents. * Retrieval is a linear combination of the recency, importance, and relevance function for each piece of memory. * The importance function is a prompt that asks the large-range model for the

](https://www.lukew.com/ff/entry.asp?2030#:~:text=,range%20model%20for%20the)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.lukew.com&sz=32)lukew.com

LukeW | Generative Agents

* From the memory stream, records are retrieved as relevant to the agents' cognitive processes. A retrieval function that takes the agent's current situation as input and returns a subset of the memory stream to pass to a LLM, which then generates the final output behavior of the agents. * Retrieval is a linear combination of the recency, importance, and relevance function for each piece of memory. * The importance function is a prompt that asks the large-range model for the event status. You're basically asking the agent in natural language, this is who you are. How important is this to you?

](https://www.lukew.com/ff/entry.asp?2030#:~:text=,important%20is%20this%20to%20you)[

![Favicon](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)arxiv.org

[2304.03442] Generative Agents: Interactive Simulacra of Human Behavior

conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with

](https://arxiv.org/abs/2304.03442#:~:text=conversations%3B%20they%20remember%20and%20reflect,behaviors%3A%20for%20example%2C%20starting%20with)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.lukew.com&sz=32)lukew.com

LukeW | Generative Agents

* Retrieval is a linear combination of the recency, importance, and relevance function for each piece of memory. * The importance function is a prompt that asks the large-range model for the event status. You're basically asking the agent in natural language, this is who you are. How important is this to you? * The relevance function clusters records of agents' memory into higher-level abstract thoughts that are called reflections. Once they are synthesized, these reflections are just a type of memory and are just stored in the memory stream

](https://www.lukew.com/ff/entry.asp?2030#:~:text=,stored%20in%20the%20memory%20stream)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.getzep.com&sz=32)getzep.com

Zep Is The New State of the Art In Agent Memory

Zep Is The New State of the Art In Agent Memory Zep is a temporal knowledge graph-based memory layer for AI agents that continuously learns from user interactions and changing business data.

](https://www.getzep.com/blog/state-of-the-art-agent-memory/#:~:text=Zep%20Is%20The%20New%20State,interactions%20and%20changing%20business%20data)[

![Favicon](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)arxiv.org

Personalized LLM Response Generation with Parameterized User Memory Injection

from real-world bionic memory mechanism to propose a novel parameterized M emory-i njected approach using parameter-efficient fine-tuning (PEFT), combined with a Bayesian Optimisation searching strategy to achieve L LM P ersonalization(MiLP). Our MiLP takes advantage from the alignment between real- world memory mechanism and the LLM‚Äôs architecture. Extensive experiments have shown the superiority and effectiveness of MiLP. To encourage further research into this area, we are releasing our implementations^{1}^{1}1 https://github.com/MatthewKKai/MiLP.

](https://arxiv.org/html/2404.03565v2#:~:text=from%20real,1%7D1%20https%3A%2F%2Fgithub.com%2FMatthewKKai%2FMiLP)[

![Favicon](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)arxiv.org

Personalized LLM Response Generation with Parameterized User Memory Injection

with a Bayesian Optimisation searching strategy to achieve L LM P ersonalization(MiLP). Our MiLP takes advantage from the alignment between real- world memory mechanism and the LLM‚Äôs architecture. Extensive experiments have shown the superiority and effectiveness of MiLP. To encourage further research

](https://arxiv.org/html/2404.03565v2#:~:text=with%20a%20Bayesian%20Optimisation%20searching,To%20encourage%20further%20research)[

![Favicon](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)arxiv.org

Personalized LLM Response Generation with Parameterized User Memory Injection

context learning to organize the user historical content as prompts, providing them to LLM so that personal information can be considered (Petrov and Macdonald, 2023 ; Kang et¬†al., 27; Liu et¬†al., 2023 ). However, this paradigm suffers from the long-context window limitation of LLM (Liu et¬†al., 29). Memory-based approaches provide a solution by leveraging a memory to store user historical content. When a new user query comes, a retriever will first retrieve relevant user information from the memory to prompt LLM produce personalized responses (Dalvi et¬†al., 2022 ; Madaan et¬†al., 31; Lewis et¬†al., 2020 ; Zhang et¬†al., 33).

](https://arxiv.org/html/2404.03565v2#:~:text=context%20learning%20to%20organize%20the,33)[

![Favicon](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)arxiv.org

Personalized LLM Response Generation with Parameterized User Memory Injection

nature of similarity comparison retrieval process (Zhang et¬†al., 2023 ). Additionally, user historical content can be complex and noisy, posing difficulties for LLMs to focus on the most relevant information without a proper learnable process. To address this, recent studies have proposed parameterizing and projecting user historical content into a learnable representation space (Ning et¬†al., 34; Deng et¬†al., 2022 ; Zhong et¬†al., 36). Instead of using text to prompt LLMs, the learned user representations can be neglected in the LLM‚Äôs decoding process via cross-attention to enable personalized response generation. In this study, we take a further step by

](https://arxiv.org/html/2404.03565v2#:~:text=nature%20of%20similarity%20comparison%20retrieval,take%20a%20further%20step%20by)[

![Favicon](https://www.google.com/s2/favicons?domain=https://aclanthology.org&sz=32)aclanthology.org

Personalized large language models (LLMs) aim to tailor interactions, content, and recommendations to individual user preferences. While parameter-efficient fine-tuning (PEFT) methods excel in performance and generalization, they are costly and limit communal benefits when used individually. To this end, we introduce PERSONALIZED PIECES (PER-PCS) 1 ,

](https://aclanthology.org/2024.emnlp-main.371.pdf#:~:text=Personalized%20large%20language%20models%20,PCS%29%201)[

![Favicon](https://www.google.com/s2/favicons?domain=https://aclanthology.org&sz=32)aclanthology.org

and assemble personalized PEFT efficiently with collaborative efforts. PER-PCS involves selecting sharers, breaking their PEFT into pieces, and training gates for each piece. These pieces are added to a pool, from which target users can select and assemble personalized PEFT using their history data. This approach preserves privacy and enables fine-grained user modeling without excessive storage and computation demands. Experimental

](https://aclanthology.org/2024.emnlp-main.371.pdf#:~:text=and%20assemble%20personalized%20PEFT%20efficiently,Experimental)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

Efficient Interactions

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=Efficient%20Interactions)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

Secondly, by eliminating the need for customers to repeat information, the AI can respond immediately with data it already holds. Consequently, this efficiency streamlines every interaction.

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=Secondly%2C%20by%20eliminating%20the%20need,this%20efficiency%20streamlines%20every%20interaction)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.generational.pub&sz=32)generational.pub

Memory in AI Agents - by Kenn So - Generational

Memory transforms an AI from a basic question-answering tool into a conversation partner that can use pronouns and implicit references ...

](https://www.generational.pub/p/memory-in-ai-agents#:~:text=Memory%20transforms%20an%20AI%20from,pronouns%20and%20implicit%20references)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

1. E-Commerce

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=1.%20E)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

For instance, imagine your online store‚Äôs AI recommending products based on a customer‚Äôs past purchases. Moreover, it could even remember details like clothing sizes or favorite brands, thereby making shopping more convenient and tailored to individual tastes.

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=For%20instance%2C%20imagine%20your%20online,and%20tailored%20to%20individual%20tastes)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

For instance, imagine your online store‚Äôs AI recommending products based on a customer‚Äôs past purchases. Moreover, it could even remember details like clothing sizes or favorite brands, thereby making shopping more convenient and tailored to individual tastes.

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=For%20instance%2C%20imagine%20your%20online,and%20tailored%20to%20individual%20tastes)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

customer‚Äôs past purchases. Moreover, it could even remember details like clothing sizes or favorite brands, thereby making shopping more convenient and tailored to individual tastes.

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=customer%E2%80%99s%20past%20purchases,and%20tailored%20to%20individual%20tastes)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

4. Healthcare

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=4)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

2. Banking

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=2)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus.com

AI Agent Memory: Human Touch for Business Growth

3. Travel and Tourism

](https://www.qiscus.com/en/blog/ai-agent-memory/#:~:text=3)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.me.bot&sz=32)me.bot

AI-native memory for personalization & AGI - Me.bot

AI-native memory for personalization & AGI - Me.bot In the L2 System, the LLM serves as the core processor, with the context of the LLM acting as RAM and the memory system functioning as the hard ...

](https://www.me.bot/blog/ai-native-memory-for-personalization-agi#:~:text=AI,functioning%20as%20the%20hard)[

![Favicon](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)arxiv.org

Personalized LLM Response Generation with Parameterized User Memory Injection

Incorporating user historical information properly to LLM can be a key towards LLM personalization. Existing works can be concluded into three lines as illustrated in Fig. 1 . Text-Prompt based methods draw inspiration from in- context learning to organize the user historical content as prompts, providing them to LLM so that personal information can be considered (Petrov and Macdonald, 26; Kang et¬†al., 2023 ; Liu et¬†al., 28). However, this paradigm suffers from the long-context window limitation of LLM (Liu et¬†al., 2024). Memory-based approaches provide a solution by leveraging a memory to store user historical content. When a new user query comes, a

](https://arxiv.org/html/2404.03565v2#:~:text=Incorporating%20user%20historical%20information%20properly,new%20user%20query%20comes%2C%20a)

All Sources

[

langchain-ai.github

5



](https://langchain-ai.github.io/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://docs.mem0.ai&sz=32)docs.mem0

2



](https://docs.mem0.ai/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.graphlit.com&sz=32)graphlit

](https://www.graphlit.com/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://arxiv.org&sz=32)arxiv

5



](https://arxiv.org/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://medium.com&sz=32)medium

2



](https://medium.com/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://microsoft.github.io&sz=32)microsoft.github

](https://microsoft.github.io/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://redis.io&sz=32)redis

2



](https://redis.io/)[

news.ycombinator

2



](https://news.ycombinator.com/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.cognee.ai&sz=32)cognee

](https://www.cognee.ai/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://aclanthology.org&sz=32)aclanthology

](https://aclanthology.org/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.lukew.com&sz=32)lukew

](https://www.lukew.com/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.qiscus.com&sz=32)qiscus

2



](https://www.qiscus.com/)[

![Favicon](https://www.google.com/s2/favicons?domain=https://www.anthropic.com&sz=32)anthropic

](https://www.anthropic.com/)
```