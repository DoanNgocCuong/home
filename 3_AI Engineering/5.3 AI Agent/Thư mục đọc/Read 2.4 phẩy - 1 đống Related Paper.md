
2. Related Work Long-term Conversations for LLMs. LLMs have demonstrated the ability to engage in extended, coherent dialogues, yet maintaining context and consistency over long-term interactions remains a challenge. Maharana et al. (2024) introduced the LoCoMo dataset to assess LLMsâ€™ performance in sustained dialogues, showing their struggles with long-range temporal and causal understanding. Existing solutions can be broadly categorized into two approaches: (1) Architectural modifications, such as enhancing attention mechanisms (Liu et al., 2024a; Zhang et al., 2024a), optimizing KV caches (Li et al., 2024c; Liu et al., 2025), and refining position embeddings (Zhao et al., 2024; Zheng et al., 2024). These methods require white-box access to model internals, making them infeasible for 2 In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents proprietary or API-based LLMs. (2) Summarization-based methods, which condense long contexts into structured events or topics for direct conditioning or retrieval (Jiang et al., 2024; Li et al., 2024a; Lu et al., 2023; Wang et al., 2023). RMM falls into this category but explicitly addresses the issue of fragmented topics arising from fixed granularity and incorporates retrospective reflection to refine the retrieval process, encouraging more coherent and contextual responses. Memory-based Personalized Dialogue Agents. The development of memory-based personalized dialogue agents has further enhanced long-term interactions by enabling systems to retain and utilize information from past conversations (Bae et al., 2022). Early approaches, such as CoMemNN (Pei et al., 2021), introduce mechanisms to incrementally enrich user profiles during dialogues. However, collecting substantial annotations for training a personalized system for long-term use is hard (Tseng et al., 2024). Recent advancements focus on integrating LLMs with memory modules. For instance, the LD-Agent framework (Li et al., 2024b) employs long-, short-term memory banks to manage conversational history for retrieval. MemoryBank (Zhong et al., 2024) incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting Curve, enabling models to retrieve relevant memories considering recency. Theanine (Kim et al., 2024) introduces timeline-based retrieval and utilizes an additional LLM for refinement. These methods typically deploy fixed retrievers with a pre-defined granularity. In contrast, the proposed RMM approach facilitates adaptive retrieval with a revised retrieval granularity
---
Tuyá»‡t, Quá»‘c Ä‘ang á»Ÿ pháº§n **Related Work** vÃ  muá»‘n hiá»ƒu rÃµ cÃ¡c baseline hiá»‡n táº¡i cho bÃ i toÃ¡n **Long-term Memory trong LLMs**. DÆ°á»›i Ä‘Ã¢y lÃ  pháº§n tÃ³m táº¯t vÃ  phÃ¢n loáº¡i cÃ¡c hÆ°á»›ng tiáº¿p cáº­n **baseline**, chia thÃ nh 2 nhÃ³m chÃ­nh nhÆ° trong paper:

---

## ğŸ§  1. **Long-term Conversations for LLMs** â€“ duy trÃ¬ há»™i thoáº¡i dÃ i

### ğŸ’¡ Má»¥c tiÃªu:

GiÃºp LLM nhá»› Ä‘Æ°á»£c thÃ´ng tin xuyÃªn suá»‘t cÃ¡c lÆ°á»£t trÃ² chuyá»‡n dÃ i (long-context).

### ğŸ”§ CÃ³ 2 hÆ°á»›ng chÃ­nh:

---

### ğŸ“ A. **Architectural modifications** â€“ sá»­a â€œbÃªn trongâ€ mÃ´ hÃ¬nh

> âš ï¸ YÃªu cáº§u truy cáº­p mÃ´ hÃ¬nh gá»‘c â†’ khÃ´ng dÃ¹ng Ä‘Æ°á»£c vá»›i API nhÆ° GPT, Gemini

Gá»“m cÃ¡c ká»¹ thuáº­t nhÆ°:

- **Better attention**: cáº£i tiáº¿n attention Ä‘á»ƒ nhá»› xa hÆ¡n (Liu et al., 2024a)
    
- **KV cache optimization**: nÃ©n/lÆ°u láº¡i thÃ´ng tin cÃ¡c lÆ°á»£t cÅ© Ä‘á»ƒ sá»­ dá»¥ng tiáº¿p (Li et al., 2024c; Liu et al., 2025)
    
- **Better positional embeddings**: mÃ£ hÃ³a vá»‹ trÃ­ tá»‘t hÆ¡n Ä‘á»ƒ model hiá»ƒu thá»© tá»± (Zhao et al., 2024)
    

---

### ğŸ“„ B. **Summarization-based methods** â€“ tÃ³m táº¯t ná»™i dung dÃ i

> âš¡ PhÃ¹ há»£p vá»›i API-based LLMs

Gá»“m:

- **Topic/Key-point summary**: trÃ­ch thÃ´ng tin chÃ­nh tá»« cÃ¡c session thÃ nh dáº¡ng cÃ³ cáº¥u trÃºc.
    
- **Memory retrieval**: dÃ¹ng cÃ¡c Ä‘oáº¡n Ä‘Ã£ tÃ³m táº¯t Ä‘á»ƒ truy xuáº¥t láº¡i khi cáº§n.
    

TiÃªu biá»ƒu:

- **Jiang et al. (2024)**, **Lu et al. (2023)**, **Wang et al. (2023)** â€“ cÃ¡c ká»¹ thuáº­t RAG vá»›i tÃ³m táº¯t Ä‘a bÆ°á»›c.
    
- **Li et al. (2024a)** â€“ khung retrieve-then-reason.
    
- **RMM** â€“ lÃ  má»™t dáº¡ng Ä‘áº·c biá»‡t trong nhÃ³m nÃ y, giáº£i quyáº¿t nhÆ°á»£c Ä‘iá»ƒm "granularity cá»©ng" vÃ  dÃ¹ng há»“i tÆ°á»Ÿng Ä‘á»ƒ cáº£i thiá»‡n chá»n lá»c memory.
    

---

## ğŸ¤– 2. **Memory-based Personalized Dialogue Agents** â€“ trá»£ lÃ½ cÃ¡ nhÃ¢n hÃ³a cÃ³ trÃ­ nhá»›

### ğŸ’¡ Má»¥c tiÃªu:

GiÃºp trá»£ lÃ½ AI **hiá»ƒu rÃµ ngÆ°á»i dÃ¹ng theo thá»i gian** vÃ  giá»¯ Ä‘Æ°á»£c â€œphong cÃ¡ch trÃ² chuyá»‡n riÃªngâ€.

---

### ğŸ§± CÃ¡c baseline ná»•i báº­t:

|TÃªn mÃ´ hÃ¬nh|Äiá»ƒm ná»•i báº­t|
|---|---|
|**CoMemNN (2021)**|MÃ´ hÃ¬nh Ä‘áº§u tiÃªn xÃ¢y dá»±ng **profile cÃ¡ nhÃ¢n** qua nhiá»u lÆ°á»£t há»™i thoáº¡i.|
|**LD-Agent (2024)**|Táº¡o **memory bank** chia theo ngáº¯n háº¡n â€“ dÃ i háº¡n. Truy xuáº¥t dá»±a trÃªn **keyphrase**.|
|**MemoryBank (2024)**|Ghi nhá»› theo **Ä‘Æ°á»ng cong quÃªn (Ebbinghaus)**, truy xuáº¥t theo thá»i gian gáº§n Ä‘Ã¢y.|
|**Theanine (2024)**|Truy xuáº¥t theo **timeline**, sá»­ dá»¥ng thÃªm má»™t LLM Ä‘á»ƒ **refine láº¡i** Ä‘oáº¡n truy xuáº¥t.|

---

### âŒ Äiá»ƒm yáº¿u chung:

- Äá»u dÃ¹ng **retriever cá»‘ Ä‘á»‹nh + granularity cá»‘ Ä‘á»‹nh** (theo session hoáº·c turn).
    
- KhÃ´ng thá»ƒ **tá»± Ä‘iá»u chá»‰nh** cÃ¡ch truy xuáº¥t theo ngá»¯ cáº£nh khÃ¡c nhau hoáº·c ngÆ°á»i dÃ¹ng khÃ¡c nhau.
    

---

## âœ… Äiá»ƒm má»›i cá»§a **RMM (Reflective Memory Management)** so vá»›i baseline:

|TÃ­nh nÄƒng|RMM cÃ³?|CÃ¡c baseline cÃ³?|
|---|---|---|
|Ghi nhá»› theo chá»§ Ä‘á» linh hoáº¡t|âœ…|âŒ (cá»©ng nháº¯c theo session/turn)|
|Há»c tá»« lá»—i truy xuáº¥t (RL)|âœ…|âŒ|
|KhÃ´ng cáº§n fine-tune retriever|âœ…|âŒ|
|Há»— trá»£ API-based LLM|âœ…|Má»™t sá»‘ khÃ´ng|

---

Náº¿u Quá»‘c muá»‘n mÃ¬nh:

- Váº½ 1 báº£ng so sÃ¡nh rÃµ nÃ©t hÆ¡n giá»¯a RMM vÃ  cÃ¡c baseline?
    
- TÆ° váº¥n chá»n baseline phÃ¹ há»£p nháº¥t Ä‘á»ƒ so sÃ¡nh trong paper?
    
- Viáº¿t pháº§n â€œRelated Workâ€ máº«u tiáº¿ng Anh theo format academic?
    

MÃ¬nh lÃ m ngay!