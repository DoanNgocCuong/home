- [Your connected workspace for wiki, docs & projects | Notion](https://rogue-minibus-d10.notion.site/LLM-Generate-waiting-text-1c8fe13a5fa580acaaebd022d17fc004)

# LLM - Generate waiting text

**Requirement**:

- Input:
    
    - User text + conversation
- Output:
    
    - Response text:
        - Text Ä‘á»ƒ hiá»ƒn thá»‹ trong thá»i gian chá» Ä‘á»£i
            
        - Text phÃ¹ há»£p vá»›i conversation
            
        - Example:
            
            Pika: HÃ´m nay cáº­u tháº¿ nÃ o
            
            User: HÃ´m nay tá»› vui láº¯m, hÃ´m nay tá»› Ä‘áº¡t 10 Ä‘iá»ƒm Ä‘Ã³
            
            Pika: **Wow, Cáº­u lÃ m bÃ i Ä‘Æ°á»£c 10 Ä‘iá»ƒm cÆ¡ Ã¡.** Cáº­u giá»i quÃ¡, â€¦..
            
- **Performance:**
    
    - CCU: >30
    - Response time: 95% < 200 ms
    - GPU: 2x3090
    - Other Metrics:
        - TTFT (Time to first token): < 50 ms
        - Inference Token expect: < 10 ms
        - Fit Score ( dÃ¹ng OpenAI Ä‘Ã¡nh giÃ¡) : expect: 85% case há»£p lÃ½
- **Plan:**
    
    - Æ¯u tiÃªn cÃ³ thá»ƒ sá»­ dá»¥ng prompt thay vÃ¬ tuning
    - Sá»­ dá»¥ng cÃ¡c model khoáº£ng 1B-3B â†’ test hiá»‡u nÄƒng
    - Serving platform: vllm, tensorRT, lmdeploy, Triton server + openAI compatible
    - Streaming: FastAPI StreamResponse
    - Tuning plan (báº¥t Ä‘áº¯c dÄ©): Synthetic Data + LoRA hoáº·c distill model

[Performance Benchmark](https://www.notion.so/Performance-Benchmark-1cffe13a5fa580aa83b5f7edc6719dc3?pvs=21)


Fine tuning 
# Performance Benchmark

- GPU 3090 24GB:
    - Gemma3-1b-it
        
        - Usage: 20/24GB
            
        - Performance:
            
            - CCU - TTFT:
                
                - CCU: 20
                - Avg Input tokens: 512 tokens
                - Avg output tokens: 128 tokens
                - perform:
                
                ![image.png](attachment:b637eef5-fcac-4755-8b33-cfc1af6e2dca:image.png)
                
        - Result:
            
            - sample1:
                
                - input: Tá»› thÃ­ch Äƒn kem láº¯m
                - output: Ai cÅ©ng thÃ­ch kem Ä‘Ã³ quÃ¡ Ä‘i! ðŸ¦âœ¨\n
            - sample2:
                
                - input: Tá»› má»›i bá»‹ máº¹ máº¯ng
                - output: Ã”i khÃ´ng, tá»› má»›i bá»‹ máº¹ máº¯ng Ã¡? ðŸ¥º Máº¹ tá»› cÅ©ng buá»“n láº¯m Ä‘Ã³! ðŸ˜”\n
            - sample3:
                
                input: hÃ´m nay Ä‘áº¹p trá»i ghÃª, tá»› muá»‘n Ä‘i chÆ¡i cÃ´ng viÃªn quÃ¡
                
                output: Ã”i dÃªs, hÃ´m nay trá»i Ä‘áº¹p quÃ¡ tháº­t! CÃ´ng viÃªn thÃ¬ sao nhá»‰? CÃ³ chÆ¡i gÃ¬ fun khÃ´ng? ðŸ˜Š\n
                
    - Qwen2.5-1.5b-it
        
        - Usage: 20/24GB
            
        - Performance:
            
            - CCU - TTFT:
                
                - CCU: 20
                - Avg Input tokens: 512 tokens
                - Avg output tokens: 128 tokens
                - perform:
                
                ![image.png](attachment:65edfdc7-a05e-4514-86ba-cf73936a1d5b:image.png)
                
        - Result:
            
            - sample1:
                
                - input: Tá»› thÃ­ch Äƒn kem láº¯m
                - output: Äáº¹p quÃ¡! Ä‚n kem cÅ©ng ráº¥t ngon pháº£i khÃ´ng? Tá»› cÅ©ng thÃ­ch Äƒn kem láº¯m.
            - sample2:
                
                - input: Tá»› má»›i bá»‹ máº¹ máº¯ng
                - output: ÄÃ³ lÃ  chuyá»‡n bÃ¬nh thÆ°á»ng thÃ´i, con Ä‘á»«ng buá»“n nhÃ©. MÃ¬nh sáº½ giÃºp con giáº£i quyáº¿t váº¥n Ä‘á» Ä‘Ã³.
            - sample3:
                
                input: hÃ´m nay Ä‘áº¹p trá»i ghÃª, tá»› muá»‘n Ä‘i chÆ¡i cÃ´ng viÃªn quÃ¡
                
                output: Äáº¹p tháº­t Ä‘Ã³! CÃ´ng viÃªn cháº¯c cháº¯n sáº½ ráº¥t vui váº». Báº¡n cÃ³ thá»ƒ mang theo Ä‘á»“ chÆ¡i vÃ  trÃ² chÆ¡i yÃªu thÃ­ch cá»§a báº¡n.