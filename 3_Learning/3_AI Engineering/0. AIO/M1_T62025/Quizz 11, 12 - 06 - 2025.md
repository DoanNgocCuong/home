2. 
```python
data = [1, 9, 3, -3] 
data.sort() 
print(data)
```

Sort này là sort từ bé đến lớn hay lớn đến bé

3. Bài hay phết 
```python 
def function_helper(x, data):

    for i in data:

        if x == i:

            return 0

    return 1

  
  

print(function_helper(9, []))

print(function_helper(9, [9]))

print(function_helper(8, [9]))

print(function_helper(8, [9, 8]))

print(function_helper(1, [9, 8]))
```


---
**Giải Thích Ví Dụ: Thay Thế If-else Bằng Các Phương Pháp Toán Học, One-hot Encoding và Softmax**

Trong ví dụ sau, mục tiêu là tính một giá trị đầu ra dựa vào hai biến đầu vào `a` (nhãn, chỉ nhận giá trị 0, 1 hoặc 2) và `b` (một số thực dương). Tùy thuộc vào giá trị của `a`, ta sẽ áp dụng một trong ba công thức:  
- Nếu `a == 0`, kết quả là `b * b`  
- Nếu `a == 1`, kết quả là `sqrt(b)`  
- Nếu `a == 2`, kết quả là `b`  

**1. Phương Pháp Truyền Thống (If-else)**  
```python
import math

def traditional_approach(a, b):
    if a == 0:
        return b * b
    elif a == 1:
        return math.sqrt(b)
    elif a == 2:
        return b
    else:
        return 0
```
Trong cách này, logic được kiểm tra tuần tự:

- Đầu tiên, câu lệnh `if a == 0` được đánh giá. Nếu đúng, hàm trả `b * b` và dừng.  
- Nếu không, chuyển sang `elif a == 1`, tiếp tục cho đến khi tìm được nhánh phù hợp.  
- Nếu `a` không khớp bất cứ giá trị nào trong 0, 1, 2, hàm trả về `0`.  

Mặc dù đơn giản nhưng khi số nhãn (các trường hợp) tăng lên, chuỗi `if-elif-else` trở nên dài dòng, khó đọc và khó mở rộng.

**2. Phương Pháp Toán Học Kết Hợp One-hot Encoding**  
Thay vì dùng rẽ nhánh, ta tạo hai cấu trúc:

- Một danh sách `weights` lưu trước kết quả của ba công thức ứng với `a = 0, 1, 2`.  
- Một vector `selector` (one-hot encoding) chỉ có đúng một phần tử bằng 1, tại vị trí tương ứng với `a`, các phần tử còn lại là 0.

```python
import math

def mathematical_approach(a, b):
    # Tính trước ba kết quả tương ứng với a=0,1,2
    weights = [b * b, math.sqrt(b), b]
    # One-hot selector: [1,0,0] nếu a=0; [0,1,0] nếu a=1; [0,0,1] nếu a=2
    selector = [1 if a == i else 0 for i in range(3)]
    # Kết quả là tích vô hướng giữa weights và selector
    result = sum(w * s for w, s in zip(weights, selector))
    return result
```

Cơ chế hoạt động:

- Danh sách `weights` lưu sẵn:  
  • `weights[0] = b*b`  
  • `weights[1] = √b`  
  • `weights[2] = b`  
- Vector `selector` chỉ có một số 1 ở vị trí `a`, giả sử `a = 1` thì `selector = [0, 1, 0]`.  
- Tích vô hướng giữa hai vector này bỏ qua mọi phần tử ngoài vị trí `a`, chỉ giữ giá trị mong muốn.  

Ưu điểm:

- Mã ngắn gọn, dễ mở rộng: chỉ cần thêm thành phần tương ứng trong `weights` và tăng độ dài `selector`.  
- Tận dụng tính toán vector hóa của NumPy hoặc các thư viện GPU để chạy song song.

**3. Phương Pháp Softmax**  
Thay vì chọn duy nhất một nhánh, đôi khi ta muốn kết hợp đầu ra của nhiều công thức hoặc lựa chọn một nhánh dựa trên xác suất. Hàm Softmax chuyển tập hợp “điểm số” thành phân phối xác suất:

$$
\text{softmax}(x_i) \;=\; \frac{e^{x_i}}{\sum_{j} e^{x_j}}
$$

Ví dụ, nếu ta định nghĩa ba hàm số tương ứng với nhãn 0, 1, 2 là `f0(b) = b*b`, `f1(b) = √b` và `f2(b) = b`, thì:

```python
import math

def softmax(values):
    exp_vals = [math.exp(v) for v in values]
    total = sum(exp_vals)
    return [v / total for v in exp_vals]

def softmax_approach(a, b):
    # Tạo mảng điểm số cho ba hàm f0, f1, f2
    scores = [b*b, math.sqrt(b), b]
    # Tính phân phối xác suất
    probs = softmax(scores)
    # Chọn nhãn có xác suất lớn nhất
    best_label = probs.index(max(probs))
    # Trả về kết quả của hàm tương ứng
    return scores[best_label]
```

Cách tiếp cận này:

- Tính lần lượt ba “điểm số” (scores).  
- Áp dụng Softmax để nhận phân phối xác suất `[p0, p1, p2]`.  
- Chọn nhãn có xác suất cao nhất (`argmax`) làm kết quả.  

Ưu điểm:

- Cho phép mô hình học và điều chỉnh trọng số sao cho hàm số ưu tiên nhãn phù hợp thông qua tối ưu hóa.  
- Dễ kết hợp trong kiến trúc neural network, tận dụng gradient descent để tối ưu hóa trực tiếp các hàm số.

---

Tóm lại, ba phương pháp trên đều thay thế chuỗi `if-else` bằng cách biến các nhánh điều kiện thành các phép toán trên vector. Khi số nhánh tăng lên hoặc khi muốn mở rộng tính học được (learning‐based), các kỹ thuật này giúp mã nguồn cô đọng, dễ tối ưu và dễ bảo trì.