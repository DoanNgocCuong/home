{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 30: Calculus in Machine Learning- Derivatives in Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DÆ°á»›i Ä‘Ã¢y lÃ  báº£n **tÃ³m táº¯t ná»™i dung tÃ i liá»‡u \"AIO2025 - Warmup: Calculus in Machine Learning â€“ Derivatives in AI\" ngÃ y 8/3/2025 cá»§a HoÃ ng-NguyÃªn VÅ©**:\n",
    "\n",
    "### ğŸ”¹ **2. Äáº¡o hÃ m trong há»c mÃ¡y**\n",
    "#### 2.1. Äá»‹nh nghÄ©a vÃ  Ã½ nghÄ©a:\n",
    "- Äáº¡o hÃ m Ä‘o tá»‘c Ä‘á»™ thay Ä‘á»•i cá»§a hÃ m sá»‘.\n",
    "- Trong AI, giÃºp Ä‘iá»u chá»‰nh tham sá»‘ Ä‘á»ƒ **giáº£m lá»—i dá»± Ä‘oÃ¡n** (Loss Function).\n",
    "\n",
    "#### 2.2. **Gradient Descent**:\n",
    "- Thuáº­t toÃ¡n cáº­p nháº­t tham sá»‘ theo hÆ°á»›ng ngÆ°á»£c láº¡i vá»›i gradient:\n",
    "  \n",
    "  `Î¸_{t+1} = Î¸_t - Î±âˆ‡J(Î¸)`\n",
    "  \n",
    "  Trong Ä‘Ã³:\n",
    "  - `Î±` lÃ  **learning rate**.\n",
    "  - `âˆ‡J(Î¸)` lÃ  **gradient cá»§a hÃ m máº¥t mÃ¡t**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/Day30_Gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ğŸŸª **PhÃ¢n tÃ­ch chi tiáº¿t cÃ¡c thÃ nh pháº§n trong áº£nh:**\n",
    "\n",
    "| ThÃ nh pháº§n | Ã nghÄ©a |\n",
    "|------------|--------|\n",
    "| **Random initial value** | GiÃ¡ trá»‹ khá»Ÿi táº¡o ngáº«u nhiÃªn cá»§a tham sá»‘ $ w $ â€“ lÃ  nÆ¡i báº¯t Ä‘áº§u hÃ nh trÃ¬nh cá»§a Gradient Descent. |\n",
    "| **Learning step (mÅ©i tÃªn cong)** | Má»—i mÅ©i tÃªn lÃ  **1 bÆ°á»›c cáº­p nháº­t** theo cÃ´ng thá»©c: $\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\nabla J(\\theta)$. |\n",
    "| **HÆ°á»›ng Ä‘i xuá»‘ng (bÃªn pháº£i)** | Do Ä‘áº¡o hÃ m ban Ä‘áº§u **Ã¢m**, nÃªn trá»« Ä‘i Ä‘áº¡o hÃ m Ã¢m â‡’ **tham sá»‘ tÄƒng lÃªn**, Ä‘i vá» bÃªn pháº£i. |\n",
    "| **Tiá»‡m cáº­n Ä‘iá»ƒm Minimum** | CÃ¡c bÆ°á»›c nhá» dáº§n vÃ¬ gradient gáº§n Ä‘iá»ƒm cá»±c tiá»ƒu sáº½ nhá» â†’ mÃ´ hÃ¬nh Ä‘ang há»™i tá»¥. |\n",
    "| **Minimum (vá»‹ trÃ­ mÃ u vÃ ng)** | LÃ  giÃ¡ trá»‹ tá»‘i Æ°u mÃ  mÃ´ hÃ¬nh mong muá»‘n Ä‘áº¡t Ä‘Æ°á»£c: **loss tháº¥p nháº¥t**. |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Œ **Giáº£i thÃ­ch Ã½ nghÄ©a Ä‘áº¡o hÃ m á»Ÿ tá»«ng bÆ°á»›c:**\n",
    "- LÃºc má»›i báº¯t Ä‘áº§u: Ä‘Æ°á»ng dá»‘c máº¡nh â‡’ **Ä‘áº¡o hÃ m lá»›n**, bÆ°á»›c nháº£y lá»›n.\n",
    "- Gáº§n cá»±c tiá»ƒu: Ä‘áº¡o hÃ m nhá» â‡’ **bÆ°á»›c nháº£y ngáº¯n láº¡i**, giÃºp khÃ´ng vÆ°á»£t quÃ¡ minimum.\n",
    "- Náº¿u bÆ°á»›c quÃ¡ lá»›n (learning rate Î± lá»›n): cÃ³ thá»ƒ **nháº£y qua nháº£y láº¡i**, khÃ´ng há»™i tá»¥.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§ª Káº¿t luáº­n:\n",
    "áº¢nh nÃ y cho tháº¥y:\n",
    "> Gradient Descent giá»‘ng nhÆ° báº¡n Ä‘ang **Ä‘i xuá»‘ng má»™t thung lÅ©ng**, báº¯t Ä‘áº§u tá»« má»™t Ä‘iá»ƒm trÃªn sÆ°á»n dá»‘c, tá»«ng bÆ°á»›c tiáº¿n vá» **Ä‘Ã¡y thung lÅ©ng (minimum)** â€“ nÆ¡i chi phÃ­ (Cost) nhá» nháº¥t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ğŸ‘‰ **CÃ´ng thá»©c nháº¯c láº¡i:**\n",
    "$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\nabla J(\\theta)\n",
    "$\n",
    "- $\\nabla J(\\theta)$: Ä‘áº¡o hÃ m (gradient) cá»§a hÃ m máº¥t mÃ¡t táº¡i $\\theta$.\n",
    "- $\\alpha$: tá»‘c Ä‘á»™ há»c (learning rate).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‰ Khi **Ä‘áº¡o hÃ m dÆ°Æ¡ng** ($\\nabla J(\\theta) > 0$):\n",
    "- Äiá»u nÃ y nghÄ©a lÃ  **hÃ m máº¥t mÃ¡t Ä‘ang tÄƒng** khi $\\theta$ tÄƒng.\n",
    "- Ta cáº§n **giáº£m giÃ¡ trá»‹ cá»§a $\\theta$** Ä‘á»ƒ lÃ m **giáº£m máº¥t mÃ¡t**.\n",
    "- CÃ´ng thá»©c sáº½ trá»« Ä‘i sá»‘ dÆ°Æ¡ng â‡’ $\\theta_{t+1} < \\theta_t$ â‡’ **di chuyá»ƒn vá» bÃªn trÃ¡i** trÃªn Ä‘á»“ thá»‹.\n",
    "\n",
    "### ğŸ“ˆ Khi **Ä‘áº¡o hÃ m Ã¢m** ($\\nabla J(\\theta) < 0$):\n",
    "- HÃ m máº¥t mÃ¡t Ä‘ang **giáº£m** khi $\\theta$ tÄƒng.\n",
    "- Ta cáº§n **tÄƒng $\\theta$** Ä‘á»ƒ tiáº¿p tá»¥c lÃ m máº¥t mÃ¡t giáº£m.\n",
    "- Trá»« Ä‘i má»™t sá»‘ Ã¢m â‡’ $\\theta_{t+1} > \\theta_t$ â‡’ **di chuyá»ƒn vá» bÃªn pháº£i** trÃªn Ä‘á»“ thá»‹.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  NÃ³i Ä‘Æ¡n giáº£n:\n",
    "- Gradient cho ta biáº¿t **nÃªn Ä‘i vá» hÆ°á»›ng nÃ o Ä‘á»ƒ giáº£m lá»—i**.\n",
    "- Náº¿u dá»‘c Ä‘i lÃªn (Ä‘áº¡o hÃ m dÆ°Æ¡ng) â†’ Ä‘i xuá»‘ng (giáº£m $\\theta$).\n",
    "- Náº¿u dá»‘c Ä‘i xuá»‘ng (Ä‘áº¡o hÃ m Ã¢m) â†’ Ä‘i lÃªn (tÄƒng $\\theta$).\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ’¡ **HÃ¬nh dung nhÆ° Ä‘ang leo nÃºi ngÆ°á»£c chiá»u:**\n",
    "> Gradient giá»‘ng nhÆ° Ä‘á»™ dá»‘c cá»§a nÃºi. Gradient dÆ°Æ¡ng â†’ báº¡n Ä‘ang dá»‘c lÃªn bÃªn pháº£i, nÃªn báº¡n pháº£i Ä‘i ngÆ°á»£c (vá» trÃ¡i). Gradient Ã¢m â†’ báº¡n dá»‘c xuá»‘ng bÃªn pháº£i, nÃªn tiáº¿p tá»¥c Ä‘i sang pháº£i.\n",
    "\n",
    "---\n",
    "\n",
    "Náº¿u Quá»‘c muá»‘n mÃ¬nh váº½ hÃ¬nh minh há»a hoáº·c lÃ m vÃ­ dá»¥ cá»¥ thá»ƒ vá»›i sá»‘, mÃ¬nh sáº½ lÃ m luÃ´n nhÃ©!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¹ **3. Äáº¡o hÃ m cá»§a hÃ m há»£p vÃ  Backpropagation**\n",
    "#### 3.1. **Äáº¡o hÃ m cá»§a hÃ m há»£p**:\n",
    "- Máº¡ng nÆ¡-ron gá»“m nhiá»u lá»›p liÃªn tiáº¿p.\n",
    "- DÃ¹ng **Chain Rule** Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m tá»•ng há»£p cá»§a cÃ¡c lá»›p:\n",
    "  \n",
    "  `dL/dW = dL/da Ã— da/dz Ã— dz/dW`\n",
    "\n",
    "#### 3.2. **Backpropagation**:\n",
    "- Lan truyá»n ngÆ°á»£c dÃ¹ng Chain Rule Ä‘á»ƒ tÃ­nh toÃ¡n gradient theo tá»«ng lá»›p:\n",
    "  \n",
    "  `Î´(l) = (W(l))áµ— Î´(l+1) âˆ˜ fâ€²(z(l))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../image/Day30_Backpropagation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ğŸ” **What? â€“ ÄÃ¢y lÃ  gÃ¬?**\n",
    "\n",
    "ÄÃ¢y lÃ  **thuáº­t toÃ¡n Backpropagation** â€“ hay cÃ²n gá»i lÃ  **Lan truyá»n ngÆ°á»£c**.\n",
    "\n",
    "Cá»¥ thá»ƒ:\n",
    "- LÃ  má»™t phÆ°Æ¡ng phÃ¡p tÃ­nh **gradient** (Ä‘áº¡o hÃ m riÃªng) cá»§a hÃ m máº¥t mÃ¡t theo tá»«ng **trá»ng sá»‘** trong máº¡ng nÆ¡-ron.\n",
    "- Má»¥c tiÃªu: **Cáº­p nháº­t trá»ng sá»‘** cá»§a máº¡ng nÆ¡-ron sao cho Ä‘áº§u ra (output) cÃ ng gáº§n nhÃ£n tháº­t cÃ ng tá»‘t.\n",
    "\n",
    "### Trong hÃ¬nh:\n",
    "- **Inputs**: cÃ¡c Ä‘áº§u vÃ o a, b, c, d.\n",
    "- **Hidden layers**: hai lá»›p áº©n vá»›i cÃ¡c node hâ‚, hâ‚‚, hâ‚ƒ, ...\n",
    "- **Output**: má»™t node Ä‘áº§u ra \"o\".\n",
    "- CÃ¡c **mÅ©i tÃªn cong ngÆ°á»£c** Ä‘áº¡i diá»‡n cho **lan truyá»n gradient ngÆ°á»£c** tá»« output vá» láº¡i hidden layers Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ (weights).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤” **Why? â€“ Táº¡i sao cáº§n Backpropagation?**\n",
    "\n",
    "TrÆ°á»›c Ä‘Ã¢y, khi máº¡ng nÆ¡-ron cÃ²n má»›i, ngÆ°á»i ta **khÃ´ng biáº¿t cÃ¡ch huáº¥n luyá»‡n hiá»‡u quáº£** vÃ¬ khÃ´ng cÃ³ cÃ¡ch nÃ o lan truyá»n lá»—i tá»« output vá» láº¡i cÃ¡c lá»›p trÆ°á»›c.\n",
    "\n",
    "**Backpropagation ra Ä‘á»i** (Ä‘Æ°á»£c phÃ¡t triá»ƒn máº¡nh tá»« nÄƒm 1986 bá»Ÿi Rumelhart et al.) Ä‘á»ƒ:\n",
    "- **Tá»‘i Æ°u hÃ³a** mÃ´ hÃ¬nh báº±ng cÃ¡ch tÃ­nh toÃ¡n chÃ­nh xÃ¡c cÃ¡c Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m máº¥t mÃ¡t vá»›i tá»«ng trá»ng sá»‘.\n",
    "- GiÃºp **huáº¥n luyá»‡n máº¡ng nÆ¡-ron nhiá»u lá»›p** (deep learning) trá»Ÿ nÃªn kháº£ thi vÃ  hiá»‡u quáº£.\n",
    "\n",
    "Náº¿u khÃ´ng cÃ³ backpropagation, viá»‡c huáº¥n luyá»‡n máº¡ng nhiá»u lá»›p sáº½ ráº¥t tá»‘n thá»i gian hoáº·c khÃ´ng kháº£ thi.\n",
    "\n",
    "---\n",
    "![](../image/Day30_Backpropagation.png)\n",
    "---\n",
    "\n",
    "## âš™ï¸ **How? â€“ CÃ¡ch hoáº¡t Ä‘á»™ng cá»§a Backpropagation**\n",
    "\n",
    "Thuáº­t toÃ¡n gá»“m 2 giai Ä‘oáº¡n chÃ­nh:\n",
    "\n",
    "### 1. **Forward pass**:\n",
    "- TÃ­nh toÃ¡n Ä‘áº§u ra cá»§a máº¡ng dá»±a trÃªn input vÃ  trá»ng sá»‘ hiá»‡n táº¡i.\n",
    "- TÃ­nh **hÃ m máº¥t mÃ¡t** giá»¯a output mÃ´ hÃ¬nh vÃ  giÃ¡ trá»‹ thá»±c táº¿ (ground truth).\n",
    "\n",
    "### 2. **Backward pass (Backpropagation)**:\n",
    "- TÃ­nh **Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t** theo tá»«ng trá»ng sá»‘, sá»­ dá»¥ng **Chain Rule**:\n",
    "  \n",
    "  $\n",
    "  \\delta^{(l)} = (W^{(l)})^T \\delta^{(l+1)} \\circ f'(z^{(l)})\n",
    "  $\n",
    "  Trong Ä‘Ã³:\n",
    "  - $\\delta^{(l)}$ lÃ  gradient á»Ÿ lá»›p $l$.\n",
    "  - $W^{(l)}$ lÃ  ma tráº­n trá»ng sá»‘ tá»« lá»›p $l$.\n",
    "  - $f'(z^{(l)})$ lÃ  Ä‘áº¡o hÃ m cá»§a hÃ m kÃ­ch hoáº¡t táº¡i lá»›p $l$.\n",
    "  - $\\circ$ lÃ  phÃ©p nhÃ¢n tá»«ng pháº§n tá»­ (Hadamard product).\n",
    "\n",
    "- DÃ¹ng gradient Ä‘Ã³ Ä‘á»ƒ **cáº­p nháº­t trá»ng sá»‘** theo cÃ´ng thá»©c:\n",
    "  \n",
    "  $\n",
    "  W := W - \\alpha \\cdot \\frac{\\partial L}{\\partial W}\n",
    "  $\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ TÃ³m láº¡i:\n",
    "\n",
    "| Má»¥c | Ná»™i dung |\n",
    "|-----|----------|\n",
    "| **What** | Backpropagation â€“ thuáº­t toÃ¡n tÃ­nh gradient lan truyá»n tá»« output vá» cÃ¡c lá»›p trÆ°á»›c. |\n",
    "| **Why** | GiÃºp máº¡ng nÆ¡-ron há»c Ä‘Æ°á»£c báº±ng cÃ¡ch cáº­p nháº­t chÃ­nh xÃ¡c cÃ¡c trá»ng sá»‘. |\n",
    "| **How** | Sá»­ dá»¥ng Chain Rule Ä‘á»ƒ lan truyá»n gradient vÃ  cáº­p nháº­t trá»ng sá»‘ trong quÃ¡ trÃ¬nh há»c. |\n",
    "\n",
    "---\n",
    "\n",
    "Náº¿u Quá»‘c muá»‘n mÃ¬nh viáº¿t má»™t Ä‘oáº¡n **minh há»a báº±ng Python**, hoáº·c giáº£i thÃ­ch vá»›i **ngÃ´n ngá»¯ Ä‘Æ¡n giáº£n nhÆ° Ä‘ang dáº¡y há»c sinh A2â€“B1**, mÃ¬nh ráº¥t sáºµn sÃ ng nhÃ©!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¹ **4. BÃ i táº­p thá»±c hÃ nh Python (á»©ng dá»¥ng Ä‘áº¡o hÃ m)**\n",
    "\n",
    "#### ğŸ”¸ **BÃ i 1. Sigmoid vÃ  Ä‘áº¡o hÃ m**\n",
    "- HÃ m: `Ïƒ(x) = 1 / (1 + e^(-x))`\n",
    "- Äáº¡o hÃ m: `Ïƒâ€²(x) = Ïƒ(x) * (1 - Ïƒ(x))`\n",
    "- TÃ­nh cho `x = [-2, -1, 0, 1, 2]`\n",
    "\n",
    "#### ğŸ”¸ **BÃ i 2. MSE vÃ  Ä‘áº¡o hÃ m**\n",
    "- Loss: `L = 1/n âˆ‘(y_i - Å·_i)^2`\n",
    "- Gradient: `âˆ‚L/âˆ‚Å·_i = -2/n * (y_i - Å·_i)`\n",
    "- TÃ­nh vá»›i:  \n",
    "  `y = [3, 5, 2, 8]`, `Å· = [2.5, 4.8, 2.1, 7.5]`\n",
    "\n",
    "#### ğŸ”¸ **BÃ i 3. ReLU vÃ  Ä‘áº¡o hÃ m**\n",
    "- HÃ m: `ReLU(x) = max(0, x)`\n",
    "- Äáº¡o hÃ m:\n",
    "  - `1` náº¿u `x > 0`\n",
    "  - `0` náº¿u `x â‰¤ 0`\n",
    "\n",
    "#### ğŸ”¸ **BÃ i 4. Softmax vÃ  Ä‘áº¡o hÃ m**\n",
    "- HÃ m: `Ïƒ(z)_i = e^{z_i} / âˆ‘ e^{z_j}`\n",
    "- Äáº¡o hÃ m:\n",
    "  - Náº¿u `i = j`: `Ïƒ_i(1 - Ïƒ_i)`\n",
    "  - Náº¿u `i â‰  j`: `-Ïƒ_i * Ïƒ_j`\n",
    "- TÃ­nh vá»›i: `z = [2.0, 1.0, 0.1]`\n",
    "\n",
    "#### ğŸ”¸ **BÃ i 5. Tanh vÃ  Ä‘áº¡o hÃ m**\n",
    "- HÃ m: `tanh(x) = (e^x - e^{-x}) / (e^x + e^{-x})`\n",
    "- Äáº¡o hÃ m: `1 - tanh^2(x)`\n",
    "- TÃ­nh vá»›i: `x = [-2, -1, 0, 1, 2]`\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Tá»•ng káº¿t**\n",
    "TÃ i liá»‡u giÃºp ngÆ°á»i há»c náº¯m vá»¯ng **á»©ng dá»¥ng Ä‘áº¡o hÃ m trong AI**, tá»« lÃ½ thuyáº¿t Ä‘áº¿n thá»±c hÃ nh vá»›i Python. ÄÃ¢y lÃ  kiáº¿n thá»©c ná»n táº£ng Ä‘á»ƒ hiá»ƒu cÃ¡ch **huáº¥n luyá»‡n vÃ  tá»‘i Æ°u mÃ´ hÃ¬nh há»c mÃ¡y**, Ä‘áº·c biá»‡t trong **Deep Learning**.\n",
    "\n",
    "---\n",
    "\n",
    "Náº¿u báº¡n cáº§n **mÃ£ hoÃ n chá»‰nh cÃ¡c bÃ i Python**, mÃ¬nh cÃ³ thá»ƒ bá»• sung luÃ´n nhÃ©!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ráº¥t sáºµn lÃ²ng Quá»‘c Æ¡i! DÆ°á»›i Ä‘Ã¢y lÃ  **báº£ng chi tiáº¿t hÆ¡n** ná»™i dung cÃ¡c bÃ i thá»±c hÃ nh Ä‘áº¡o hÃ m trong AI, theo Ä‘Ãºng format **What â€“ Why â€“ How**, vá»›i lá»i giáº£i thÃ­ch cá»¥ thá»ƒ, thá»±c táº¿, dá»… hiá»ƒu â€“ ráº¥t thÃ­ch há»£p cho há»c viÃªn má»›i hoáº·c dÃ¹ng Ä‘á»ƒ giáº£ng dáº¡y:\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ **Báº£ng chi tiáº¿t â€“ 5 bÃ i thá»±c hÃ nh Ä‘áº¡o hÃ m trong AI**\n",
    "\n",
    "| **BÃ i** | **What â€“ ÄÃ¢y lÃ  gÃ¬?** | **Why â€“ Táº¡i sao cáº§n?** | **How â€“ DÃ¹ng ra sao?** |\n",
    "|--------|-------------------------|--------------------------|--------------------------|\n",
    "| **1. Sigmoid** | LÃ  hÃ m kÃ­ch hoáº¡t Ä‘Æ°a má»i giÃ¡ trá»‹ thá»±c vá» khoáº£ng (0, 1), hÃ¬nh dáº¡ng cong hÃ¬nh chá»¯ S. | - GiÃºp chuyá»ƒn output thÃ nh xÃ¡c suáº¥t, dÃ¹ng nhiá»u trong phÃ¢n loáº¡i nhá»‹ phÃ¢n (nhÆ°: cÃ³ bá»‹ bá»‡nh khÃ´ng, email cÃ³ pháº£i spam khÃ´ng).<br>- LÃ m cho output cÃ³ thá»ƒ so sÃ¡nh Ä‘Æ°á»£c vá»›i nhÃ£n tháº­t (label = 0 hoáº·c 1). | - Vá»›i má»—i giÃ¡ trá»‹ Ä‘áº§u vÃ o x, Ã¡p dá»¥ng sigmoid Ä‘á»ƒ ra xÃ¡c suáº¥t.<br>- Äáº¡o hÃ m giÃºp tÃ­nh xem cáº§n Ä‘iá»u chá»‰nh Ä‘áº§u vÃ o bao nhiÃªu Ä‘á»ƒ output tiáº¿n gáº§n Ä‘Ãºng hÆ¡n.<br>- DÃ¹ng Ä‘áº¡o hÃ m trong quÃ¡ trÃ¬nh cáº­p nháº­t trá»ng sá»‘ (backpropagation). |\n",
    "| **2. MSE (Mean Squared Error)** | HÃ m máº¥t mÃ¡t Ä‘o Ä‘á»™ lá»‡ch trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c táº¿. | - GiÃºp biáº¿t mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tá»‘t Ä‘áº¿n Ä‘Ã¢u (dÃ¹ng trong há»“i quy).<br>- CÃ ng gáº§n giÃ¡ trá»‹ thá»±c táº¿, MSE cÃ ng nhá» â†’ mÃ´ hÃ¬nh cÃ ng tá»‘t. | - TÃ­nh MSE Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh.<br>- DÃ¹ng Ä‘áº¡o hÃ m Ä‘á»ƒ biáº¿t má»—i dá»± Ä‘oÃ¡n sai bao nhiÃªu vÃ  nÃªn Ä‘iá»u chá»‰nh theo hÆ°á»›ng nÃ o.<br>- GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n tháº¥p hÆ¡n thá»±c táº¿ â†’ tÄƒng; ngÆ°á»£c láº¡i â†’ giáº£m. |\n",
    "| **3. ReLU (Rectified Linear Unit)** | HÃ m kÃ­ch hoáº¡t giá»¯ giÃ¡ trá»‹ dÆ°Æ¡ng, cÃ²n giÃ¡ trá»‹ Ã¢m thÃ¬ tráº£ vá» 0. Ráº¥t Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£. | - LÃ m máº¡ng há»c nhanh hÆ¡n vÃ  trÃ¡nh tÃ¬nh tráº¡ng gradient quÃ¡ nhá».<br>- Khi output Ã¢m â†’ \"táº¯t\" neuron Ä‘Ã³, giÃºp máº¡ng Ä‘Æ¡n giáº£n vÃ  á»•n Ä‘á»‹nh hÆ¡n. | - Vá»›i má»—i giÃ¡ trá»‹ Ä‘áº§u vÃ o: náº¿u > 0 thÃ¬ giá»¯ nguyÃªn, náº¿u <= 0 thÃ¬ thÃ nh 0.<br>- Äáº¡o hÃ m giÃºp xÃ¡c Ä‘á»‹nh neuron nÃ o sáº½ Ä‘Æ°á»£c cáº­p nháº­t (chá»‰ neuron dÆ°Æ¡ng).<br>- DÃ¹ng phá»• biáº¿n trong hidden layers cá»§a deep learning. |\n",
    "| **4. Softmax** | HÃ m biáº¿n vector thÃ nh phÃ¢n phá»‘i xÃ¡c suáº¥t: má»—i pháº§n tá»­ lÃ  má»™t xÃ¡c suáº¥t, tá»•ng báº±ng 1. | - DÃ¹ng trong bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p (vÃ­ dá»¥: nháº­n diá»‡n áº£nh â€“ chÃ³, mÃ¨o, vá»‹t...).<br>- GiÃºp mÃ´ hÃ¬nh nÃ³i â€œtÃ´i tin áº£nh nÃ y lÃ  con mÃ¨o vá»›i 90%â€ thay vÃ¬ chá»‰ ra sá»‘ Ä‘iá»ƒm. | - Vá»›i vector Ä‘áº§u vÃ o (logits), dÃ¹ng softmax Ä‘á»ƒ biáº¿n thÃ nh xÃ¡c suáº¥t cho tá»«ng lá»›p.<br>- Äáº¡o hÃ m cá»§a softmax dÃ¹ng trong káº¿t há»£p vá»›i cross-entropy Ä‘á»ƒ Ä‘iá»u chá»‰nh trá»ng sá»‘ sao cho mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n xÃ¡c suáº¥t Ä‘Ãºng cao hÆ¡n. |\n",
    "| **5. Tanh (Hyperbolic Tangent)** | HÃ m kÃ­ch hoáº¡t Ä‘áº§u ra náº±m trong khoáº£ng (-1, 1), giá»‘ng sigmoid nhÆ°ng cÃ³ tÃ­nh Ä‘á»‘i xá»©ng. | - PhÃ¹ há»£p hÆ¡n sigmoid trong nhiá»u trÆ°á»ng há»£p vÃ¬ Ä‘áº§u ra cÃ³ thá»ƒ Ã¢m (dá»¯ liá»‡u Ä‘Ã£ chuáº©n hÃ³a cÃ³ giÃ¡ trá»‹ Ã¢m).<br>- GiÃºp trung tÃ¢m hÃ³a dá»¯ liá»‡u quanh 0 â†’ há»c nhanh hÆ¡n. | - Má»—i Ä‘áº§u vÃ o Ä‘Æ°á»£c Ä‘Æ°a qua hÃ m tanh Ä‘á»ƒ táº¡o ra Ä‘áº§u ra náº±m trong (-1, 1).<br>- Äáº¡o hÃ m tanh dÃ¹ng Ä‘á»ƒ Ä‘iá»u chá»‰nh trá»ng sá»‘ nhÆ° cÃ¡c hÃ m kÃ­ch hoáº¡t khÃ¡c.<br>- DÃ¹ng trong hidden layer khi muá»‘n giá»¯ thÃ´ng tin Ã¢m vÃ  dÆ°Æ¡ng. |\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ Náº¿u Quá»‘c muá»‘n mÃ¬nh lÃ m báº£ng nÃ y thÃ nh **file Excel, Google Sheet, Markdown hay PDF** Ä‘á»ƒ tiá»‡n in hoáº·c chia sáº» vá»›i Ä‘á»“ng Ä‘á»™i, mÃ¬nh cÅ©ng lÃ m nhanh cho nhÃ©!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
