- Có cách nào để ko cần dùng nhiều time để train 

![[Pasted image 20250808202145.png]]


# 1. Motivation: 
- Lazy Learning. Gần thi mới học và bạn cứ quyển sách 1 và quyển sách 2. 
- New Data point => Biết được nó nên nằm ở cuốn 1 hay cuốn 2. 


![[Pasted image 20250808205712.png]]

DỰ ÁN THẬT + CHIA SẺ + ĐƯỢC COACHING + NETWORKIN\

k = 4, 2 đỏ, 2 xanh, 

Precision = Số dự đoán là ung thư đúng / tổng số dự đoán  
Recall = Số dự đoán là ung thư đúng / tổng số ung thư thực tế


```
như ví dụ cái auto ra không bị cancer ở data bị biased tính precision mà mẫu số = 0 thì output từ skikit learn là gì ạ  
  
**You** 9:33 PM  
@Hieu Ngo Chưa hiểu câu hỏi của bạn lắm ạ  
  
**Hieu Ngo** 9:34 PM  
như ví dụ auto ra “không bị cancer” ở data bị biased vừa nãy, lúc tính precision mà mẫu số = 0 thì output từ skikit learn là gì ạ  
  
**Đức Nguyễn** 9:37 PM  
= 0 hết thì nó rơi vào mấy cái trường hợp False Negative rồi vẫn cộng ở mẫu á bạn, nên mẫu kbh = 0 dc  
  
**You** 9:39 PM (Edited)  
Precision = tổng số ung thư đúng / tổng số dự đoán ung thư  
+, Với bài vừa nãy khi F(x): y = 0 (tức với mọi x, dự đoán luôn là 0)  
thì tổng số dự đoán ung thư = 0  
  
Ý bạn là vậy ạ
```


---



# 1. Sáng chủ nhật: 
## 1.1 Code chạy bài công ty done. 11h30
## 1.2. KNN

- KNN thường sử dụng loại khoảng cách nào mặc định. Tại sao??? 
=> 

---

# **KNN = K-Nearest Neighbors** 
>  - To know what group you are in, you look at the **k closest people** to you.  
>  - You see what group most of them are in.  
>  - Then you join that group.

> 
Weighted Voting: 
Suppose k = 3
- Neighbor 1 (distance = 1, class = Cat) → weight = 1/1 = 1.0
- Neighbor 2 (distance = 2, class = Dog) → weight = 1/2 = 0.5
- Neighbor 3 (distance = 3, class = Dog) → weight = 1/3 ≈ 0.33

Votes:

- Cat = 1.0
- Dog = 0.5 + 0.33 = 0.83
    

Winner = **Cat** 🐱

$w = \frac{1}{d}$  hoặc  $w = \frac{1}{d^2}$




---
# Bruce Force and KDTree

```bash
            [P2: x=5]
           /           \
   [P7: y=4]         [P6: y=2]
   /      \           /      \
P1(z=4)  P4(z=9)   P5(z=5)  P3(z=7)

```

+, Difference between. 
+, 
+, 

```
1 số câu hỏi:

1. Với K = 1 thì cách quét đi qua hết từ node root đến các node lá. Sau đó chọn best, right??

2. Với K = 2, 3 <= log_2_(6) +1 => ???

3. Với K > log_2_(6) + 1 ???
```

https://www.geeksforgeeks.org/cpp/kd-trees-in-cpp/
https://viblo.asia/p/gioi-thieu-thuat-toan-kd-trees-nearest-neighbour-search-RQqKLvjzl7z
[2004.04523](https://arxiv.org/pdf/2004.04523)


---

![[Pasted image 20250810154215.png]]



---
## **A) Bag-of-Words (BoW)**

- **What it does:** Counts how many times each word appears in a document.
- **Output:** A vector of raw counts.
> Vocabulary = [“cat”, “dog”, “eat”]  
> Document: “cat eat cat” → BoW vector = [2, 0, 1]

---

## **B) TF-IDF**

We need a small corpus to compute IDF. Suppose we have 3 documents:

1. `"cat eat cat"`
    
2. `"dog eat dog"`
    
3. `"cat dog eat"`
    

**Step 1 – Term Frequency (TF)** for document 1:

- cat: 2/3 ≈ 0.6667
    
- dog: 0/3 = 0
    
- eat: 1/3 ≈ 0.3333
    

**Step 2 – Inverse Document Frequency (IDF)**:

IDF(t)=log⁡(Total docsDocs containing t)\text{IDF}(t) = \log\left(\frac{\text{Total docs}}{\text{Docs containing t}}\right)

- cat: log(3/2) ≈ 0.1761
    
- dog: log(3/2) ≈ 0.1761
    
- eat: log(3/3) = 0
    

**Step 3 – TF-IDF** (TF × IDF):

- cat: 0.6667 × 0.1761 ≈ 0.1174
    
- dog: 0 × 0.1761 = 0
    
- eat: 0.3333 × 0 = 0
    

**TF-IDF vector** → 0.1174,0,00.1174, 0, 0

---
![[Pasted image 20250810162330.png]]

> Ban đầu mình tưởng rằng: Khi cập nhật centroids là mới cập nhật 1 vài nodes, còn các nodes khác nữa. 
> => Cơ mà: đúng ra là: sau khi chọn 1 centroids thì ta cần tính toán tất cả khoảng cách các điểm còn lại với các centroids rồi => Sau khi tính toán chỉ cần 1 lần centroids không đổi thì ta dừng lại ngay . 


