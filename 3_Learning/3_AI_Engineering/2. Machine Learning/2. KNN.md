- C√≥ c√°ch n√†o ƒë·ªÉ ko c·∫ßn d√πng nhi·ªÅu time ƒë·ªÉ train 

![[Pasted image 20250808202145.png]]


# 1. Motivation: 
- Lazy Learning. G·∫ßn thi m·ªõi h·ªçc v√† b·∫°n c·ª© quy·ªÉn s√°ch 1 v√† quy·ªÉn s√°ch 2. 
- New Data point => Bi·∫øt ƒë∆∞·ª£c n√≥ n√™n n·∫±m ·ªü cu·ªën 1 hay cu·ªën 2. 


![[Pasted image 20250808205712.png]]

D·ª∞ √ÅN TH·∫¨T + CHIA S·∫∫ + ƒê∆Ø·ª¢C COACHING + NETWORKIN\

k = 4, 2 ƒë·ªè, 2 xanh, 

Precision = S·ªë d·ª± ƒëo√°n l√† ung th∆∞ ƒë√∫ng / t·ªïng s·ªë d·ª± ƒëo√°n  
Recall = S·ªë d·ª± ƒëo√°n l√† ung th∆∞ ƒë√∫ng / t·ªïng s·ªë ung th∆∞ th·ª±c t·∫ø


```
nh∆∞ v√≠ d·ª• c√°i auto ra kh√¥ng b·ªã cancer ·ªü data b·ªã biased t√≠nh precision m√† m·∫´u s·ªë = 0 th√¨ output t·ª´ skikit learn l√† g√¨ ·∫°  
  
**You** 9:33 PM  
@Hieu Ngo Ch∆∞a hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n l·∫Øm ·∫°  
  
**Hieu Ngo** 9:34 PM  
nh∆∞ v√≠ d·ª• auto ra ‚Äúkh√¥ng b·ªã cancer‚Äù ·ªü data b·ªã biased v·ª´a n√£y, l√∫c t√≠nh precision m√† m·∫´u s·ªë = 0 th√¨ output t·ª´ skikit learn l√† g√¨ ·∫°  
  
**ƒê·ª©c Nguy·ªÖn** 9:37 PM  
= 0 h·∫øt th√¨ n√≥ r∆°i v√†o m·∫•y c√°i tr∆∞·ªùng h·ª£p False Negative r·ªìi v·∫´n c·ªông ·ªü m·∫´u √° b·∫°n, n√™n m·∫´u kbh = 0 dc  
  
**You** 9:39 PM (Edited)  
Precision = t·ªïng s·ªë ung th∆∞ ƒë√∫ng / t·ªïng s·ªë d·ª± ƒëo√°n ung th∆∞  
+, V·ªõi b√†i v·ª´a n√£y khi F(x): y = 0 (t·ª©c v·ªõi m·ªçi x, d·ª± ƒëo√°n lu√¥n l√† 0)  
th√¨ t·ªïng s·ªë d·ª± ƒëo√°n ung th∆∞ = 0  
  
√ù b·∫°n l√† v·∫≠y ·∫°
```


---



# 1. S√°ng ch·ªß nh·∫≠t: 
## 1.1 Code ch·∫°y b√†i c√¥ng ty done. 11h30
## 1.2. KNN

- KNN th∆∞·ªùng s·ª≠ d·ª•ng lo·∫°i kho·∫£ng c√°ch n√†o m·∫∑c ƒë·ªãnh. T·∫°i sao??? 
=> 

---

# **KNN = K-Nearest Neighbors** 
>  - To know what group you are in, you look at the **k closest people** to you.  
>  - You see what group most of them are in.  
>  - Then you join that group.

> 
Weighted Voting: 
Suppose k = 3
- Neighbor 1 (distance = 1, class = Cat) ‚Üí weight = 1/1 = 1.0
- Neighbor 2 (distance = 2, class = Dog) ‚Üí weight = 1/2 = 0.5
- Neighbor 3 (distance = 3, class = Dog) ‚Üí weight = 1/3 ‚âà 0.33

Votes:

- Cat = 1.0
- Dog = 0.5 + 0.33 = 0.83
    

Winner = **Cat** üê±

$w = \frac{1}{d}$  ho·∫∑c  $w = \frac{1}{d^2}$




---
# Bruce Force and KDTree

```bash
            [P2: x=5]
           /           \
   [P7: y=4]         [P6: y=2]
   /      \           /      \
P1(z=4)  P4(z=9)   P5(z=5)  P3(z=7)

```

+, Difference between. 
+, 
+, 

```
1 s·ªë c√¢u h·ªèi:

1. V·ªõi K = 1 th√¨ c√°ch qu√©t ƒëi qua h·∫øt t·ª´ node root ƒë·∫øn c√°c node l√°. Sau ƒë√≥ ch·ªçn best, right??

2. V·ªõi K = 2, 3 <= log_2_(6) +1 => ???

3. V·ªõi K > log_2_(6) + 1 ???
```

https://www.geeksforgeeks.org/cpp/kd-trees-in-cpp/
https://viblo.asia/p/gioi-thieu-thuat-toan-kd-trees-nearest-neighbour-search-RQqKLvjzl7z
[2004.04523](https://arxiv.org/pdf/2004.04523)


---

![[Pasted image 20250810154215.png]]



---
## **A) Bag-of-Words (BoW)**

- **What it does:** Counts how many times each word appears in a document.
- **Output:** A vector of raw counts.
> Vocabulary = [‚Äúcat‚Äù, ‚Äúdog‚Äù, ‚Äúeat‚Äù]  
> Document: ‚Äúcat eat cat‚Äù ‚Üí BoW vector = [2, 0, 1]

---

## **B) TF-IDF**

We need a small corpus to compute IDF. Suppose we have 3 documents:

1. `"cat eat cat"`
    
2. `"dog eat dog"`
    
3. `"cat dog eat"`
    

**Step 1 ‚Äì Term Frequency (TF)** for document 1:

- cat: 2/3 ‚âà 0.6667
    
- dog: 0/3 = 0
    
- eat: 1/3 ‚âà 0.3333
    

**Step 2 ‚Äì Inverse Document Frequency (IDF)**:

IDF(t)=log‚Å°(Total¬†docsDocs¬†containing¬†t)\text{IDF}(t) = \log\left(\frac{\text{Total docs}}{\text{Docs containing t}}\right)

- cat: log(3/2) ‚âà 0.1761
    
- dog: log(3/2) ‚âà 0.1761
    
- eat: log(3/3) = 0
    

**Step 3 ‚Äì TF-IDF** (TF √ó IDF):

- cat: 0.6667 √ó 0.1761 ‚âà 0.1174
    
- dog: 0 √ó 0.1761 = 0
    
- eat: 0.3333 √ó 0 = 0
    

**TF-IDF vector** ‚Üí 0.1174,0,00.1174, 0, 0

---
![[Pasted image 20250810162330.png]]

> Ban ƒë·∫ßu m√¨nh t∆∞·ªüng r·∫±ng: Khi c·∫≠p nh·∫≠t centroids l√† m·ªõi c·∫≠p nh·∫≠t 1 v√†i nodes, c√≤n c√°c nodes kh√°c n·ªØa. 
> => C∆° m√†: ƒë√∫ng ra l√†: sau khi ch·ªçn 1 centroids th√¨ ta c·∫ßn t√≠nh to√°n t·∫•t c·∫£ kho·∫£ng c√°ch c√°c ƒëi·ªÉm c√≤n l·∫°i v·ªõi c√°c centroids r·ªìi => Sau khi t√≠nh to√°n ch·ªâ c·∫ßn 1 l·∫ßn centroids kh√¥ng ƒë·ªïi th√¨ ta d·ª´ng l·∫°i ngay . 




---
[KNN - K Nearest Neighbor](https://alo2025alconquer.hashnode.dev/knn-k-nearest-neighbor)



---
C√¢u h·ªèi : n√™n d·ª±a v√†o y·∫øu t·ªë n√†o ƒë·ªÉ ch·ªçn 5-folds, 10-folds, 3-folds
+, B·∫£n ch·∫•t l√† train nhi·ªÅu l·∫ßn ƒë·ªÉ l·∫•y average sau ƒë√≥ l√†m ƒë·∫°i di·ªán cho K. 

--- 
![[Pasted image 20250810210841.png]]


M·ª•c ƒë√≠ch validation ƒë·ªÉ l·ª±a ch·ªçn K t·ªët nh·∫•t. => ƒë·ªÉ l·ª±a ch·ªçn K t·ªëi ∆∞u. 
K c·ªßa KNN ch·∫°y t·ª´ 1-> 15. 
+, Test K v·ªõi vi·ªác cho ch·∫°y tr√™n K-folds (Cross validation) ƒë·ªÉ n√≥ t√≠nh ra ƒë∆∞·ª£c score ƒë·∫°i di·ªán cho K


---
TF-IDF 
Bag of word
Onehot embedding
Embedding?? 

---
+, Khi k·∫øt qu·∫£ perform t·ªá. 
- V·∫•n ƒë·ªÅ do model hay do data. 
  +, ƒê·ªÉ bi·∫øt ƒë∆∞·ª£c data kh√¥ng t·ªët -> t√°ch th√†nh t·∫≠p 

![[Pasted image 20250810211905.png]]
+, Train vs train-dev: cao tr√™n t·∫≠p train, t·ªá tr√™n t·∫°p train-dev => overfit 
+, N·∫øu train t·ªët, train-dev t·ªët => oke => mang ƒëi ƒë√°nh gi√° m√† t·ªá => data maybe t·ªá. 


---

T·∫°i sao: b·ªô t·ª´ ch·ªâ 1000 t·ª´ 

![[Pasted image 20250810212309.png]]


T√¥i ƒëi h·ªçc: bi·ªÉu di·ªÖn 200.000 chi·ªÅu => Nhi·ªÅu c√°i b·ªã 0, r·∫•t √≠t c√°i c√≥ s·ªë => B·ªã t·ªën b·ªô nh·ªõ. 
+, 

![[Pasted image 20250810212539.png]]


Vector th∆∞a, to√†n c√°i b·ªã = 0 