- Có cách nào để ko cần dùng nhiều time để train 

![[Pasted image 20250808202145.png]]


# 1. Motivation: 
- Lazy Learning. Gần thi mới học và bạn cứ quyển sách 1 và quyển sách 2. 
- New Data point => Biết được nó nên nằm ở cuốn 1 hay cuốn 2. 


![[Pasted image 20250808205712.png]]

DỰ ÁN THẬT + CHIA SẺ + ĐƯỢC COACHING + NETWORKIN\

k = 4, 2 đỏ, 2 xanh, 

Precision = Số dự đoán là ung thư đúng / tổng số dự đoán  
Recall = Số dự đoán là ung thư đúng / tổng số ung thư thực tế


```
như ví dụ cái auto ra không bị cancer ở data bị biased tính precision mà mẫu số = 0 thì output từ skikit learn là gì ạ  
  
**You** 9:33 PM  
@Hieu Ngo Chưa hiểu câu hỏi của bạn lắm ạ  
  
**Hieu Ngo** 9:34 PM  
như ví dụ auto ra “không bị cancer” ở data bị biased vừa nãy, lúc tính precision mà mẫu số = 0 thì output từ skikit learn là gì ạ  
  
**Đức Nguyễn** 9:37 PM  
= 0 hết thì nó rơi vào mấy cái trường hợp False Negative rồi vẫn cộng ở mẫu á bạn, nên mẫu kbh = 0 dc  
  
**You** 9:39 PM (Edited)  
Precision = tổng số ung thư đúng / tổng số dự đoán ung thư  
+, Với bài vừa nãy khi F(x): y = 0 (tức với mọi x, dự đoán luôn là 0)  
thì tổng số dự đoán ung thư = 0  
  
Ý bạn là vậy ạ
```


---



# 1. Sáng chủ nhật: 
## 1.1 Code chạy bài công ty done. 11h30
## 1.2. KNN

- KNN thường sử dụng loại khoảng cách nào mặc định. Tại sao??? 
=> 

---

# **KNN = K-Nearest Neighbors** 
>  - To know what group you are in, you look at the **k closest people** to you.  
>  - You see what group most of them are in.  
>  - Then you join that group.

> 
Weighted Voting: 
Suppose k = 3
- Neighbor 1 (distance = 1, class = Cat) → weight = 1/1 = 1.0
- Neighbor 2 (distance = 2, class = Dog) → weight = 1/2 = 0.5
- Neighbor 3 (distance = 3, class = Dog) → weight = 1/3 ≈ 0.33

Votes:

- Cat = 1.0
- Dog = 0.5 + 0.33 = 0.83
    

Winner = **Cat** 🐱

$w = \frac{1}{d}$  hoặc  $w = \frac{1}{d^2}$




---
# Bruce Force and KDTree

```bash
            [P2: x=5]
           /           \
   [P7: y=4]         [P6: y=2]
   /      \           /      \
P1(z=4)  P4(z=9)   P5(z=5)  P3(z=7)

```

+, Difference between. 
+, 
+, 

```
1 số câu hỏi:

1. Với K = 1 thì cách quét đi qua hết từ node root đến các node lá. Sau đó chọn best, right??

2. Với K = 2, 3 <= log_2_(6) +1 => ???

3. Với K > log_2_(6) + 1 ???
```

https://www.geeksforgeeks.org/cpp/kd-trees-in-cpp/
https://viblo.asia/p/gioi-thieu-thuat-toan-kd-trees-nearest-neighbour-search-RQqKLvjzl7z
[2004.04523](https://arxiv.org/pdf/2004.04523)


---

![[Pasted image 20250810154215.png]]



---
## **A) Bag-of-Words (BoW)**

- **What it does:** Counts how many times each word appears in a document.
- **Output:** A vector of raw counts.
> Vocabulary = [“cat”, “dog”, “eat”]  
> Document: “cat eat cat” → BoW vector = [2, 0, 1]

---

## **B) TF-IDF**

We need a small corpus to compute IDF. Suppose we have 3 documents:

1. `"cat eat cat"`
    
2. `"dog eat dog"`
    
3. `"cat dog eat"`
    

**Step 1 – Term Frequency (TF)** for document 1:

- cat: 2/3 ≈ 0.6667
    
- dog: 0/3 = 0
    
- eat: 1/3 ≈ 0.3333
    

**Step 2 – Inverse Document Frequency (IDF)**:

IDF(t)=log⁡(Total docsDocs containing t)\text{IDF}(t) = \log\left(\frac{\text{Total docs}}{\text{Docs containing t}}\right)

- cat: log(3/2) ≈ 0.1761
    
- dog: log(3/2) ≈ 0.1761
    
- eat: log(3/3) = 0
    

**Step 3 – TF-IDF** (TF × IDF):

- cat: 0.6667 × 0.1761 ≈ 0.1174
    
- dog: 0 × 0.1761 = 0
    
- eat: 0.3333 × 0 = 0
    

**TF-IDF vector** → 0.1174,0,00.1174, 0, 0

---
![[Pasted image 20250810162330.png]]

> Ban đầu mình tưởng rằng: Khi cập nhật centroids là mới cập nhật 1 vài nodes, còn các nodes khác nữa. 
> => Cơ mà: đúng ra là: sau khi chọn 1 centroids thì ta cần tính toán tất cả khoảng cách các điểm còn lại với các centroids rồi => Sau khi tính toán chỉ cần 1 lần centroids không đổi thì ta dừng lại ngay . 




---
[KNN - K Nearest Neighbor](https://alo2025alconquer.hashnode.dev/knn-k-nearest-neighbor)



---
Câu hỏi : nên dựa vào yếu tố nào để chọn 5-folds, 10-folds, 3-folds
+, Bản chất là train nhiều lần để lấy average sau đó làm đại diện cho K. 

--- 
![[Pasted image 20250810210841.png]]


Mục đích validation để lựa chọn K tốt nhất. => để lựa chọn K tối ưu. 
K của KNN chạy từ 1-> 15. 
+, Test K với việc cho chạy trên K-folds (Cross validation) để nó tính ra được score đại diện cho K


---
TF-IDF 
Bag of word
Onehot embedding
Embedding?? 

---
+, Khi kết quả perform tệ. 
- Vấn đề do model hay do data. 
  +, Để biết được data không tốt -> tách thành tập 

![[Pasted image 20250810211905.png]]
+, Train vs train-dev: cao trên tập train, tệ trên tạp train-dev => overfit 
+, Nếu train tốt, train-dev tốt => oke => mang đi đánh giá mà tệ => data maybe tệ. 


---

Tại sao: bộ từ chỉ 1000 từ 

![[Pasted image 20250810212309.png]]


Tôi đi học: biểu diễn 200.000 chiều => Nhiều cái bị 0, rất ít cái có số => Bị tốn bộ nhớ. 
+, 

![[Pasted image 20250810212539.png]]


Vector thưa, toàn cái bị = 0 


``` 
Xử lý vấn đề khi : hello thì model trả lời ra phần text wiki feed cho nó ?? 

```

![[Pasted image 20250810213206.png]]


Ok, mình sẽ nói kỹ về **phương pháp Convert Text to Feature – Bag of Words (BoW)** để bạn vừa hiểu bản chất vừa hình dung được tại sao code của bạn lại viết như vậy.

---

## 1. Ý tưởng của BoW

**Bag of Words** là cách biểu diễn văn bản thành số để máy học xử lý.  
Nguyên tắc:

- Mỗi **từ** (hoặc token) trong tập dữ liệu được coi như một **feature** (đặc trưng).
    
- Ta đếm số lần từ đó xuất hiện trong mỗi văn bản.
    
- Bỏ qua thứ tự từ, ngữ pháp → chỉ quan tâm đến **tần suất**.
    

Ví dụ:

```
Văn bản 1: "I love AI"
Văn bản 2: "I love Python"

Từ vựng (vocab) = ["I", "love", "AI", "Python"]

Biểu diễn BoW:
V1: [1, 1, 1, 0]
V2: [1, 1, 0, 1]
```

Ở đây, `[1, 1, 1, 0]` nghĩa là:

- "I": xuất hiện 1 lần
    
- "love": 1 lần
    
- "AI": 1 lần
    
- "Python": 0 lần
    

---

## 2. Các bước trong code của bạn

```python
vectorizer = CountVectorizer(max_features=1000) 
X_train = vectorizer.fit_transform(imdb_train['text']).toarray()
X_test = vectorizer.transform(imdb_test['text']).toarray()
```

- **`CountVectorizer`**: công cụ của scikit-learn để tự động:
    
    1. Tách từ (tokenize)
        
    2. Xây dựng từ điển (vocab)
        
    3. Đếm tần suất từ trong mỗi văn bản
        
- **`max_features=1000`**: giới hạn số từ đặc trưng nhiều nhất là 1000 từ phổ biến → giảm chiều dữ liệu và tránh overfitting.
    
- **`fit_transform`**:
    
    - `fit`: học vocab từ tập train.
        
    - `transform`: đếm tần suất từ cho từng văn bản.
        
    - Kết quả trả về dạng **sparse matrix** → `.toarray()` để đổi sang NumPy array.
        
- **`transform` cho tập test**:
    
    - Không `fit` lại vì vocab phải giữ nguyên như lúc train → tránh "rò rỉ dữ liệu".
        

---

## 3. Điểm mạnh của BoW

- Đơn giản, dễ triển khai.
    
- Hiệu quả với các mô hình truyền thống (Naive Bayes, Logistic Regression, SVM…).
    
- Dễ kiểm soát kích thước đặc trưng (bằng `max_features` hoặc `min_df`).
    

---

## 4. Hạn chế

- **Mất thông tin ngữ cảnh**: Không biết thứ tự từ, mối quan hệ giữa từ.
    
- **Vector thưa (sparse)**: Nhiều số 0 → tốn bộ nhớ nếu từ vựng lớn.
    
- Không xử lý tốt các từ đồng nghĩa hoặc đa nghĩa (semantic meaning).
    
- Không tận dụng thông tin từ embedding như Word2Vec, BERT.
    

---

## 5. Một số cải tiến trên BoW

- **TF-IDF**: thay vì chỉ đếm, còn tính trọng số dựa trên mức độ đặc trưng của từ trong toàn bộ corpus.
    
- **N-gram**: thay vì chỉ lấy 1 từ (unigram), có thể lấy cụm từ 2 từ (bigram), 3 từ (trigram)… để giữ thêm chút ngữ cảnh.
    
- **Embedding**: dùng Word2Vec, GloVe, FastText, BERT… để biểu diễn từ/văn bản thành vector dense, mang thông tin ngữ nghĩa.
    

---

Nếu bạn muốn, mình có thể vẽ cho bạn **sơ đồ flow từ văn bản → BoW → model ML** để nhìn trực quan hơn.  
Bạn có muốn mình vẽ luôn không?


---
Ok, mình sẽ so sánh **BoW** và **TF-IDF** ngắn gọn, rồi giải thích vì sao dù không hiểu ngữ nghĩa chúng vẫn dùng tốt.

---

## 1. BoW vs. TF-IDF

|Tiêu chí|**BoW**|**TF-IDF**|
|---|---|---|
|**Cách tính**|Đếm số lần mỗi từ xuất hiện trong văn bản.|Đếm số lần từ xuất hiện, nhưng **giảm trọng số** nếu từ xuất hiện nhiều trong toàn bộ corpus (dùng IDF).|
|**Trọng số từ**|Chỉ dựa vào tần suất xuất hiện (TF).|TF × IDF → tăng trọng số cho từ đặc trưng, giảm cho từ phổ biến như “the”, “is”.|
|**Thông tin ngữ nghĩa**|Không giữ ngữ cảnh, không biết nghĩa từ.|Cũng không giữ ngữ cảnh, nhưng lọc bớt từ không quan trọng.|
|**Kích thước vector**|Phụ thuộc số lượng từ vựng.|Giống BoW, nhưng giá trị trong vector là trọng số TF-IDF.|
|**Khi nên dùng**|Khi dữ liệu nhỏ, từ quan trọng là các từ khóa lặp lại nhiều trong một văn bản.|Khi cần phân biệt từ đặc trưng so với từ phổ biến trong tập dữ liệu.|

---

## 2. Vì sao không hiểu ngữ nghĩa vẫn dùng tốt?

1. **Nhiều tác vụ phân loại chỉ cần tần suất từ**  
    Ví dụ: với phân loại sentiment (tích cực/tiêu cực), sự xuất hiện của các từ “good”, “excellent” hay “bad”, “terrible” đã đủ để phân biệt.
    
2. **Ngữ nghĩa nhiều khi được “ẩn” trong thống kê từ**  
    Các từ xuất hiện nhiều trong một nhãn và ít ở nhãn khác → tự động trở thành dấu hiệu phân loại.
    
3. **Thuật toán học máy sẽ tự học quy luật từ vector này**  
    Dù vector chỉ là số đếm, model như Logistic Regression, Naive Bayes… sẽ tìm ra trọng số phù hợp để phân biệt.
    
4. **Đơn giản và hiệu quả cho baseline**  
    BoW và TF-IDF dễ tính, nhanh, ít tài nguyên → phù hợp để thử nghiệm nhanh hoặc khi dữ liệu không đủ để train embedding phức tạp.
    

---

Nếu bạn muốn, mình có thể làm thêm **một hình minh họa flow: Văn bản → BoW & TF-IDF → model ML**, nhìn phát hiểu ngay luôn.  
Bạn có muốn mình vẽ không?


![[Pasted image 20250810214752.png]]

![[Pasted image 20250810214959.png]]

![[Pasted image 20250810220248.png]]


Khi nào dùng Min - Max / Standard 
+, Min max nhạy cảm với nhiễu 
+, Standard
+< 


| gender | male | female |
| ------ | ---- | ------ |
| male   | 1    | 0      |
| female | 0    | 1      |
| male   | 1    | 0      |

điểm:  giỏi khá trung bình -> encod: 0, 1, 2, 3, 4, 