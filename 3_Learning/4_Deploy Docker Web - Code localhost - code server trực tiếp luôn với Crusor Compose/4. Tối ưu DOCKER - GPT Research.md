
```
# Sử dụng image nhẹ hơn với python3.10-slim
FROM python:3.10-slim

# Thiết lập các biến môi trường để tối ưu Python và pip
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONASYNCIODEBUG=0 \
    PYTHONOPTIMIZE=2 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Đặt thư mục làm việc
WORKDIR /app

# Cài đặt các package system cần thiết
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Sao chép và cài đặt requirements trước
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Cài đặt thêm gunicorn và uvicorn
RUN pip install --no-cache-dir gunicorn uvicorn[standard]

# Sao chép mã nguồn
COPY ./api_export_v2_AgentOrWorkflow.py ./api_export.py

# Sao chép mô hình
COPY ./trained_models/v7_trainsets_ckp-300_XLMRoBERTa_20eps ./trained_models/v7_trainsets_ckp-300_XLMRoBERTa_20eps

# Tạo non-root user
RUN adduser --disabled-password --gecos "" appuser
USER appuser

# Expose port
EXPOSE 25041

# Chạy ứng dụng với gunicorn và giới hạn memory
CMD ["gunicorn", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", \
     "--timeout", "30", "--keep-alive", "5", "--max-requests", "1000", \
     "-b", "0.0.0.0:25041", "api_export:app"]


========

version: '3.8'

services:
  fastapi:
    build: 
      context: .
      dockerfile: Dockerfile
    container_name: deploy_FastResponseIntention
    ports:
      - "25041:25041"
    volumes:
      - ./trained_models:/app/trained_models:ro  # Mount read-only
    environment:
      - MODEL_PATH=./trained_models/v7_trainsets_ckp-300_XLMRoBERTa_20eps
      - WORKERS=4  # Số lượng worker processes
      - MAX_WORKERS=8  # Số worker tối đa khi scale
      - TIMEOUT=30  # Timeout cho mỗi request (giây)
      - KEEP_ALIVE=5  # Giữ connection (giây)
    deploy:
      resources:
        limits:
          cpus: '4'  # Giới hạn CPU
          memory: 8G  # Giới hạn RAM
        reservations:
          cpus: '2'  # Đảm bảo luôn có 2 CPU
          memory: 4G  # Đảm bảo luôn có 4GB RAM
    healthcheck:  # Kiểm tra health của container
      test: ["CMD", "curl", "-f", "http://localhost:25041/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"  # Giới hạn kích thước file log
        max-file: "3"    # Số file log tối đa
    mem_swappiness: 0    # Giảm thiểu sử dụng swap
    mem_reservation: 4G   # Đảm bảo memory cho container
    runtime: nvidia      # Sử dụng NVIDIA GPU
    shm_size: '2gb'      # Shared memory size
    ulimits:            # System limits
      nofile:
        soft: 65536
        hard: 65536
    sysctls:            # Kernel parameters
      net.core.somaxconn: 1024
      net.ipv4.tcp_max_syn_backlog: 1024
    networks:
      - app_network     # Custom network

networks:
  app_network:
    driver: bridge



========
VIẾT TẤT TẦN TẬT VỀ TỐI ƯU DOCKER ??
```

Tuyệt vời! Mình sẽ tìm hiểu tất tần tật về cách tối ưu Docker, bao gồm:

- **Tối ưu Docker Image**: Giảm dung lượng image, sử dụng base image phù hợp, tận dụng layer caching.
- **Tối ưu hiệu suất container**: Giảm thời gian khởi động, tối ưu bộ nhớ và CPU, sử dụng multistage build.
- **Bảo mật Docker**: Giới hạn quyền truy cập, bảo vệ secrets, giảm attack surface.
- **Tối ưu tài nguyên hệ thống**: Sử dụng cgroups, giới hạn CPU/RAM, tối ưu network và I/O.
- **Thực hành tốt nhất cho Docker Compose & Kubernetes**: Quản lý service hiệu quả, load balancing, monitoring.

Mình sẽ tổng hợp và gửi bạn hướng dẫn chi tiết sớm nhất có thể!

# Tối ưu Docker Image

## Giảm dung lượng Docker Image

Docker image nhẹ giúp quá trình build và deploy nhanh hơn ([Tối ưu hóa Ảo hóa Docker: Giảm kích thước để triển khai nhanh hơn](https://viblo.asia/p/toi-uu-hoa-ao-hoa-docker-giam-kich-thuoc-de-trien-khai-nhanh-hon-aNj4vPN8L6r#:~:text=,v%E1%BB%9Bi%20%C3%ADt%20l%E1%BB%97%20h%E1%BB%95ng%20h%C6%A1n)). Để giảm kích thước image, hãy sử dụng **base image tối giản**. Ví dụ, thay vì dùng `ubuntu:latest` (~800MB), chuyển sang Alpine Linux chỉ ~5MB; điều này đã giảm image từ 800MB xuống dưới 30MB ([Tối ưu hóa Ảo hóa Docker: Giảm kích thước để triển khai nhanh hơn](https://viblo.asia/p/toi-uu-hoa-ao-hoa-docker-giam-kich-thuoc-de-trien-khai-nhanh-hon-aNj4vPN8L6r#:~:text=1,thi%E1%BB%83u)). Bên cạnh đó, hãy **loại bỏ các file không cần thiết** khỏi image. Tạo file `.dockerignore` để loại trừ thư mục, file tạm (như `node_modules`, file log, thư mục .git, v.v.) khỏi _build context_, tránh chép chúng vào image ([Tối ưu hóa Ảo hóa Docker: Giảm kích thước để triển khai nhanh hơn](https://viblo.asia/p/toi-uu-hoa-ao-hoa-docker-giam-kich-thuoc-de-trien-khai-nhanh-hon-aNj4vPN8L6r#:~:text=Trong%20khi%20debug%2C%20ch%C3%BAng%20t%C3%B4i,gi%E1%BB%9D%20xu%E1%BA%A5t%20hi%E1%BB%87n%20trong%20image)).

Bạn cũng nên **xóa dữ liệu tạm và cache** trong quá trình build. Chẳng hạn, sau khi cài đặt bằng `apt-get`, hãy xóa cache (`apt-get clean` và xóa `/var/lib/apt/lists/*`) để image gọn nhẹ hơn.

## Tận dụng bộ nhớ đệm (layer caching)

Docker build tạo image theo từng lệnh Dockerfile, và có thể _cache_ các layer để tăng tốc độ build. Mỗi lệnh và nội dung file liên quan nếu không thay đổi sẽ dùng lại từ cache ở lần build sau ([Optimize cache usage in builds | Docker Docs](https://docs.docker.com/build/cache/optimize/#:~:text=When%20building%20with%20Docker%2C%20a,to%20rebuild%20the%20layer%20again) ). Để tận dụng điều này, hãy **sắp xếp các lệnh Dockerfile hợp lý**. Đưa những lệnh ít thay đổi lên trước, và những lệnh thường xuyên thay đổi xuống cuối ([Optimize cache usage in builds | Docker Docs](https://docs.docker.com/build/cache/optimize/#:~:text=Order%20your%20layers) ) ([Optimize cache usage in builds | Docker Docs](https://docs.docker.com/build/cache/optimize/#:~:text=Instead%2C%20the%20,is%20subject%20to%20frequent%20change) ). Ví dụ, thay vì chép toàn bộ mã nguồn rồi mới chạy `npm install`, hãy tách riêng bước sao chép file khai báo dependency (như `package.json`) trước, chạy `npm install` ở layer sớm. Sau đó mới `COPY` phần code ứng dụng vào cuối ([Optimize cache usage in builds | Docker Docs](https://docs.docker.com/build/cache/optimize/#:~:text=Instead%2C%20the%20,is%20subject%20to%20frequent%20change) ). Cách làm này đảm bảo khi code thay đổi, Docker chỉ build lại các layer cuối, còn layer cài dependencies vẫn dùng cache, giúp build nhanh hơn.

Ngoài ra, **giữ cho _context_ build nhỏ** cũng rất quan trọng. Các file không cần thiết đã được liệt kê trong `.dockerignore` sẽ không được gửi vào context, tránh invalid cache không cần thiết ([Optimize cache usage in builds | Docker Docs](https://docs.docker.com/build/cache/optimize/#:~:text=Keep%20the%20context%20small) ) ([Optimize cache usage in builds | Docker Docs](https://docs.docker.com/build/cache/optimize/#:~:text=) ).

## Sử dụng multi-stage build

**Multi-stage build** cho phép chia Dockerfile thành nhiều giai đoạn build để loại bỏ các thành phần thừa trong image cuối. Ở các stage đầu, bạn có thể sử dụng image đầy đủ để build/compile ứng dụng, sau đó chỉ copy những artifact cần thiết sang stage cuối với base image tối giản ([How to Reduce Docker Image Size](https://www.nebula-graph.io/posts/how-to-reduce-docker-image-size#:~:text=In%20short%2C%20multi,Let%27s%20consider%20an%20example)) ([How to Reduce Docker Image Size](https://www.nebula-graph.io/posts/how-to-reduce-docker-image-size#:~:text=,in%20the%20previous%20building%20step)). Ví dụ, với ứng dụng front-end React, có thể dùng Node.js ở stage build để chạy `npm install` và build ra file tĩnh, rồi dùng `nginx:alpine` ở stage runtime và `COPY --from=builder` các file tĩnh sang, tạo image chỉ chứa web server và file tĩnh ([Tối ưu hóa Ảo hóa Docker: Giảm kích thước để triển khai nhanh hơn](https://viblo.asia/p/toi-uu-hoa-ao-hoa-docker-giam-kich-thuoc-de-trien-khai-nhanh-hon-aNj4vPN8L6r#:~:text=,RUN%20npm%20run%20build)). Nhờ multi-stage build, image cuối cùng không chứa tool hay thư viện dư thừa, giảm đáng kể dung lượng và tăng bảo mật (ít thành phần hơn đồng nghĩa ít lỗ hổng hơn) ([Tối ưu hóa Ảo hóa Docker: Giảm kích thước để triển khai nhanh hơn](https://viblo.asia/p/toi-uu-hoa-ao-hoa-docker-giam-kich-thuoc-de-trien-khai-nhanh-hon-aNj4vPN8L6r#:~:text=,v%E1%BB%9Bi%20%C3%ADt%20l%E1%BB%97%20h%E1%BB%95ng%20h%C6%A1n)).

## Tối ưu lệnh RUN và COPY trong Dockerfile

Mỗi lệnh RUN, COPY, ADD trong Dockerfile tạo ra một layer. Quá nhiều layer sẽ làm image phình to ([Tối ưu hóa Ảo hóa Docker: Giảm kích thước để triển khai nhanh hơn](https://viblo.asia/p/toi-uu-hoa-ao-hoa-docker-giam-kich-thuoc-de-trien-khai-nhanh-hon-aNj4vPN8L6r#:~:text=4,thi%E1%BB%83u%20c%C3%A1c%20Layer)). Vì vậy, **kết hợp nhiều thao tác trong một lệnh RUN** để giảm số layer. Ví dụ, thay vì:

```Dockerfile
RUN apt-get update  
RUN apt-get install -y curl vim  
RUN apt-get clean  
RUN rm -rf /var/lib/apt/lists/*
```

Hãy gộp lại thành một lệnh RUN duy nhất dùng toán tử `&&` nối các lệnh:

```Dockerfile
RUN apt-get update && apt-get install -y curl vim \
    && apt-get clean && rm -rf /var/lib/apt/lists/*
```

Như vậy chỉ tạo ra một layer chứa đầy đủ kết quả của các bước cài đặt ([Tối ưu hóa Ảo hóa Docker: Giảm kích thước để triển khai nhanh hơn](https://viblo.asia/p/toi-uu-hoa-ao-hoa-docker-giam-kich-thuoc-de-trien-khai-nhanh-hon-aNj4vPN8L6r#:~:text=Thay%20v%C3%AC%20vi%E1%BA%BFt%3A)). Tương tự, gộp nhiều lệnh copy nhỏ thành một lệnh `COPY` duy nhất khi có thể. Ngoài ra, cân nhắc **dọn dẹp file tạm trong cùng lệnh RUN** (như ví dụ trên) để tránh tạo layer riêng chỉ để xóa file. Việc tối ưu các lệnh như vậy giúp image sạch và nhỏ hơn, đồng thời tận dụng cache tốt hơn.

# Tối ưu hiệu suất container

## Giảm thời gian khởi động container

Thời gian khởi động container (startup time) ảnh hưởng trực tiếp đến độ phản hồi của dịch vụ, đặc biệt khi mở rộng số lượng container. Để container khởi động nhanh, trước hết image cần nhỏ để kéo về nhanh và load nhanh ([Reduce container startup time on Amazon EKS with Bottlerocket data volume | Containers](https://aws.amazon.com/blogs/containers/reduce-container-startup-time-on-amazon-eks-with-bottlerocket-data-volume/#:~:text=1,the%20need%20of%20image%20downloading)). Sử dụng các kỹ thuật ở trên (base image nhỏ, multi-stage build) sẽ giúp giảm thời gian container start (do giảm thời gian pull và extract image). Ngoài ra, **tối ưu ứng dụng và script khởi động**: đảm bảo ứng dụng không thực hiện những tác vụ nặng nề không cần thiết khi khởi động. Ví dụ, nếu container chạy một script entrypoint, hãy loại bỏ các `sleep` thừa hoặc thao tác khởi tạo có thể chuyển sang lúc build. Chuẩn bị sẵn dữ liệu hoặc cache cần thiết ngay trong image để container chạy là có thể phục vụ ngay. Bạn cũng có thể cấu hình để container **prefetch** dữ liệu hoặc kết nối trước khi nhận traffic (kết hợp với probe, xem phần Health Check bên dưới). Mục tiêu là giảm thiểu độ trễ từ lúc `docker run` đến khi ứng dụng sẵn sàng.

Trong môi trường Kubernetes, việc **giữ sẵn image trên node** (image caching) cũng giúp pod khởi động nhanh hơn, tránh thời gian kéo image mỗi lần ([Reduce container startup time on Amazon EKS with Bottlerocket data volume | Containers](https://aws.amazon.com/blogs/containers/reduce-container-startup-time-on-amazon-eks-with-bottlerocket-data-volume/#:~:text=When%20the%20container%20runtime%20initiates,image%20from%20the%20image%20repository)). Với các ứng dụng đòi hỏi image lớn, có thể cân nhắc giải pháp pre-pull image hoặc dùng Node OS tối ưu cho container khởi động (như Bottlerocket của AWS) để cải thiện tốc độ ([Reduce container startup time on Amazon EKS with Bottlerocket data volume | Containers](https://aws.amazon.com/blogs/containers/reduce-container-startup-time-on-amazon-eks-with-bottlerocket-data-volume/#:~:text=which%20include%20to%3A)).

## Cấu hình bộ nhớ và CPU hợp lý

Mỗi container nên được cấp phát tài nguyên (RAM, CPU) phù hợp với nhu cầu để vừa đảm bảo hiệu suất, vừa tránh lãng phí hoặc ảnh hưởng container khác. Docker cho phép đặt giới hạn tài nguyên khi chạy container, ví dụ `--memory="256m"` để giới hạn RAM hoặc `--cpus="0.5"` để giới hạn CPU tối đa 0.5 core ( [Optimizing Docker Container Performance: Best Practices for Resource Allocation - LoadForge Guides - LoadForge](https://loadforge.com/guides/best-practices-for-docker-container-resource-allocation#:~:text=,and%20Usage%20Analysis) ). **Giới hạn cứng** ngăn container dùng quá mức tài nguyên, tránh trường hợp một container chiếm hết CPU/RAM và làm chậm cả hệ thống ( [Optimizing Docker Container Performance: Best Practices for Resource Allocation - LoadForge Guides - LoadForge](https://loadforge.com/guides/best-practices-for-docker-container-resource-allocation#:~:text=,and%20Usage%20Analysis) ). Ngược lại, nếu container quá hạn chế tài nguyên có thể dẫn đến nghẽn hiệu suất (ứng dụng bị OOM kill nếu dùng quá RAM, hoặc bị throttle CPU). Vì vậy, cần **điều chỉnh giới hạn phù hợp**: đủ cao để ứng dụng chạy mượt, nhưng đủ thấp để ngăn ảnh hưởng liên container.

Trong môi trường Docker Compose (phiên bản 3 trở lên) hoặc Swarm, có thể đặt giới hạn trong phần `deploy.resources.limits` của file Compose ( [Optimizing Docker Container Performance: Best Practices for Resource Allocation - LoadForge Guides - LoadForge](https://loadforge.com/guides/best-practices-for-docker-container-resource-allocation#:~:text=match%20at%20L353%20your%20entire,starve%20others%20of%20required%20resources) ) ( [Optimizing Docker Container Performance: Best Practices for Resource Allocation - LoadForge Guides - LoadForge](https://loadforge.com/guides/best-practices-for-docker-container-resource-allocation#:~:text=) ). Kubernetes cũng có khái niệm request/limit cho CPU, memory để scheduler phân bổ và giới hạn tài nguyên container. Best practice là luôn xác định rõ lượng CPU, RAM mỗi service cần qua thử nghiệm, sau đó đặt request/limit tương ứng. Điều này giúp đảm bảo tính ổn định và hiệu năng dự đoán được cho container của bạn.

## Tận dụng tmpfs cho dữ liệu tạm thời

Nếu ứng dụng trong container tạo nhiều file tạm, cache hoặc ghi nhật ký tạm thời, hãy cân nhắc sử dụng **tmpfs** (filesystem trong RAM) thay vì ghi xuống đĩa. Docker hỗ trợ gắn `tmpfs` vào container (chỉ trên Linux) bằng flag `--tmpfs` khi chạy container ([tmpfs mounts | Docker Docs](https://docs.docker.com/engine/storage/tmpfs/#:~:text=If%20you%27re%20running%20Docker%20on,outside%20the%20container%27s%20writable%20layer) ) ([tmpfs mounts | Docker Docs](https://docs.docker.com/engine/storage/tmpfs/#:~:text=you%20create%20a%20container%20with,outside%20the%20container%27s%20writable%20layer) ). Dữ liệu trên tmpfs **chỉ lưu trong bộ nhớ** và không bao giờ ghi xuống ổ đĩa vật lý, nên tốc độ I/O rất nhanh. Ví dụ: `docker run -d --tmpfs /tmp:size=100m myimage` sẽ tạo mount `/tmp` trong container trên RAM, giúp mọi ghi/đọc trong `/tmp` nhanh hơn và giảm hao mòn ổ đĩa. Cách này hữu ích cho dữ liệu không cần persist (khi container tắt là mất) và cải thiện hiệu suất khi ứng dụng ghi nhiều dữ liệu tạm ([tmpfs mounts | Docker Docs](https://docs.docker.com/engine/storage/tmpfs/#:~:text=As%20opposed%20to%20volumes%20and,written%20there%20won%27t%20be%20persisted) ) ([tmpfs mounts | Docker Docs](https://docs.docker.com/engine/storage/tmpfs/#:~:text=tmpfs%20mounts%20are%20best%20used,persistent%20state%20data) ). Lưu ý không lạm dụng tmpfs cho dữ liệu quá lớn vì sẽ tiêu tốn RAM của host.

Bên cạnh đó, nếu container cần swap hoặc caching, hãy xem xét phân bổ đủ memory swap (hoặc tắt swap nếu không muốn ảnh hưởng hiệu năng). Môi trường host (như Docker Desktop) cũng có thể cần tinh chỉnh để cho phép giới hạn swap/memory (ví dụ bật cgroups v2, cấu hình nhân Linux) ([Resource constraints | Docker Docs](https://docs.docker.com/engine/containers/resource_constraints/#:~:text=It%27s%20important%20not%20to%20allow,the%20wrong%20process%20is%20killed) ) ([Resource constraints | Docker Docs](https://docs.docker.com/engine/containers/resource_constraints/#:~:text=You%20can%20mitigate%20the%20risk,instability%20due%20to%20OOME%20by) ).

# Bảo mật Docker

## Chạy container với user không phải root

Theo mặc định, container thường chạy dưới quyền root (UID 0) bên trong, điều này tiềm ẩn rủi ro vì nếu ứng dụng bị tấn công, kẻ tấn công có quyền cao trong container và có thể tìm cách leo thang ra host. Best practice là **tạo user thường trong Dockerfile và sử dụng lệnh `USER` để chuyển sang user đó** trước khi chạy ứng dụng ([Top 20 Dockerfile best practices | Sysdig](https://sysdig.com/learn-cloud-native/dockerfile-best-practices/#:~:text=root%2C%20so%20don%E2%80%99t%20forget%20to,root%20user)) ([Top 20 Dockerfile best practices | Sysdig](https://sysdig.com/learn-cloud-native/dockerfile-best-practices/#:~:text=,copy%20application%20files%20USER%20myuser)). Ví dụ, trong Dockerfile:

```Dockerfile
RUN adduser -D appuser && chown -R appuser /app 
USER appuser
```

Lệnh trên tạo user `appuser` và chuyển context sang user này. Như vậy, quy trình trong container không chạy với quyền root, giảm thiểu mức độ tác động nếu bị xâm nhập ([Top 20 Dockerfile best practices | Sysdig](https://sysdig.com/learn-cloud-native/dockerfile-best-practices/#:~:text=root%2C%20so%20don%E2%80%99t%20forget%20to,root%20user)). Khi chạy container, cũng tránh dùng `--privileged` trừ khi thật sự cần, vì nó cấp rất nhiều quyền cho container. Một số môi trường (như OpenShift) thậm chí không cho chạy container dưới user root, nên việc tuân thủ nguyên tắc **least privilege** ngay từ đầu là cần thiết.

## Hạn chế quyền truy cập, sử dụng volume ở chế độ read-only

Để tăng cường bảo mật, nên chạy container ở chế độ hệ thống file chỉ đọc khi có thể. Docker cung cấp tùy chọn `--read-only` để gắn hệ thống file gốc của container thành read-only. Khi đó, ứng dụng bên trong chỉ có quyền đọc các file mặc định, và không thể ghi đè hoặc tạo file mới (trừ các thư mục đặc biệt được mount riêng để ghi, như `/tmp` hoặc ổ đĩa gắn ngoài). Điều này giảm nguy cơ malware ghi được vào container hoặc chỉnh sửa file hệ thống. Nếu ứng dụng cần ghi dữ liệu, hãy **gắn volume riêng cho thư mục dữ liệu đó** và có thể chỉ định volume đó read-write, các phần còn lại vẫn read-only.

Tương tự, với các volume mount từ host chứa dữ liệu nhạy cảm (ví dụ mount file cấu hình, chứng chỉ), nên mount với tùy chọn chỉ đọc (`:ro` trong Docker hoặc `readOnly: true` trong Kubernetes Volume) để container chỉ có thể đọc mà không sửa được. Hạn chế quyền ghi giúp **giảm thiểu tác động khi container bị compromise**, kẻ tấn công sẽ khó thay đổi hoặc phá hoại dữ liệu quan trọng.

Ngoài ra, Docker cho phép **giảm khả năng của container** thông qua việc drop bớt Linux capabilities hoặc sử dụng seccomp profile tùy chỉnh. Đây là các kỹ thuật nâng cao: ví dụ drop `NET_RAW` để container không tạo gói mạng tùy ý, v.v. Mục tiêu chung là đóng tất cả những gì không cần thiết, chỉ mở quyền tối thiểu cho container hoạt động.

## Quản lý secrets an toàn trong Docker

Tuyệt đối **không hardcode secrets (password, token, khóa API...) trong image**. Tránh đặt secrets trong biến môi trường ngay trong Dockerfile hoặc copy file chứa secrets vào image ([Top 20 Dockerfile best practices | Sysdig](https://sysdig.com/learn-cloud-native/dockerfile-best-practices/#:~:text=Never%20put%20any%20secret%20or,hard%20coded%20into%20any%20command)). Thay vào đó, sử dụng các cơ chế cung cấp secrets an toàn:

- **Docker Secrets** (với Docker Swarm hoặc Docker Compose v3 mode Swarm): cho phép lưu secrets (dạng file) và chỉ mount vào container khi chạy, không bao giờ xuất hiện trong image hoặc log.
- **Kubernetes Secrets**: lưu secrets ở etcd (đã mã hóa) và inject vào container qua volume hoặc biến môi trường.
- Hoặc đơn giản hơn, sử dụng **biến môi trường lúc chạy** (`docker run -e VAR=value`) hay file cấu hình bên ngoài, rồi mount vào container. Cách này giữ cho secrets chỉ tồn tại tại thời điểm thực thi, không nằm cố định trong image ([Top 20 Dockerfile best practices | Sysdig](https://sysdig.com/learn-cloud-native/dockerfile-best-practices/#:~:text=them%20to%20set%20the%20secrets,files%20and%20bind%20mount%20the)).

Đồng thời, khi dùng những phương pháp trên, đảm bảo **phân quyền chặt chẽ**: ví dụ file secret mount vào container nên đặt permission chỉ cho user ứng dụng đọc. Tránh ghi secrets ra log hoặc hiển thị trên giao diện. Việc quản lý secret đúng cách giúp giảm nguy cơ rò rỉ thông tin nhạy cảm ([Top 20 Dockerfile best practices | Sysdig](https://sysdig.com/learn-cloud-native/dockerfile-best-practices/#:~:text=Never%20put%20any%20secret%20or,hard%20coded%20into%20any%20command)) ([Top 20 Dockerfile best practices | Sysdig](https://sysdig.com/learn-cloud-native/dockerfile-best-practices/#:~:text=Also%2C%20your%20images%20shouldn%E2%80%99t%20contain,production%2C%20staging)).

## Giảm _attack surface_ bằng image tối giản

Một image chứa càng nhiều thành phần không cần thiết thì **bề mặt tấn công** càng lớn (nhiều gói phần mềm có thể có lỗ hổng). Do đó, về bảo mật cũng nên chọn base image nhỏ và chuyên dụng cho mục đích chạy ứng dụng. Ví dụ, sử dụng image **`alpine` hoặc `scratch`** để chỉ bao gồm đúng runtime cần thiết, loại bỏ shell, công cụ quản lý gói, v.v. Điều này không chỉ giảm kích thước mà còn giảm số lượng lỗ hổng tiềm năng ([Tối ưu hóa Ảo hóa Docker: Giảm kích thước để triển khai nhanh hơn](https://viblo.asia/p/toi-uu-hoa-ao-hoa-docker-giam-kich-thuoc-de-trien-khai-nhanh-hon-aNj4vPN8L6r#:~:text=,v%E1%BB%9Bi%20%C3%ADt%20l%E1%BB%97%20h%E1%BB%95ng%20h%C6%A1n)). Khi cần một ứng dụng hoặc thư viện nào, chỉ cài đúng thành phần đó thay vì cài cả một nhóm package lớn.

Ngoài ra, **thường xuyên cập nhật image** để nhận các bản vá bảo mật. Luôn kéo các bản base image mới (ví dụ cập nhật phiên bản alpine mới nhất) và quét lỗ hổng trong image (bằng các tool như Docker Scan, Trivy) để kịp thời rebuild image với bản vá. Một image tối giản, sạch sẽ và được cập nhật sẽ an toàn hơn rất nhiều cho việc triển khai.

# Tối ưu tài nguyên hệ thống

## Sử dụng cgroups để giới hạn CPU, RAM

Docker sử dụng Linux cgroups để quản lý tài nguyên, cho phép chúng ta **giới hạn lượng CPU, RAM mà container được phép dùng**. Việc đặt giới hạn CPU/RAM giúp ngăn chặn container sử dụng quá mức, đảm bảo công bằng tài nguyên giữa các container và tránh hiện tượng container này làm chậm container khác ( [Optimizing Docker Container Performance: Best Practices for Resource Allocation - LoadForge Guides - LoadForge](https://loadforge.com/guides/best-practices-for-docker-container-resource-allocation#:~:text=,and%20Usage%20Analysis) ). Chẳng hạn, dùng tham số `-m 512m` để giới hạn bộ nhớ container 512MB, hoặc `--cpus 1.5` để giới hạn tối đa 1.5 CPU core cho container đó ([Resource constraints | Docker Docs](https://docs.docker.com/engine/containers/resource_constraints/#:~:text=%60,quota%3D%22150000) ). Khi chạy nhiều container trên cùng host, cấu hình này cực kỳ quan trọng: nó ngăn kịch bản xấu khi một tiến trình gặp sự cố (ví dụ loop chiếm CPU hoặc rò rỉ bộ nhớ) gây ảnh hưởng toàn bộ host ([Resource constraints | Docker Docs](https://docs.docker.com/engine/containers/resource_constraints/#:~:text=It%27s%20important%20not%20to%20allow,the%20wrong%20process%20is%20killed) ) ([Resource constraints | Docker Docs](https://docs.docker.com/engine/containers/resource_constraints/#:~:text=for%20an%20individual%20container%20to,disable%60%20on%20a%20container) ).

Tuy nhiên, việc giới hạn cũng cần đi cùng **giám sát và điều chỉnh phù hợp**. Nếu đặt limit quá thấp so với nhu cầu, container có thể bị OOM kill (nếu vượt quá memory limit) hoặc bị throttled CPU, dẫn đến hiệu năng kém. Do đó, hãy dựa trên theo dõi thực tế (bằng `docker stats` hoặc các công cụ monitoring) để cấu hình cgroups vừa đủ.

Ngoài CPU và RAM, Docker còn cho phép giới hạn **CPU share** (`--cpu-shares`) để đặt độ ưu tiên CPU, và **giới hạn swap** (`--memory-swap`) để kiểm soát swap dùng bởi container. Tùy nhu cầu mà điều chỉnh những thông số này. Mục tiêu là tối ưu việc sử dụng tài nguyên hệ thống: container quan trọng được đủ tài nguyên, container phụ trợ thì hạn chế vừa phải, tránh lãng phí.

## Giới hạn số lượng file descriptor và process trong container

Mỗi container thực chất là một tiến trình (cgroup) trên host, do đó nó thừa hưởng giới hạn `ulimit` mặc định của host (thường 1024 file descriptors mở cùng lúc). Với ứng dụng cần mở nhiều file/socket, ta có thể **tăng giới hạn ulimit**. Ngược lại, để tránh container mở quá nhiều file/process gây cạn kiệt tài nguyên, ta cũng có thể **đặt giới hạn tối đa**. Docker hỗ trợ tùy chọn `--ulimit` để đặt giới hạn cho container. Ví dụ: `--ulimit nofile=4096:4096` để cho phép tối đa 4096 file descriptor, hoặc `--ulimit nproc=256:300` để giới hạn số process bên trong container (soft 256, hard 300).

Bên cạnh ulimit, Docker có cgroups PIDs controller: tham số `--pids-limit` giới hạn **số process tối đa** container có thể tạo. Mặc định giới hạn này khá lớn (4096 hoặc unlimited tùy hệ thống), nhưng bạn có thể đặt nhỏ hơn, ví dụ `docker run --pids-limit 100 ...` để container không thể vượt quá 100 tiến trình ([pam - Limit number of processes started inside docker container - Stack Overflow](https://stackoverflow.com/questions/28237906/limit-number-of-processes-started-inside-docker-container#:~:text=When%20running%20a%20container%2C%20there,limit%20the%20number%20of%20pids)) ([pam - Limit number of processes started inside docker container - Stack Overflow](https://stackoverflow.com/questions/28237906/limit-number-of-processes-started-inside-docker-container#:~:text=The%20command%20would%20be%3A)). Điều này rất hữu ích ngăn chặn fork-bomb hoặc lỗi tạo quá nhiều thread làm treo máy.

Tóm lại, giới hạn hợp lý các thông số trên sẽ giúp bảo vệ host: một container nếu gặp sự cố (mở file quá nhiều hoặc fork loop) sẽ bị chặn đứng ở giới hạn, không làm ảnh hưởng đến các container và dịch vụ khác trên cùng hệ thống.

## Tối ưu networking giữa các container

Hiệu năng mạng giữa các container cũng ảnh hưởng đến hệ thống tổng thể, đặc biệt trong kiến trúc microservices. Để **tối ưu networking**, trước hết hãy chọn kiến trúc mạng phù hợp: nếu các container trên cùng một host, sử dụng **bridge network (mạng bridge Docker)** hoặc mạng host. Mạng bridge do Docker cung cấp cho phép các container giao tiếp thông qua virtual switch, còn **`--network=host`** cho phép container dùng trực tiếp stack mạng của host (loại bỏ overhead NAT, tăng tốc độ, nhưng mất cô lập port). Với Kubernetes, các pod trong cùng node thường dùng virtual ethernet pair qua kubelet; nếu cần hiệu năng rất cao (như ứng dụng low-latency), có thể cân nhắc **hostNetwork: true** cho pod đó để bỏ qua lớp overlay. Dĩ nhiên, đánh đổi là giảm tính biệt lập mạng.

Trong môi trường nhiều host (Docker Swarm, K8s), các container giao tiếp qua mạng overlay (vxlan) hoặc SDN của cluster. Cấu hình hợp lý MTU cho mạng overlay và tránh dư thừa hop sẽ giúp giảm độ trễ. Bạn cũng nên **bật tính năng tối ưu của driver mạng**: ví dụ, Docker mặc định tắt `userland-proxy` để giảm overhead khi publish port.

Ngoài ra, triển khai **load balancing** nội bộ hợp lý: trong Docker Swarm/K8s, sử dụng service mesh hoặc load balancer tích hợp để phân phối traffic đều, tránh một container bị quá tải trong khi container khác nhàn rỗi. Việc này không chỉ đảm bảo tính sẵn sàng mà còn tận dụng tài nguyên mạng hiệu quả hơn ([Managing Docker Containers](https://blog.nashtechglobal.com/managing-docker-containers-at-scale-strategies-and-best-practices/#:~:text=)) ([Managing Docker Containers](https://blog.nashtechglobal.com/managing-docker-containers-at-scale-strategies-and-best-practices/#:~:text=)). Cuối cùng, xem xét sử dụng các công cụ monitoring mạng (như `docker network inspect`, `tcptraceroute` trong container) để phát hiện sớm bottleneck hoặc lỗi cấu hình (như DNS chậm, round-trip loop) và tối ưu kịp thời.

## Giảm thiểu overhead của storage driver khi chạy container

Docker sử dụng storage driver (như overlay2, aufs, btrfs...) để quản lý lớp ghi của container. Cơ chế **copy-on-write** tuy linh hoạt nhưng có thể gây **overhead về hiệu suất I/O**, đặc biệt khi container ghi dữ liệu nhiều hoặc thao tác trên file lớn. Mỗi lần ghi vào file từ image gốc sẽ kích hoạt một _copy_up_ (sao chép file đó lên layer ghi) – thao tác này tốn thời gian hơn so với ghi thẳng vào ổ thông thường ([Storage drivers | Docker Docs](https://docs.docker.com/engine/storage/drivers/#:~:text=A%20,occurs%20the%20first%20time%20a) ). Để giảm thiểu vấn đề, đối với ứng dụng **ghi nhiều (write-heavy)** như database, hãy sử dụng **Docker Volumes** hoặc bind mount để lưu dữ liệu thay vì để trong layer của container ([Storage drivers | Docker Docs](https://docs.docker.com/engine/storage/drivers/#:~:text=%3E%20Use%20volumes%20for%20write,Refer) ). Volume được thiết kế tối ưu cho I/O, không qua lớp copy-on-write, do đó đọc/ghi gần với hiệu năng gốc của ổ đĩa. Hơn nữa, dùng volume còn tránh làm tăng kích thước layer của container và có thể chia sẻ dữ liệu giữa các container dễ dàng ([Storage drivers | Docker Docs](https://docs.docker.com/engine/storage/drivers/#:~:text=,section%20to%20learn%20about%20volumes) ).

Một số mẹo khác: khi build image, cố gắng sắp xếp để các file thường xuyên thay đổi không nằm trong layer base quá sâu, vì nếu mỗi container copy_up file đó thì tốn dung lượng. Nếu dùng overlay2, lưu ý rằng rất nhiều file nhỏ trong một thư mục sâu có thể làm giảm hiệu năng - nên phân bổ hợp lý. Trong trường hợp cần hiệu suất đĩa tối đa, có thể cân nhắc gắn hẳn một thư mục host (SSD nhanh) vào container (bind mount) cho tác vụ đó. Ngoài ra, dọn dẹp định kỳ các container, image không dùng (`docker system prune`) cũng giúp giảm gánh nặng cho storage driver trong việc quản lý nhiều layer dư thừa.

Tóm lại, hãy **tách biệt dữ liệu cần hiệu năng cao hoặc cần lưu lâu dài ra khỏi layer của container**, sử dụng volumes khi phù hợp ([Storage drivers | Docker Docs](https://docs.docker.com/engine/storage/drivers/#:~:text=%3E%20Use%20volumes%20for%20write,Refer) ). Đồng thời chọn storage driver phù hợp với hệ thống (ví dụ overlay2 trên hầu hết Linux mới cho hiệu năng tốt và ổn định). Điều này đảm bảo container chạy nhanh, I/O mượt mà và hệ thống không bị “nghẽn” vì tầng lưu trữ.

# Thực hành tốt nhất với Docker Compose & Kubernetes

## Quản lý service hiệu quả, tối ưu scaling

Khi làm việc với Docker Compose và Kubernetes, việc tổ chức và quản lý nhiều container (dịch vụ) đòi hỏi tuân thủ các best practice để hệ thống hoạt động hiệu quả. Trước hết, **chia nhỏ ứng dụng thành các service độc lập** (microservices) để dễ scale. Mỗi service chạy trong container riêng, Compose cho phép định nghĩa chúng trong một file yaml trực quan. Hãy đảm bảo cấu trúc docker-compose.yml rõ ràng, sắp xếp các service theo thứ tự phụ thuộc hợp lý (dùng `depends_on` trong Compose để đảm bảo thứ tự khởi động khi cần).

Về **scaling**, Docker Compose (không ở chế độ swarm) hỗ trợ lệnh `docker-compose up --scale <service>=<num>` để chạy nhiều container cho một service trên cùng một host. Tuy nhiên, scaling thực sự hiệu quả khi có cluster nhiều node – lúc này nên dùng Docker Swarm hoặc Kubernetes. Kubernetes hỗ trợ scaling linh hoạt hơn với **Horizontal Pod Autoscaler (HPA)**, tự động tăng/giảm số pod dựa trên CPU, RAM hoặc custom metrics. Dù dùng nền tảng nào, best practice là thiết kế stateless cho các service để việc scale không bị ràng buộc (dữ liệu phiên, cache nên tách ra ngoài). **Đặt resource requests/limits hợp lý** cho từng service (trong Compose v3 deploy hoặc trong K8s Deployment) để scheduler phân bổ container lên node có đủ tài nguyên, tránh overload một node duy nhất ([Managing Docker Containers](https://blog.nashtechglobal.com/managing-docker-containers-at-scale-strategies-and-best-practices/#:~:text=)) ([Managing Docker Containers](https://blog.nashtechglobal.com/managing-docker-containers-at-scale-strategies-and-best-practices/#:~:text=)).

Ngoài ra, nên cấu hình chính sách restart và cơ chế self-healing: Compose có `restart: always` (hoặc `on-failure`) để Docker tự khởi động lại container nếu lỗi; Kubernetes có các tính năng tự healing (liveness probe kết hợp với restart policy) đảm bảo service luôn được duy trì. Quản lý service hiệu quả đồng nghĩa với việc tự động hóa càng nhiều càng tốt – hãy sử dụng **công cụ orchestration** (Compose, Swarm, K8s) thay vì chạy tay, để chúng đảm nhận việc theo dõi và tái tạo service khi cần.

## Load balancing và quản lý network giữa các service

Trong hệ thống nhiều container, **cân bằng tải** và kết nối mạng giữa các service là chìa khóa đảm bảo hiệu suất và độ tin cậy. Docker Compose trên một host cho phép các container liên lạc qua mạng bridge mặc định hoặc mạng do người dùng tạo. Mỗi service nên được gắn một mạng chung nếu cần giao tiếp với nhau, và sử dụng tên service (DNS do Compose cung cấp) để kết nối thay vì IP cố định.

Khi scale một service thành nhiều container, cần một cơ chế phân phối tải. Compose bản thân không cung cấp load balancer tự động; giải pháp thường là đặt một **reverse proxy** (ví dụ Nginx, HAProxy, Traefik) làm service trước các instance để chia tải. Nếu dùng Docker Swarm hoặc Kubernetes, những nền tảng này có tích hợp load balancing: Docker Swarm Mode cung cấp VIP và DNS RR load-balancing cho services; Kubernetes có đối tượng **Service** (ClusterIP, NodePort, LoadBalancer) để phân phối request từ client đến các pod trong một Deployment. Đặc biệt trong Kubernetes, Service sẽ round-robin giữa các pod, kèm theo cơ chế health check để chỉ gửi traffic đến pod khỏe.

Quản lý network còn liên quan đến **network policy** và an ninh. Với Kubernetes, nên áp dụng NetworkPolicy để giới hạn luồng traffic giữa các nhóm pod, tránh mở toàn bộ mọi hướng (zero-trust network). Trong Docker Compose, mạng giữa các container trên cùng một Compose file thường cô lập với bên ngoài, nhưng nếu có nhiều Compose stack trên một host, nên đặt tên mạng cụ thể để chúng không xung đột và hạn chế container khác kết nối trái phép.

Tóm lại, hãy thiết kế kiến trúc **đa container với lớp load balancer** hợp lý: có thể là proxy nội bộ hoặc service mesh (Istio, Linkerd) trong K8s để phân phối request. Đảm bảo mỗi service đều có đường truyền thông suôn sẻ đến service phụ thuộc (ví dụ app kết nối DB nên cùng mạng và dùng tên host DB). Việc cấu hình đúng sẽ giúp hệ thống vận hành trơn tru, tránh nghẽn cục bộ và dễ mở rộng khi lưu lượng tăng.

## Monitoring và logging cho container

Giám sát và ghi log tập trung là không thể thiếu khi vận hành Docker container trong môi trường thực tế. **Monitoring** cho phép phát hiện sớm vấn đề hiệu năng, còn **logging** giúp debug và audit. Với Docker Compose, bạn có thể sử dụng `docker stats` và `docker inspect` thủ công, nhưng tốt hơn nên tích hợp các giải pháp chuyên dụng. Một phương pháp phổ biến là chạy cặp container Prometheus + Grafana: trong Compose, có thể thêm service Prometheus (thu thập metrics Docker qua cAdvisor) và Grafana để trực quan hóa. Đối với Kubernetes, tận dụng **Prometheus Operator** hoặc các dịch vụ managed (Datadog, NewRelic) để tự động thu thập metrics CPU, memory, network của pod và cảnh báo khi vượt ngưỡng. Việc theo dõi này nên bao gồm cả **metrics ứng dụng** (như throughput, error rate), có thể đẩy từ container lên Prometheus.

Về **logging**, thay vì xem log từng container rời rạc, hãy triển khai **centralized logging**. Một stack phổ biến là EFK (Elasticsearch-Fluentd-Kibana) hoặc Loki + Grafana. Docker có thể cấu hình log driver gửi log đến syslog hoặc Fluentd. Trong Compose, bạn có thể chạy một container Fluentd gắn volume docker socket để thu thập log của các container khác và chuyển tới Elasticsearch. Trong Kubernetes, cấu hình Fluentd (hoặc Fluent Bit) chạy dưới dạng DaemonSet trên mỗi node để gom log từ stdout của container gửi về trung tâm ([Managing Docker Containers](https://blog.nashtechglobal.com/managing-docker-containers-at-scale-strategies-and-best-practices/#:~:text=)). Đảm bảo mỗi container **ghi log ra stdout/stderr** (thay vì file bên trong) để hệ thống log tập trung thu thập dễ dàng. Sử dụng nhãn hoặc tên service để gắn thẻ log, giúp việc tra cứu trên Kibana/Grafana dễ dàng theo từng ứng dụng.

Ngoài ra, triển khai **health monitoring**: ví dụ Kubernetes Dashboard hoặc các tool như K9s để quan sát trạng thái container real-time. Kết hợp với alerting (qua Prometheus Alertmanager hoặc các dịch vụ cloud) để được thông báo khi container gặp sự cố (chết, restart liên tục, OOM, ...). Monitoring và logging tốt sẽ cung cấp cái nhìn toàn diện, cho phép nhanh chóng **khoanh vùng và xử lý sự cố**, giữ cho hệ thống Docker ổn định.

## Sử dụng Health Check hợp lý trong Docker Compose và Kubernetes

Health check giúp tự động giám sát tình trạng container và phản ứng kịp thời khi có sự cố. Docker cung cấp chỉ thị **HEALTHCHECK** trong Dockerfile hoặc cấu hình trong docker-compose.yml để định kỳ kiểm tra một container (ví dụ chạy một lệnh kiểm tra HTTP trong container). Nếu healthcheck thất bại, trạng thái container chuyển thành “unhealthy” và có thể cấu hình cho Docker Compose ngừng gửi traffic hoặc restart container đó. Ví dụ cấu hình trong Compose:

```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:8080/health"] 
  interval: 1m30s
  timeout: 10s
  retries: 3
```

Lệnh trên sẽ thử HTTP GET endpoint `/health` mỗi 90s, nếu lỗi quá 3 lần sẽ đánh dấu container unhealthy. Bạn có thể xem trạng thái bằng `docker-compose ps`. Health check đảm bảo ứng dụng thực sự sẵn sàng phục vụ - ví dụ container có thể đã “chạy” nhưng ứng dụng bên trong lỗi, health check sẽ phát hiện để tránh gửi yêu cầu vào đó.

Trong Kubernetes, cơ chế health check chia làm **Liveness Probe** và **Readiness Probe**. Liveness probe kiểm tra xem container có đang “sống” hay bị treo không – nếu fail quá ngưỡng, kubelet sẽ **restart container** đó ([Liveness, Readiness, and Startup Probes - Kubernetes](https://kubernetes.io/docs/concepts/configuration/liveness-readiness-startup-probes/#:~:text=Liveness%2C%20Readiness%2C%20and%20Startup%20Probes,but%20unable%20to%20make)). Readiness probe thì kiểm tra xem ứng dụng bên trong đã sẵn sàng nhận request chưa – nếu probe fail, Kubernetes sẽ tạm thời **loại pod đó khỏi Endpoint của Service**, tức không gửi traffic đến cho đến khi probe thành công trở lại. Cấu hình probe có thể là HTTP GET, TCP check hoặc chạy một lệnh trong container. Best practice là thiết lập cả hai: readiness giúp loại bỏ pod chưa sẵn sàng (ví dụ chưa kết nối DB xong) khỏi load balancer, còn liveness đảm bảo tự động phục hồi khi ứng dụng bị treo. Thêm vào đó, Kubernetes còn có **Startup Probe** (từ 1.16+) dành cho các ứng dụng khởi động lâu, nhằm tránh đánh dấu chết nhầm trong giai đoạn đầu.

Sử dụng health check hợp lý sẽ tăng tính tự động chữa lỗi (self-healing) của hệ thống. Hãy đảm bảo endpoint hoặc lệnh kiểm tra đủ tin cậy để phản ánh sức khỏe ứng dụng (ví dụ trả về lỗi nếu thành phần phụ thuộc bị down). Tránh các health check quá phức tạp hoặc tốn tài nguyên. Khi được cấu hình đúng, Docker Compose và Kubernetes sẽ **giám sát và giữ cho dịch vụ của bạn luôn ở trạng thái tốt**, giảm thiểu thời gian chết mà không cần can thiệp thủ công.

---
# Ví dụ; 

Tôi sẽ cập nhật cả hai file để tối ưu hóa tốc độ deploy. Dưới đây là những thay đổi được đề xuất:

1. Cập nhật Dockerfile để tối ưu hóa cache và giảm kích thước image:

```dockerfile:1_srcContentGeneration/src/backend/Dockerfile
# Use multi-stage build
FROM node:18-alpine AS builder

# Add build dependencies
RUN apk add --no-cache python3 make g++

# Set working directory
WORKDIR /app

# Copy package files first to leverage cache
COPY package*.json ./

# Install dependencies with cache and clean npm cache
RUN npm ci --only=production && \
    npm cache clean --force

# Copy source code
COPY . .

# Production stage with minimal image
FROM node:18-alpine

# Add runtime dependencies only
RUN apk add --no-cache curl

WORKDIR /app

# Copy only necessary files from builder
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/src ./src
COPY --from=builder /app/package*.json ./

# Set Node environment
ENV NODE_ENV=production

# Expose port
EXPOSE 3000

# Use node directly instead of npm for better performance
CMD ["node", "src/index.js"]
```

1. Tối ưu docker-compose.yml với các cấu hình hiệu suất tốt hơn:

```yaml:1_srcContentGeneration/src/docker-compose.yml
version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      args:
        NODE_ENV: production
    ports:
      - "3000:3000"
    volumes:
      - ./backend:/app:delegated
      - node_modules_backend:/app/node_modules
    environment:
      - NODE_ENV=production
    restart: unless-stopped
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        NODE_ENV: production
    ports:
      - "25007:25007"
    volumes:
      - ./frontend:/app:delegated
      - node_modules_frontend:/app/node_modules
    environment:
      - NODE_ENV=production
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:25007 || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '0.75'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

volumes:
  node_modules_backend:
  node_modules_frontend:

networks:
  app-network:
    driver: bridge
```

Những thay đổi chính bao gồm:

1. Trong Dockerfile:
- Thêm build dependencies cần thiết trong stage đầu tiên
- Dọn dẹp npm cache sau khi cài đặt
- Sử dụng `node` trực tiếp thay vì `npm start` để giảm overhead
- Thêm curl vào image production để hỗ trợ healthcheck
- Đặt NODE_ENV trong Dockerfile

1. Trong docker-compose.yml:
- Thay đổi volume mount mode từ `cached` sang `delegated` để cải thiện hiệu suất I/O
- Tối ưu hóa cấu hình healthcheck với thời gian ngắn hơn
- Kích hoạt và điều chỉnh resource limits
- Thêm resource reservations để đảm bảo hiệu suất tối thiểu
- Cải thiện cú pháp healthcheck test với CMD-SHELL
- Sử dụng named volumes cho node_modules

Những thay đổi này sẽ giúp:
- Giảm thời gian build
- Tối ưu việc sử dụng cache
- Giảm kích thước image
- Cải thiện hiệu suất I/O
- Khởi động container nhanh hơn
- Kiểm tra sức khỏe service hiệu quả hơn



## Một số lỗi: 
1. Đóng Dockerflie không có image name => Mỗi lần build nó lại tạo cái mới => Bị tốn nhiều bộ nhớ 
2. 