{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Nguyên lý của các mô hình tạo ảnh khác gì so với các mô hình gen văn bản: \n",
    "\n",
    "\n",
    "\n",
    "#### 2. Cấu trúc Prompt ảnh chung sau quá trình làm thực chiến: \n",
    "Stable Diffusion: \n",
    "```\n",
    "1. Subject - Features - Background\n",
    "2. Medium (/Type/category of artwork): Ultra realistic illustration (Drawings that are very realistic. Good to use with people.), concept art (Illustration style, 2D.), Digital painting(Digital art style), Digital, Digital Minimalist Illustration\n",
    "3. Style: Modernist(vibrant color, high contrast), pastel color, \n",
    "4. Art-sharing website, Artist: artstation(website Modern illustration, fantasy)\n",
    "5. Resolution: unreal engine\n",
    "6. Additional details\n",
    "7. Color: iridescent gold, pastel color (Màu sắc nhẹ nhàng), vibrant color (Màu sắc rực rỡ,), Soft color\n",
    "8. Lighting: cinematic lighting\n",
    "```\n",
    "\n",
    "DALL - E - 3: \n",
    "```\n",
    "1. Type: Digital Minimalist Illustration - Clean Lines and Shapes - Flat Design - Soft Shading.\n",
    "6. Style: clean and simple style, flat design, pastel color\n",
    "5. Mood/Color: Use only 3-4 pastel colors, create a warm and friendly atmosphere with a well-balanced contrast to highlight {context}.\n",
    "\n",
    "2. Subject: Từ step 1 => 'Conceptualize the Image'.\n",
    "3. Features: Focused on {context} with the following elements:\n",
    "- Characters: Facial expressions and positions that perfectly match {context}.\n",
    "4. Setting: Setting: A plain background with no patterns, only include two small elements surrounding the characters. \n",
    "\n",
    "7. Emphasis: Highlight context {context} as the image's main focus. \n",
    "8. Note: Do not include any words or text in the image. \n",
    "```\n",
    "\n",
    "#### 3. Thực chiến: Bài toán giữ nguyên style trong gen ảnh với Stable Diffusion - ComfyUI và Dalle3 with gen_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Nguyên lý của các mô hình tạo ảnh khác gì so với các mô hình gen văn bản\n",
    "\n",
    "**Mô hình tạo ảnh:**\n",
    "\n",
    "- **Nguyên lý hoạt động:** Mô hình tạo ảnh, như Stable Diffusion và DALL-E, thường sử dụng mạng nơ-ron sâu (Deep Neural Networks) với các kiến trúc phức tạp như Generative Adversarial Networks (GANs) hoặc Variational Autoencoders (VAEs). Chúng học cách tạo ra hình ảnh bằng cách tối ưu hóa để tạo ra những hình ảnh trông giống như dữ liệu huấn luyện.\n",
    "    - So sánh mô hình bên trong của DALL-E3 và Stable Diffusion\n",
    "- **Quá trình huấn luyện:** Mô hình được huấn luyện trên hàng triệu hình ảnh với các chú thích hoặc mô tả. Mục tiêu là để mô hình hiểu được mối liên hệ giữa các mô tả và hình ảnh tương ứng.\n",
    "- **Đầu vào:** Thường là một đoạn văn bản mô tả (prompt) và đôi khi có thêm các tham số điều chỉnh khác (như phong cách, màu sắc).\n",
    "- **Đầu ra:** Một hình ảnh phù hợp với mô tả.\n",
    "\n",
    "**Mô hình gen văn bản:** Các mô hình gen văn bản như GPT-4 sử dụng kiến trúc Transformer để học mối quan hệ giữa các từ và ngữ cảnh. Chúng tối ưu hóa để dự đoán từ tiếp theo trong một câu dựa trên ngữ cảnh trước đó.\n",
    "\n",
    "**Sự khác biệt chính:**\n",
    "\n",
    "- **Kiến trúc mạng:** Các mô hình tạo ảnh thường sử dụng GANs hoặc VAEs, trong khi mô hình gen văn bản thường sử dụng Transformer.\n",
    "- **Mục tiêu tối ưu hóa:** Mô hình tạo ảnh tối ưu hóa để tạo ra hình ảnh đúng với mô tả, trong khi mô hình gen văn bản tối ưu hóa để dự đoán từ tiếp theo hợp lý.\n",
    "\n",
    "\n",
    "### 2. Cấu trúc Prompt ảnh chung sau quá trình làm thực chiến\n",
    "\n",
    "**Stable Diffusion:**\n",
    "\n",
    "```\n",
    "1. Subject - Features - Background\n",
    "2. Medium (/Type/category of artwork): Ultra realistic illustration, concept art, Digital painting, Digital, Digital Minimalist Illustration\n",
    "3. Style: Modernist, pastel color\n",
    "4. Art-sharing website, Artist: artstation\n",
    "5. Resolution: unreal engine\n",
    "6. Additional details\n",
    "7. Color: iridescent gold, pastel color, vibrant color, Soft color\n",
    "8. Lighting: cinematic lighting\n",
    "```\n",
    "\n",
    "**DALL-E 3:**\n",
    "\n",
    "```\n",
    "1. Type: Digital Minimalist Illustration - Clean Lines and Shapes - Flat Design - Soft Shading.\n",
    "2. Style: clean and simple style, flat design, pastel color\n",
    "3. Mood/Color: Use only 3-4 pastel colors, create a warm and friendly atmosphere with a well-balanced contrast to highlight {context}.\n",
    "4. Subject: Conceptualize the Image based on the context.\n",
    "5. Features: Focused on {context} with the following elements:\n",
    "    - Characters: Facial expressions and positions that perfectly match {context}.\n",
    "6. Setting: A plain background with no patterns, only include two small elements surrounding the characters.\n",
    "7. Emphasis: Highlight context {context} as the image's main focus.\n",
    "8. Note: Do not include any words or text in the image.\n",
    "```\n",
    "\n",
    "### 3. Thực chiến: Bài toán giữ nguyên style trong gen ảnh với Stable Diffusion - ComfyUI và Dalle3 with gen_id\n",
    "\n",
    "**Stable Diffusion với ComfyUI:**\n",
    "\n",
    "- **Kỹ thuật:** Để giữ nguyên style khi gen ảnh, bạn có thể sử dụng các mô hình huấn luyện thêm (fine-tuning) với các dataset được lựa chọn cẩn thận. ComfyUI có thể giúp bạn điều chỉnh các thông số như seed, noise, và các layers của mô hình để duy trì một phong cách nhất quán.\n",
    "- **Ví dụ:** Nếu bạn muốn gen các ảnh với phong cách pastel và minimalist, bạn có thể tạo một dataset chỉ bao gồm các ảnh theo phong cách đó và huấn luyện thêm mô hình với dataset này. Khi tạo ảnh, bạn đảm bảo rằng prompt bao gồm các từ khóa liên quan đến phong cách mong muốn.\n",
    "\n",
    "**DALL-E 3 with gen_id:**\n",
    "\n",
    "- **Kỹ thuật:** Sử dụng `gen_id` để duy trì sự nhất quán trong phong cách. `gen_id` giúp tạo ra các ảnh tiếp theo dựa trên một ảnh gốc, giúp duy trì phong cách và chủ đề.\n",
    "- **Ví dụ:** Khi bạn có một ảnh gốc với phong cách cụ thể, bạn sử dụng `gen_id` để yêu cầu mô hình tạo ra các ảnh tiếp theo với phong cách tương tự. Điều này đảm bảo rằng các ảnh được gen ra sẽ có sự đồng nhất về phong cách và chi tiết.\n",
    "\n",
    "Những kỹ thuật và cấu trúc trên sẽ giúp bạn tạo ra các hình ảnh nhất quán và phù hợp với yêu cầu công việc của mình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngày 1 - 27/07/2024 - Bài viết đầu này: Điểm khác biệt về NGUYÊN LÝ HOẠT ĐỘNG của Mô Hình Tạo Ảnh so với Mô Hình Tạo Văn Bản và So Sánh Nhanh 2 Kiến Trúc GANs-VAEs\n",
    "\n",
    "https://doanngoccuongbkegnh.github.io/home/blog/2024/07/27/Bai-viet-1-Image-Prompting-P1.html\n",
    "\n",
    "#myteam #ai #prompting #prompting_image #GANs #VAEs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài 1: ***Nguyên lý của các mô hình tạo ảnh khác gì so với các mô hình gen văn bản và so sánh kiến trúc GANs-VAEs trong việc Gen Ảnh***\n",
    "\n",
    "### Phần 1: Khác biệt giữa mô hình tạo ảnh và mô hình gen văn bản\n",
    "**Sự khác biệt chính:**\n",
    "- **Kiến trúc mạng:** Mô hình tạo ảnh dùng GANs/VAEs, mô hình gen văn bản dùng Transformer.\n",
    "- **Mục tiêu tối ưu hóa:** Mô hình tạo ảnh tạo hình ảnh đúng mô tả, mô hình gen văn bản dự đoán từ tiếp theo hợp lý.\n",
    "\n",
    "**Mô hình tạo ảnh:**\n",
    "- **Nguyên lý hoạt động:** Sử dụng mạng nơ-ron sâu như GANs hoặc VAEs để tạo ra hình ảnh từ mô tả văn bản.\n",
    "- **Quá trình huấn luyện:** Được huấn luyện trên hàng triệu hình ảnh có chú thích để hiểu mối liên hệ giữa mô tả và hình ảnh.\n",
    "- **Đầu vào:** Đoạn văn bản mô tả (prompt) và các tham số điều chỉnh. => **Đầu ra:** Hình ảnh phù hợp với mô tả.\n",
    "\n",
    "\n",
    "### Phần 2: So sánh kiến trúc GANs và VAEs (DALL-E3 và Stable Diffusion)\n",
    "\n",
    "**Khác biệt chính:**\n",
    "- **Kiến trúc mạng:** GANs sử dụng hai mạng cạnh tranh (Generator và Discriminator), trong khi VAEs sử dụng một mạng để mã hóa và giải mã.\n",
    "- **Quá trình huấn luyện:** GANs huấn luyện qua quá trình cạnh tranh, trong khi VAEs huấn luyện qua quá trình tối ưu hóa phân phối tiềm ẩn.\n",
    "- **Mục tiêu:** GANs tập trung vào việc tạo ra hình ảnh giả mà Discriminator không thể phân biệt được, còn VAEs tập trung vào việc tái tạo lại dữ liệu đầu vào và học được biểu diễn tiềm ẩn của dữ liệu.\n",
    "\n",
    "\n",
    "    **GANs (Generative Adversarial Networks):**\n",
    "    - **Kiến trúc:** Gồm hai mạng nơ-ron: Generator và Discriminator. Generator tạo ra hình ảnh mới, trong khi Discriminator cố gắng phân biệt giữa hình ảnh thật và giả.\n",
    "    - **Nguyên lý hoạt động:** Generator và Discriminator được huấn luyện cùng nhau trong một quá trình cạnh tranh. Generator cố gắng tạo ra hình ảnh ngày càng giống thật hơn để đánh lừa Discriminator.\n",
    "    - **Ví dụ:** DALL-E3 sử dụng các yếu tố từ kiến trúc GANs để tạo ra hình ảnh từ văn bản mô tả, dựa trên một quá trình cạnh tranh giữa các mạng.\n",
    "\n",
    "    **VAEs (Variational Autoencoders):**\n",
    "    - **Kiến trúc:** Gồm hai phần chính: Encoder và Decoder. Encoder chuyển đổi dữ liệu đầu vào thành một phân phối tiềm ẩn (latent distribution), sau đó Decoder tái tạo lại hình ảnh từ phân phối này.\n",
    "    - **Nguyên lý hoạt động:** VAEs tối ưu hóa để tái tạo lại dữ liệu đầu vào và học được biểu diễn tiềm ẩn của dữ liệu, giúp tạo ra hình ảnh mới từ phân phối tiềm ẩn này.\n",
    "    - **Ví dụ:** Stable Diffusion sử dụng VAEs để tạo ra hình ảnh, bắt đầu từ một biểu diễn tiềm ẩn và khuếch tán (diffuse) nó để tạo ra hình ảnh mới.\n",
    "\n",
    "**Ứng dụng:**\n",
    "- **DALL-E3:** Sử dụng các nguyên lý của GANs để tạo ra hình ảnh phức tạp từ văn bản.\n",
    "- **Stable Diffusion:** Sử dụng VAEs để tạo ra hình ảnh từ biểu diễn tiềm ẩn và khuếch tán nó để tạo ra hình ảnh chi tiết hơn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAEs) là một loại mô hình học sâu dùng để tạo ra dữ liệu mới, chẳng hạn như hình ảnh. Quá trình huấn luyện VAEs bao gồm các bước chính sau đây:\n",
    "\n",
    "### Quá trình Huấn luyện VAEs\n",
    "\n",
    "1. **Chuẩn bị dữ liệu:**\n",
    "   - Thu thập và chuẩn bị dữ liệu huấn luyện, chẳng hạn như một tập hợp lớn các hình ảnh.\n",
    "\n",
    "2. **Kiến trúc của VAE:**\n",
    "   - **Encoder (Bộ mã hóa):** Nhận đầu vào là dữ liệu (ví dụ: hình ảnh) và ánh xạ nó vào một không gian tiềm ẩn (latent space) có kích thước nhỏ hơn. Encoder tạo ra hai đầu ra chính: giá trị trung bình (mean) và độ lệch chuẩn (standard deviation) của phân phối tiềm ẩn.\n",
    "   - **Latent Space (Không gian tiềm ẩn):** Tạo ra biểu diễn tiềm ẩn bằng cách sử dụng giá trị trung bình và độ lệch chuẩn để lấy mẫu từ phân phối Gaussian.\n",
    "   - **Decoder (Bộ giải mã):** Nhận biểu diễn tiềm ẩn và tái tạo lại dữ liệu đầu vào từ không gian tiềm ẩn này.\n",
    "\n",
    "3. **Hàm mất mát (Loss Function):**\n",
    "   - Hàm mất mát của VAE gồm hai thành phần chính:\n",
    "     - **Reconstruction Loss (Mất mát tái tạo):** Đo lường sự khác biệt giữa dữ liệu gốc và dữ liệu được tái tạo từ Decoder. Thông thường, mất mát tái tạo sử dụng MSE (Mean Squared Error) hoặc BCE (Binary Cross-Entropy) tùy thuộc vào dạng dữ liệu.\n",
    "     - **KL Divergence (Kullback-Leibler Divergence):** Đo lường sự khác biệt giữa phân phối tiềm ẩn được tạo ra bởi Encoder và phân phối Gaussian chuẩn. Thành phần này giúp không gian tiềm ẩn trở nên liên tục và có cấu trúc, đảm bảo rằng các điểm gần nhau trong không gian tiềm ẩn sẽ tạo ra các dữ liệu tương tự.\n",
    "\n",
    "4. **Quá trình huấn luyện:**\n",
    "   - **Forward Pass:** Đầu vào được chuyển qua Encoder để tạo ra biểu diễn tiềm ẩn, sau đó biểu diễn này được chuyển qua Decoder để tái tạo lại đầu vào.\n",
    "   - **Loss Calculation:** Tính toán mất mát tổng cộng bằng cách kết hợp Reconstruction Loss và KL Divergence.\n",
    "   - **Backward Pass:** Sử dụng thuật toán backpropagation để cập nhật trọng số của Encoder và Decoder nhằm giảm thiểu hàm mất mát.\n",
    "\n",
    "5. **Cập nhật trọng số:**\n",
    "   - Sử dụng các kỹ thuật tối ưu hóa như SGD (Stochastic Gradient Descent) hoặc Adam để cập nhật trọng số của mạng nơ-ron dựa trên gradient được tính toán trong bước backward pass.\n",
    "\n",
    "### Ví dụ Minh Họa\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Định nghĩa Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mean, logvar\n",
    "\n",
    "# Định nghĩa Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc3(z))\n",
    "        x_reconstructed = torch.sigmoid(self.fc4(h))\n",
    "        return x_reconstructed\n",
    "\n",
    "# Định nghĩa VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mean + eps * std\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mean, logvar\n",
    "\n",
    "# Định nghĩa hàm mất mát\n",
    "def loss_function(x, x_reconstructed, mean, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Khởi tạo các tham số\n",
    "input_dim = 784 # 28x28 hình ảnh MNIST\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Chuẩn bị dữ liệu MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Khởi tạo mô hình và optimizer\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.view(-1, input_dim)\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, mean, logvar = model(data)\n",
    "        loss = loss_function(data, x_reconstructed, mean, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "```\n",
    "\n",
    "Mô hình này sử dụng các thành phần chính của VAE: Encoder, Decoder, và quá trình huấn luyện với hàm mất mát kết hợp Reconstruction Loss và KL Divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
