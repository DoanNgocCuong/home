

```python 
#!/usr/bin/env python3

# -*- coding: utf-8 -*-

"""

Parallel Processing Module for Main Pipeline

===========================================

  

Features:

- Adaptive batch sizing based on data complexity

- Smart worker management with performance monitoring

- Thread-safe result collection

- Progress tracking and ETA calculation

- Error handling and retry mechanisms

- Memory usage optimization

"""

  

import threading

import time

from concurrent.futures import ThreadPoolExecutor, as_completed

from datetime import datetime, timedelta

from typing import List, Dict, Any, Callable, Optional

import psutil

import math

  

class ParallelProcessor:

Â  Â  """

Â  Â  Advanced parallel processor with adaptive optimization

Â  Â  """

Â  Â  def __init__(self,

Â  Â  Â  Â  Â  Â  Â  Â  Â max_workers: Optional[int] = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â batch_size: Optional[int] = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â memory_limit_mb: int = 1024,

Â  Â  Â  Â  Â  Â  Â  Â  Â enable_adaptive: bool = True,

Â  Â  Â  Â  Â  Â  Â  Â  Â timeout_seconds: int = 300):

Â  Â  Â  Â  """

Â  Â  Â  Â  Initialize parallel processor

Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  max_workers: Maximum number of worker threads (None = auto-detect)

Â  Â  Â  Â  Â  Â  batch_size: Items per batch (None = auto-calculate)

Â  Â  Â  Â  Â  Â  memory_limit_mb: Memory limit in MB

Â  Â  Â  Â  Â  Â  enable_adaptive: Enable adaptive optimization

Â  Â  Â  Â  """

Â  Â  Â  Â  self.max_workers = max_workers or self._auto_detect_workers()

Â  Â  Â  Â  self.batch_size = batch_size

Â  Â  Â  Â  self.memory_limit_mb = memory_limit_mb

Â  Â  Â  Â  self.enable_adaptive = enable_adaptive

Â  Â  Â  Â  self.timeout_seconds = timeout_seconds

Â  Â  Â  Â  # Performance tracking

Â  Â  Â  Â  self.start_time = None

Â  Â  Â  Â  self.completed_items = 0

Â  Â  Â  Â  self.total_items = 0

Â  Â  Â  Â  self.error_count = 0

Â  Â  Â  Â  self.retry_count = 0

Â  Â  Â  Â  # Thread safety

Â  Â  Â  Â  self.lock = threading.Lock()

Â  Â  Â  Â  self.results = []

Â  Â  Â  Â  # Memory monitoring

Â  Â  Â  Â  self.process = psutil.Process()

Â  Â  def _auto_detect_workers(self) -> int:

Â  Â  Â  Â  """Auto-detect optimal number of workers based on system"""

Â  Â  Â  Â  cpu_count = psutil.cpu_count(logical=True)

Â  Â  Â  Â  memory_gb = psutil.virtual_memory().total / (1024**3)

Â  Â  Â  Â  # Conservative approach: don't overwhelm the system

Â  Â  Â  Â  if memory_gb < 4:

Â  Â  Â  Â  Â  Â  return min(2, cpu_count)

Â  Â  Â  Â  elif memory_gb < 8:

Â  Â  Â  Â  Â  Â  return min(4, cpu_count)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  return min(8, cpu_count)

Â  Â  def _calculate_optimal_batch_size(self, total_items: int, avg_time_per_item: float = None) -> int:

Â  Â  Â  Â  """Calculate optimal batch size based on data characteristics"""

Â  Â  Â  Â  if self.batch_size:

Â  Â  Â  Â  Â  Â  return self.batch_size

Â  Â  Â  Â  # Adaptive batch sizing

Â  Â  Â  Â  if self.enable_adaptive:

Â  Â  Â  Â  Â  Â  # Estimate based on available memory

Â  Â  Â  Â  Â  Â  available_memory_mb = psutil.virtual_memory().available / (1024**2)

Â  Â  Â  Â  Â  Â  estimated_memory_per_item = 50 Â # MB per item (conservative estimate)

Â  Â  Â  Â  Â  Â  memory_based_batch = max(1, int(available_memory_mb / estimated_memory_per_item))

Â  Â  Â  Â  Â  Â  # Time-based optimization

Â  Â  Â  Â  Â  Â  if avg_time_per_item:

Â  Â  Â  Â  Â  Â  Â  Â  # Aim for batches that take 30-60 seconds to process

Â  Â  Â  Â  Â  Â  Â  Â  target_batch_time = 45 Â # seconds

Â  Â  Â  Â  Â  Â  Â  Â  time_based_batch = max(1, int(target_batch_time / avg_time_per_item))

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  time_based_batch = 10

Â  Â  Â  Â  Â  Â  # Worker-based optimization

Â  Â  Â  Â  Â  Â  worker_based_batch = max(1, total_items // (self.max_workers * 2))

Â  Â  Â  Â  Â  Â  # Take the minimum to be conservative

Â  Â  Â  Â  Â  Â  optimal_batch = min(memory_based_batch, time_based_batch, worker_based_batch)

Â  Â  Â  Â  Â  Â  return max(1, min(optimal_batch, 50)) Â # Between 1 and 50

Â  Â  Â  Â  return 10 Â # Default fallback

Â  Â  def _create_batches(self, items: List[Any], batch_size: int) -> List[List[Any]]:

Â  Â  Â  Â  """Create batches from items list"""

Â  Â  Â  Â  return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]

Â  Â  def _monitor_memory(self) -> float:

Â  Â  Â  Â  """Monitor current memory usage"""

Â  Â  Â  Â  memory_info = self.process.memory_info()

Â  Â  Â  Â  memory_mb = memory_info.rss / (1024**2)

Â  Â  Â  Â  return memory_mb

Â  Â  def _should_pause_for_memory(self) -> bool:

Â  Â  Â  Â  """Check if we should pause for memory management"""

Â  Â  Â  Â  current_memory = self._monitor_memory()

Â  Â  Â  Â  return current_memory > self.memory_limit_mb

Â  Â  def process_parallel(self,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  items: List[Any],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  process_func: Callable,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_callback: Optional[Callable] = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **kwargs) -> List[Any]:

Â  Â  Â  Â  """

Â  Â  Â  Â  Process items in parallel with advanced optimization

Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  items: List of items to process

Â  Â  Â  Â  Â  Â  process_func: Function to process each batch

Â  Â  Â  Â  Â  Â  progress_callback: Optional callback for progress updates

Â  Â  Â  Â  Â  Â  **kwargs: Additional arguments for process_func

Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  List of results from all batches

Â  Â  Â  Â  """

Â  Â  Â  Â  self.total_items = len(items)

Â  Â  Â  Â  self.completed_items = 0

Â  Â  Â  Â  self.start_time = datetime.now()

Â  Â  Â  Â  self.results = []

Â  Â  Â  Â  if not items:

Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  # Calculate optimal batch size

Â  Â  Â  Â  batch_size = self._calculate_optimal_batch_size(len(items))

Â  Â  Â  Â  batches = self._create_batches(items, batch_size)

Â  Â  Â  Â  print(f"ðŸš€ Parallel Processing Setup:")

Â  Â  Â  Â  print(f" Â  ðŸ“Š Total items: {self.total_items}")

Â  Â  Â  Â  print(f" Â  ðŸ“¦ Batch size: {batch_size}")

Â  Â  Â  Â  print(f" Â  ðŸ§µ Max workers: {self.max_workers}")

Â  Â  Â  Â  print(f" Â  ðŸ“‹ Total batches: {len(batches)}")

Â  Â  Â  Â  print(f" Â  ðŸ’¾ Memory limit: {self.memory_limit_mb}MB")

Â  Â  Â  Â  print(f" Â  ðŸ”§ Adaptive mode: {'âœ…' if self.enable_adaptive else 'âŒ'}")

Â  Â  Â  Â  # Process batches in parallel

Â  Â  Â  Â  with ThreadPoolExecutor(max_workers=self.max_workers) as executor:

Â  Â  Â  Â  Â  Â  # Submit all batches - filter out timeout_seconds from kwargs

Â  Â  Â  Â  Â  Â  process_kwargs = {k: v for k, v in kwargs.items() if k != 'timeout_seconds'}

Â  Â  Â  Â  Â  Â  future_to_batch = {

Â  Â  Â  Â  Â  Â  Â  Â  executor.submit(self._process_batch_with_retry, batch, process_func, **process_kwargs): batch

Â  Â  Â  Â  Â  Â  Â  Â  for batch in batches

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  # Process completed batches with timeout

Â  Â  Â  Â  Â  Â  for future in as_completed(future_to_batch):

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add timeout to prevent hanging

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_result = future.result(timeout=self.timeout_seconds)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with self.lock:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.results.extend(batch_result)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.completed_items += len(batch_result)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Progress update

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if progress_callback:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_callback(self.completed_items, self.total_items)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Memory management

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if self._should_pause_for_memory():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â High memory usage ({self._monitor_memory():.1f}MB), pausing...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(1) Â # Brief pause

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Performance monitoring

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self._log_progress()

Â  Â  Â  Â  Â  Â  Â  Â  except TimeoutError:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.error_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"â° Batch processing timeout after {self.timeout_seconds}s, skipping...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.error_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âŒ Batch processing error: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  # Final summary

Â  Â  Â  Â  self._log_final_summary()

Â  Â  Â  Â  return self.results

Â  Â  def _process_batch_with_retry(self,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â batch: List[Any],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â process_func: Callable,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â max_retries: int = 2,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â **kwargs) -> List[Any]:

Â  Â  Â  Â  """Process a batch with retry mechanism"""

Â  Â  Â  Â  for attempt in range(max_retries + 1):

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  return process_func(batch, **kwargs)

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  if attempt < max_retries:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.retry_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"ðŸ”„ Retry {attempt + 1}/{max_retries} for batch due to: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(2 ** attempt) Â # Exponential backoff

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"ðŸ’¥ Batch failed after {max_retries} retries: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return [] Â # Return empty result for failed batch

Â  Â  def _log_progress(self):

Â  Â  Â  Â  """Log current progress with ETA"""

Â  Â  Â  Â  if self.completed_items == 0:

Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  elapsed_time = (datetime.now() - self.start_time).total_seconds()

Â  Â  Â  Â  avg_time_per_item = elapsed_time / self.completed_items

Â  Â  Â  Â  remaining_items = self.total_items - self.completed_items

Â  Â  Â  Â  estimated_remaining_time = avg_time_per_item * remaining_items

Â  Â  Â  Â  progress_percent = (self.completed_items / self.total_items) * 100

Â  Â  Â  Â  eta = datetime.now() + timedelta(seconds=estimated_remaining_time)

Â  Â  Â  Â  memory_usage = self._monitor_memory()

Â  Â  Â  Â  print(f"ðŸ“Š Progress: {self.completed_items}/{self.total_items} ({progress_percent:.1f}%)")

Â  Â  Â  Â  print(f" Â  â±ï¸ Â Elapsed: {elapsed_time/60:.1f}min | ETA: {eta.strftime('%H:%M:%S')}")

Â  Â  Â  Â  print(f" Â  ðŸš€ Speed: {avg_time_per_item:.1f}s/item | ðŸ’¾ Memory: {memory_usage:.1f}MB")

Â  Â  Â  Â  print(f" Â  âŒ Errors: {self.error_count} | ðŸ”„ Retries: {self.retry_count}")

Â  Â  def _log_final_summary(self):

Â  Â  Â  Â  """Log final processing summary"""

Â  Â  Â  Â  total_time = (datetime.now() - self.start_time).total_seconds()

Â  Â  Â  Â  success_rate = ((self.total_items - self.error_count) / self.total_items) * 100

Â  Â  Â  Â  print(f"\nðŸŽ‰ Parallel Processing Complete!")

Â  Â  Â  Â  print(f" Â  â±ï¸ Â Total time: {total_time/60:.1f} minutes")

Â  Â  Â  Â  print(f" Â  ðŸ“Š Success rate: {success_rate:.1f}%")

Â  Â  Â  Â  print(f" Â  âŒ Errors: {self.error_count}")

Â  Â  Â  Â  print(f" Â  ðŸ”„ Retries: {self.retry_count}")

Â  Â  Â  Â  print(f" Â  ðŸ’¾ Peak memory: {self._monitor_memory():.1f}MB")

Â  Â  Â  Â  if self.total_items > 0:

Â  Â  Â  Â  Â  Â  throughput = self.total_items / total_time

Â  Â  Â  Â  Â  Â  print(f" Â  ðŸš€ Throughput: {throughput:.2f} items/second")

  
  

# Convenience functions for easy integration

def process_situations_parallel(situations: List[Dict],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â process_func: Callable,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â max_workers: int = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â batch_size: int = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â timeout_seconds: int = 300,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â **kwargs) -> List[Dict]:

Â  Â  """

Â  Â  Convenience function to process situations in parallel

Â  Â  Args:

Â  Â  Â  Â  situations: List of situation dictionaries

Â  Â  Â  Â  process_func: Function to process each situation

Â  Â  Â  Â  max_workers: Maximum worker threads

Â  Â  Â  Â  batch_size: Items per batch

Â  Â  Â  Â  **kwargs: Additional arguments for process_func

Â  Â  Returns:

Â  Â  Â  Â  List of processed results

Â  Â  """

Â  Â  processor = ParallelProcessor(

Â  Â  Â  Â  max_workers=max_workers,

Â  Â  Â  Â  batch_size=batch_size,

Â  Â  Â  Â  enable_adaptive=True

Â  Â  )

Â  Â  def progress_callback(completed, total):

Â  Â  Â  Â  if completed % 10 == 0: Â # Update every 10 items

Â  Â  Â  Â  Â  Â  print(f"ðŸ”„ Processed {completed}/{total} situations...")

Â  Â  return processor.process_parallel(

Â  Â  Â  Â  items=situations,

Â  Â  Â  Â  process_func=process_func,

Â  Â  Â  Â  progress_callback=progress_callback,

Â  Â  Â  Â  timeout_seconds=timeout_seconds,

Â  Â  Â  Â  **kwargs

Â  Â  )

  
  

if __name__ == "__main__":

Â  Â  # Demo usage

Â  Â  def demo_process_func(batch, **kwargs):

Â  Â  Â  Â  """Demo processing function"""

Â  Â  Â  Â  time.sleep(0.1) Â # Simulate work

Â  Â  Â  Â  return [f"Processed: {item}" for item in batch]

Â  Â  # Test data

Â  Â  test_items = list(range(100))

Â  Â  print("ðŸ§ª Testing Parallel Processor...")

Â  Â  processor = ParallelProcessor(max_workers=4, batch_size=10)

Â  Â  results = processor.process_parallel(test_items, demo_process_func)

Â  Â  print(f"âœ… Demo completed with {len(results)} results")
Â  Â  
```


