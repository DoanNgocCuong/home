
![[Pasted image 20241214232612.png]]

### Gi·∫£i th√≠ch chi ti·∫øt file script

ƒê√¢y l√† m·ªôt script ch·∫°y c√°c th√†nh ph·∫ßn c·ªßa h·ªá th·ªëng **Big Data**, g·ªìm hai ph·∫ßn ch√≠nh:
1. **Stream Layer (X·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c).**
2. **Batch Layer (X·ª≠ l√Ω d·ªØ li·ªáu theo l√¥).**
		- Hadoop l∆∞u tr·ªØ  HDFS v√† x·ª≠ l√Ω d·ªØ li·ªáu Map reduces

---

### **Chi ti·∫øt t·ª´ng ph·∫ßn**

#### **1. Stream Layer**
Ph·∫ßn n√†y kh·ªüi ƒë·ªông v√† thi·∫øt l·∫≠p c√°c c√¥ng c·ª• ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c.

| **L·ªánh**                                                                                           | **Ch·ª©c nƒÉng**                                                                                                                                 |
|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| `zookeeper-server-start.bat C:/kafka_2.13_2.6.0/config/zookeeper.properties`                       | Kh·ªüi ƒë·ªông **Zookeeper**, m·ªôt d·ªãch v·ª• h·ªó tr·ª£ qu·∫£n l√Ω c√°c cluster Kafka.                                                                      |
| `kafka-server-start.bat C:/kafka_2.13_2.6.0/config/server.properties`                              | Kh·ªüi ƒë·ªông **Kafka server**, d·ªãch v·ª• x·ª≠ l√Ω d·ªØ li·ªáu streaming (lu·ªìng d·ªØ li·ªáu).                                                                |
| `kafka-topics.bat --create --topic smartphoneTopic --bootstrap-server localhost:9092`              | T·∫°o m·ªôt **topic** trong Kafka t√™n l√† `smartphoneTopic`.                                                                                     |
| `kafka-console-producer.bat --topic smartphoneTopic --bootstrap-server localhost:9092`             | Kh·ªüi ch·∫°y **producer** ƒë·ªÉ g·ª≠i d·ªØ li·ªáu v√†o topic `smartphoneTopic`.                                                                          |
| `kafka-console-consumer.bat --topic smartphoneTopic --from-beginning --bootstrap-server localhost:9092` | Kh·ªüi ch·∫°y **consumer** ƒë·ªÉ nh·∫≠n v√† x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ topic `smartphoneTopic`.                                                                 |
| `start-all`                                                                                        | Kh·ªüi ƒë·ªông HDFS (Hadoop Distributed File System) v√† YARN (c√¥ng c·ª• qu·∫£n l√Ω t√†i nguy√™n).                                                       |
| `hbase shell`                                                                                      | M·ªü **HBase shell** ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi HBase, c∆° s·ªü d·ªØ li·ªáu NoSQL d√πng l∆∞u tr·ªØ d·ªØ li·ªáu th·ªùi gian th·ª±c.                                         |
| `hbase thrift start`                                                                               | Kh·ªüi ch·∫°y **Thrift server**, cho ph√©p c√°c ·ª©ng d·ª•ng b√™n ngo√†i k·∫øt n·ªëi v·ªõi HBase th√¥ng qua API.                                               |

---

#### **2. Batch Layer**
Ph·∫ßn n√†y d√πng ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu theo l√¥, th∆∞·ªùng √°p d·ª•ng cho c√°c ph√¢n t√≠ch l·ªõn v√† l∆∞u tr·ªØ d·ªØ li·ªáu l·ªãch s·ª≠.

| **L·ªánh**                                                                                           | **Ch·ª©c nƒÉng**                                                                                                                                 |
|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| `docker-compose up -d`                                                                             | Kh·ªüi ch·∫°y Apache **Airflow** (c√¥ng c·ª• l·∫≠p l·ªãch tr√¨nh v√† ƒëi·ªÅu ph·ªëi c√°c lu·ªìng c√¥ng vi·ªác) b·∫±ng Docker.                                          |
| `spark-shell`                                                                                      | M·ªü **Spark Shell** ƒë·ªÉ th·ª±c hi·ªán c√°c t√°c v·ª• x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn theo l√¥.                                                                        |
| `zookeeper-server-start.bat C:/kafka_2.13_2.6.0/config/zookeeper.properties`                       | (T∆∞∆°ng t·ª± Stream Layer) Kh·ªüi ƒë·ªông **Zookeeper** ƒë·ªÉ qu·∫£n l√Ω Kafka cluster.                                                                   |
| `kafka-server-start.bat C:/kafka_2.13_2.6.0/config/server.properties`                              | (T∆∞∆°ng t·ª± Stream Layer) Kh·ªüi ƒë·ªông Kafka server ƒë·ªÉ x·ª≠ l√Ω lu·ªìng d·ªØ li·ªáu.                                                                      |
| `kafka-console-producer.bat --topic smartphoneTopic --bootstrap-server localhost:9092`             | (T∆∞∆°ng t·ª± Stream Layer) Kh·ªüi ch·∫°y producer ƒë·ªÉ g·ª≠i d·ªØ li·ªáu v√†o Kafka.                                                                        |
| `kafka-console-consumer.bat --topic smartphoneTopic --from-beginning --bootstrap-server localhost:9092` | (T∆∞∆°ng t·ª± Stream Layer) Kh·ªüi ch·∫°y consumer ƒë·ªÉ nh·∫≠n d·ªØ li·ªáu t·ª´ Kafka.                                                                        |
| `start-all`                                                                                        | Kh·ªüi ƒë·ªông HDFS v√† YARN (gi·ªëng Stream Layer).                                                                                                |

---

### **T√≥m t·∫Øt c√°c th√†nh ph·∫ßn**

| **Th√†nh ph·∫ßn**      | **Vai tr√≤**                                                                                                                                                          |
|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Zookeeper**       | Qu·∫£n l√Ω tr·∫°ng th√°i Kafka Cluster v√† ƒëi·ªÅu ph·ªëi c√°c node trong cluster.                                                                                              |
| **Kafka**           | H·ªá th·ªëng x·ª≠ l√Ω d·ªØ li·ªáu lu·ªìng, d√πng ƒë·ªÉ l∆∞u tr·ªØ v√† truy·ªÅn d·ªØ li·ªáu gi·ªØa c√°c h·ªá th·ªëng kh√°c nhau.                                                                         |
| **HDFS**            | H·ªá th·ªëng l∆∞u tr·ªØ ph√¢n t√°n, l∆∞u d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω trong Batch Layer.                                                                                                 |
| **YARN**            | Qu·∫£n l√Ω t√†i nguy√™n trong cluster Hadoop (qu·∫£n l√Ω CPU, RAM cho c√°c t√°c v·ª•).                                                                                         |
| **HBase**           | C∆° s·ªü d·ªØ li·ªáu NoSQL l∆∞u tr·ªØ d·ªØ li·ªáu th·ªùi gian th·ª±c (real-time view).                                                                                               |
| **Thrift Server**   | D·ªãch v·ª• giao ti·∫øp v·ªõi HBase qua c√°c giao th·ª©c nh∆∞ HTTP ho·∫∑c RPC.                                                                                                   |
| **Airflow**         | ƒêi·ªÅu ph·ªëi c√°c t√°c v·ª• (v√≠ d·ª•: x·ª≠ l√Ω theo l√¥, ETL).                                                                                                                   |
| **Spark**           | C√¥ng c·ª• x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn, d√πng ƒë·ªÉ t√≠nh to√°n v√† ph√¢n t√≠ch d·ªØ li·ªáu trong Batch Layer.                                                                                |

---

### **C√°ch ho·∫°t ƒë·ªông t·ªïng quan**
1. **Stream Layer**:
   - Kafka nh·∫≠n v√† x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c (d·ªØ li·ªáu smartphone).
   - D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u t·∫°m trong Kafka, r·ªìi ghi v√†o HBase (l∆∞u tr·ªØ real-time view).

2. **Batch Layer**:
   - D·ªØ li·ªáu t·ª´ Kafka ho·∫∑c HDFS ƒë∆∞·ª£c x·ª≠ l√Ω theo l√¥ b·∫±ng Spark.
   - Airflow qu·∫£n l√Ω c√°c quy tr√¨nh ETL v√† chuy·ªÉn k·∫øt qu·∫£ v√†o c∆° s·ªü d·ªØ li·ªáu kh√°c ƒë·ªÉ ph√¢n t√≠ch.

---

### **L∆∞u √Ω**
- **ƒê∆∞·ªùng d·∫´n c·ª• th·ªÉ** (`C:/kafka_2.13_2.6.0`) ph·∫£i ƒë∆∞·ª£c thay ƒë·ªïi t√πy v√†o h·ªá th·ªëng c·ªßa b·∫°n.
- **Docker Compose**: Airflow ch·∫°y trong container, v√¨ v·∫≠y c·∫ßn Docker ƒë∆∞·ª£c c√†i ƒë·∫∑t tr∆∞·ªõc.

D∆∞·ªõi ƒë√¢y l√† gi·∫£i th√≠ch **c·ª±c k·ª≥ ƒë∆°n gi·∫£n** v·ªÅ c√°c c√¥ng c·ª• trong h·ªá th·ªëng Big Data, ƒë∆∞·ª£c tr√¨nh b√†y d∆∞·ªõi d·∫°ng b·∫£ng ƒë·ªÉ d·ªÖ hi·ªÉu:

---

### **1. Apache Kafka**

|**C√¥ng c·ª•**|**Apache Kafka**|
|---|---|
|**Ch·ª©c nƒÉng ch√≠nh**|Chuy·ªÉn d·ªØ li·ªáu gi·ªØa c√°c h·ªá th·ªëng theo th·ªùi gian th·ª±c (gi·ªëng nh∆∞ b∆∞u ƒëi·ªán chuy·ªÉn th∆∞).|
|**Th√†nh ph·∫ßn**|- **Producer**: G·ª≠i d·ªØ li·ªáu v√†o h·ªá th·ªëng.- **Consumer**: L·∫•y d·ªØ li·ªáu ra t·ª´ h·ªá th·ªëng.- **Topic**: "H·ªôp th∆∞" ch·ª©a d·ªØ li·ªáu.|
|**V√≠ d·ª• d·ªÖ hi·ªÉu**|Khi b·∫°n nh·∫Øn tin qua Messenger, Kafka gi·ªëng nh∆∞ h·ªá th·ªëng chuy·ªÉn tin nh·∫Øn t·ª´ ng∆∞·ªùi g·ª≠i ƒë·∫øn ng∆∞·ªùi nh·∫≠n.|
|**Vai tr√≤**|- L∆∞u v√† chuy·ªÉn d·ªØ li·ªáu.- X·ª≠ l√Ω d·ªØ li·ªáu lu·ªìng (streaming).|

---

### **2. Zookeeper**

|**C√¥ng c·ª•**|**Apache Zookeeper**|
|---|---|
|**Ch·ª©c nƒÉng ch√≠nh**|Qu·∫£n l√Ω c√°c d·ªãch v·ª• trong h·ªá th·ªëng ph√¢n t√°n (gi·ªëng nh∆∞ "qu·∫£n gia" c·ªßa Kafka).|
|**Vai tr√≤**|- Qu·∫£n l√Ω th√¥ng tin c·ªßa Kafka Cluster.- ƒê·∫£m b·∫£o c√°c node (m√°y t√≠nh) trong h·ªá th·ªëng ph·ªëi h·ª£p nh·ªãp nh√†ng.|
|**V√≠ d·ª• d·ªÖ hi·ªÉu**|Gi·ªëng nh∆∞ "ƒëi·ªÅu ph·ªëi vi√™n" c·ªßa m·ªôt ƒë·ªôi b√≥ng ƒë√°, gi√∫p c√°c c·∫ßu th·ªß ph·ªëi h·ª£p ƒÉn √Ω.|

---

### **3. HDFS (Hadoop Distributed File System)**

|**C√¥ng c·ª•**|**HDFS**|
|---|---|
|**Ch·ª©c nƒÉng ch√≠nh**|L∆∞u tr·ªØ d·ªØ li·ªáu l·ªõn tr√™n nhi·ªÅu m√°y t√≠nh (gi·ªëng nh∆∞ m·ªôt ·ªï c·ª©ng kh·ªïng l·ªì).|
|**C√°ch ho·∫°t ƒë·ªông**|- D·ªØ li·ªáu ƒë∆∞·ª£c chia nh·ªè th√†nh c√°c m·∫£nh nh·ªè (blocks).- C√°c m·∫£nh nh·ªè n√†y ƒë∆∞·ª£c l∆∞u tr√™n nhi·ªÅu m√°y.|
|**V√≠ d·ª• d·ªÖ hi·ªÉu**|Gi·ªëng nh∆∞ khi b·∫°n l∆∞u m·ªôt b·ªô phim d√†i tr√™n nhi·ªÅu USB kh√°c nhau ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng.|
|**Vai tr√≤**|- L∆∞u tr·ªØ d·ªØ li·ªáu l·ªõn nh∆∞ file log, video, d·ªØ li·ªáu c·∫£m bi·∫øn,...|

---

### **4. YARN (Yet Another Resource Negotiator)**

|**C√¥ng c·ª•**|**YARN**|
|---|---|
|**Ch·ª©c nƒÉng ch√≠nh**|Qu·∫£n l√Ω t√†i nguy√™n (CPU, RAM) cho c√°c t√°c v·ª• (gi·ªëng nh∆∞ "qu·∫£n l√Ω t√†i nguy√™n m√°y ch·ªß").|
|**C√°ch ho·∫°t ƒë·ªông**|- Khi m·ªôt ·ª©ng d·ª•ng c·∫ßn t√†i nguy√™n, YARN s·∫Ω c·∫•p ph√°t t√†i nguy√™n ƒë√≥.- ƒê·∫£m b·∫£o m·ªçi t√°c v·ª• ƒë·ªÅu ch·∫°y ·ªïn ƒë·ªãnh.|
|**V√≠ d·ª• d·ªÖ hi·ªÉu**|Gi·ªëng nh∆∞ qu·∫£n l√Ω nh√† b·∫øp trong nh√† h√†ng, ph√¢n chia b·∫øp v√† nguy√™n li·ªáu cho t·ª´ng m√≥n ƒÉn.|

---

### **5. HBase**

|**C√¥ng c·ª•**|**HBase**|
|---|---|
|**Ch·ª©c nƒÉng ch√≠nh**|C∆° s·ªü d·ªØ li·ªáu NoSQL, l∆∞u tr·ªØ v√† truy v·∫•n d·ªØ li·ªáu l·ªõn theo th·ªùi gian th·ª±c.|
|**C√°ch ho·∫°t ƒë·ªông**|- L∆∞u d·ªØ li·ªáu d∆∞·ªõi d·∫°ng b·∫£ng, nh∆∞ng h·ªó tr·ª£ l∆∞u tr·ªØ v√† truy v·∫•n nhanh ch√≥ng.- T√≠ch h·ª£p t·ªët v·ªõi HDFS.|
|**V√≠ d·ª• d·ªÖ hi·ªÉu**|Gi·ªëng nh∆∞ m·ªôt b·∫£ng t√≠nh Excel kh·ªïng l·ªì, c√≥ th·ªÉ ghi v√† ƒë·ªçc d·ªØ li·ªáu c·ª±c nhanh.|
|**Vai tr√≤**|L∆∞u tr·ªØ d·ªØ li·ªáu th·ªùi gian th·ª±c nh∆∞ gi√° c·ªï phi·∫øu, c·∫£m bi·∫øn IoT,...|

---

### **6. Apache Spark**

|**C√¥ng c·ª•**|**Apache Spark**|
|---|---|
|**Ch·ª©c nƒÉng ch√≠nh**|X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn c·ª±c nhanh, h·ªó tr·ª£ c·∫£ th·ªùi gian th·ª±c v√† theo l√¥ (batch processing).|
|**Th√†nh ph·∫ßn ch√≠nh**|- Spark Core: X·ª≠ l√Ω d·ªØ li·ªáu.- Spark SQL: Truy v·∫•n d·ªØ li·ªáu d·∫°ng SQL.- Spark Streaming: X·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c.|
|**V√≠ d·ª• d·ªÖ hi·ªÉu**|Gi·ªëng nh∆∞ m·ªôt ƒë·∫ßu b·∫øp t·ªëc ƒë·ªô cao, c√≥ th·ªÉ n·∫•u nhi·ªÅu m√≥n ƒÉn (d·ªØ li·ªáu) trong th·ªùi gian ng·∫Øn.|
|**Vai tr√≤**|- Ph√¢n t√≠ch d·ªØ li·ªáu l·ªõn.- T√≠nh to√°n nhanh ch√≥ng.|

---

### **7. Apache Airflow**

|**C√¥ng c·ª•**|**Apache Airflow**|
|---|---|
|**Ch·ª©c nƒÉng ch√≠nh**|Qu·∫£n l√Ω v√† t·ª± ƒë·ªông h√≥a c√°c lu·ªìng c√¥ng vi·ªác (workflow).|
|**C√°ch ho·∫°t ƒë·ªông**|- B·∫°n ƒë·ªãnh nghƒ©a c√°c t√°c v·ª• (tasks) v√† th·ª© t·ª± ch·∫°y c·ªßa ch√∫ng.- Airflow s·∫Ω t·ª± ƒë·ªông ch·∫°y c√°c t√°c v·ª• n√†y theo l·ªãch tr√¨nh.|
|**V√≠ d·ª• d·ªÖ hi·ªÉu**|Gi·ªëng nh∆∞ m·ªôt ng∆∞·ªùi qu·∫£n l√Ω d·ª± √°n, s·∫Øp x·∫øp c√¥ng vi·ªác cho t·ª´ng th√†nh vi√™n v√† ƒë·∫£m b·∫£o m·ªçi th·ª© ƒë√∫ng h·∫°n.|
|**Vai tr√≤**|- L·∫≠p l·ªãch v√† ƒëi·ªÅu ph·ªëi c√°c t√°c v·ª• ETL (Extract, Transform, Load).|

---

### **T√≥m t·∫Øt b·∫£ng t·ªïng quan**

|**C√¥ng c·ª•**|**Vai tr√≤ ch√≠nh**|**V√≠ d·ª• d·ªÖ hi·ªÉu**|
|---|---|---|
|Kafka|Chuy·ªÉn d·ªØ li·ªáu gi·ªØa c√°c h·ªá th·ªëng theo th·ªùi gian th·ª±c.|Chuy·ªÉn tin nh·∫Øn Messenger gi·ªØa ng∆∞·ªùi g·ª≠i v√† ng∆∞·ªùi nh·∫≠n.|
|Zookeeper|ƒêi·ªÅu ph·ªëi v√† qu·∫£n l√Ω Kafka cluster.|Qu·∫£n l√Ω ƒë·ªôi b√≥ng ƒë√° ƒë·ªÉ c√°c c·∫ßu th·ªß ph·ªëi h·ª£p hi·ªáu qu·∫£.|
|HDFS|L∆∞u tr·ªØ d·ªØ li·ªáu l·ªõn tr√™n nhi·ªÅu m√°y.|L∆∞u file phim l·ªõn tr√™n nhi·ªÅu USB kh√°c nhau.|
|YARN|Qu·∫£n l√Ω t√†i nguy√™n (CPU, RAM) cho c√°c ·ª©ng d·ª•ng.|Qu·∫£n l√Ω nh√† b·∫øp, ph√¢n chia b·∫øp cho t·ª´ng m√≥n ƒÉn.|
|HBase|L∆∞u tr·ªØ v√† truy v·∫•n d·ªØ li·ªáu l·ªõn theo th·ªùi gian th·ª±c.|M·ªôt b·∫£ng t√≠nh Excel kh·ªïng l·ªì, truy v·∫•n si√™u nhanh.|
|Spark|X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn nhanh ch√≥ng, h·ªó tr·ª£ th·ªùi gian th·ª±c v√† theo l√¥.|ƒê·∫ßu b·∫øp t·ªëc ƒë·ªô cao n·∫•u nhi·ªÅu m√≥n c√πng l√∫c.|
|Airflow|Qu·∫£n l√Ω v√† t·ª± ƒë·ªông h√≥a c√°c lu·ªìng c√¥ng vi·ªác.|Qu·∫£n l√Ω d·ª± √°n, s·∫Øp x·∫øp v√† t·ª± ƒë·ªông h√≥a c√¥ng vi·ªác.|

---

N·∫øu b·∫°n c·∫ßn th√™m th√¥ng tin ho·∫∑c gi·∫£i th√≠ch c·ª• th·ªÉ v·ªÅ c√°ch s·ª≠ d·ª•ng, m√¨nh s·∫µn s√†ng h·ªó tr·ª£! üòä


D∆∞·ªõi ƒë√¢y l√† b·∫£ng t·ªïng h·ª£p th√¥ng tin c√°c **port** trong HDFS v√† ch·ª©c nƒÉng c·ªßa ch√∫ng:

|**C√¥ng c·ª•/Th√†nh ph·∫ßn**|**Port**|**Ch·ª©c nƒÉng**|
|---|---|---|
|**NameNode RPC**|`8020`|Giao ti·∫øp ch√≠nh gi·ªØa NameNode v√† client ho·∫∑c c√°c DataNode.|
|**NameNode Web Interface**|`9870`|Giao di·ªán qu·∫£n tr·ªã web c·ªßa NameNode (truy c·∫≠p th√¥ng tin cluster qua tr√¨nh duy·ªát).|
|**DataNode IPC**|`50020`|Giao ti·∫øp ch√≠nh gi·ªØa DataNode v√† NameNode.|
|**DataNode Web Interface**|`9864`|Giao di·ªán qu·∫£n tr·ªã web c·ªßa DataNode (th√¥ng tin block, dung l∆∞·ª£ng).|
|**Secondary NameNode Web**|`9868`|Giao di·ªán web c·ªßa Secondary NameNode (n·∫øu c√≥).|
|**YARN ResourceManager RPC**|`8032`|Giao ti·∫øp gi·ªØa ResourceManager v√† c√°c NodeManager ho·∫∑c client (trong YARN).|
|**YARN Web Interface**|`8088`|Giao di·ªán web qu·∫£n tr·ªã ResourceManager.|
|**NodeManager Web Interface**|`8042`|Giao di·ªán web c·ªßa NodeManager (trong YARN).|

### L∆∞u √Ω:

1. C√°c port tr√™n c√≥ th·ªÉ ƒë∆∞·ª£c thay ƒë·ªïi trong file c·∫•u h√¨nh c·ªßa Hadoop (`core-site.xml`, `hdfs-site.xml`, `yarn-site.xml`).
2. M·∫∑c ƒë·ªãnh, t·∫•t c·∫£ giao di·ªán web v√† giao ti·∫øp RPC ƒë·ªÅu ho·∫°t ƒë·ªông tr√™n **HTTP** ho·∫∑c **HTTPS** n·∫øu ƒë∆∞·ª£c b·∫≠t.
3. ƒê·∫£m b·∫£o c√°c port ƒë∆∞·ª£c m·ªü trong t∆∞·ªùng l·ª≠a ƒë·ªÉ c√°c th√†nh ph·∫ßn c·ªßa cluster c√≥ th·ªÉ k·∫øt n·ªëi ƒë∆∞·ª£c.

N·∫øu c·∫ßn th√™m th√¥ng tin chi ti·∫øt v·ªÅ c√°ch c·∫•u h√¨nh, h√£y cho m√¨nh bi·∫øt nh√©!


---
#### **Stream Layer**

[](https://github.com/DoanNgocCuong/Big-Data-Project_2#1-stream-layer)

- Start Apache zookeeper

```batchfile
zookeeper-server-start.bat C:/kafka_2.13_2.6.0/config/zookeeper.properties
```

- Start Kafka server

```batchfile
kafka-server-start.bat C:/kafka_2.13_2.6.0/config/server.properties
```

- Create Kafka topic

```batchfile
kafka-topics.bat --create --topic smartphoneTopic --bootstrap-server localhost:9092
```

- Run the kafka producer

```batchfile
kafka-console-producer.bat --topic smartphoneTopic --bootstrap-server localhost:9092
```

- Run the kafka consumer

```batchfile
kafka-console-consumer.bat --topic smartphoneTopic --from-beginning --bootstrap-server localhost:9092
```

- Start HDFS and yarn (start-all or start-dfs and start-yarn)

```batchfile
start-all  
```

- Start Hbase

```batchfile
start-hbase  
```

- Run thrift server (for Hbase)

```batchfile
hbase thrift start
```

after all this run¬†`stream_pipeline.py`¬†script.

and then open the spring boot appliation in your idea and run it (you can access to the web app locally on¬†`localhost:8081/`)


### **B·∫£ng t√≥m t·∫Øt c√°c b∆∞·ªõc ch·∫°y Stream Layer**

|**B∆∞·ªõc**|**L·ªánh th·ª±c thi**|**M√¥ t·∫£**|
|---|---|---|
|**1. Start Apache Zookeeper**|`zookeeper-server-start.bat C:/kafka_2.13_2.6.0/config/zookeeper.properties`|Kh·ªüi ch·∫°y Zookeeper ƒë·ªÉ qu·∫£n l√Ω Kafka.|
|**2. Start Kafka Server**|`kafka-server-start.bat C:/kafka_2.13_2.6.0/config/server.properties`|Kh·ªüi ch·∫°y Kafka Broker ƒë·ªÉ x·ª≠ l√Ω message.|
|**3. Create Kafka Topic**|`kafka-topics.bat --create --topic smartphoneTopic --bootstrap-server localhost:9092`|T·∫°o m·ªôt topic Kafka t√™n l√† `smartphoneTopic` ƒë·ªÉ l∆∞u tr·ªØ message.|
|**4. Run Kafka Producer**|`kafka-console-producer.bat --topic smartphoneTopic --bootstrap-server localhost:9092`|Ch·∫°y producer ƒë·ªÉ g·ª≠i message t·ªõi topic `smartphoneTopic`.|
|**5. Run Kafka Consumer**|`kafka-console-consumer.bat --topic smartphoneTopic --from-beginning --bootstrap-server localhost:9092`|Ch·∫°y consumer ƒë·ªÉ nh·∫≠n message t·ª´ topic `smartphoneTopic`.|
|**6. Start HDFS and YARN**|`start-all`|Kh·ªüi ƒë·ªông HDFS v√† YARN ƒë·ªÉ qu·∫£n l√Ω l∆∞u tr·ªØ ph√¢n t√°n v√† x·ª≠ l√Ω d·ªØ li·ªáu.|
|**7. Start HBase**|`start-hbase`|Kh·ªüi ƒë·ªông HBase ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu phi c·∫•u tr√∫c.|
|**8. Run Thrift Server**|`hbase thrift start`|Ch·∫°y Thrift Server ƒë·ªÉ cung c·∫•p giao di·ªán REST ho·∫∑c giao ti·∫øp qua RPC cho HBase.|
|**9. Run Python Pipeline**|`python stream_pipeline.py`|Ch·∫°y script `stream_pipeline.py` ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu stream t·ª´ Kafka v√† l∆∞u tr·ªØ v√†o HDFS ho·∫∑c HBase.|
|**10. Start Spring Boot App**|Ch·∫°y Spring Boot App trong IDE. Truy c·∫≠p t·∫°i `http://localhost:8081/`.|Kh·ªüi ch·∫°y ·ª©ng d·ª•ng Spring Boot ƒë·ªÉ hi·ªÉn th·ªã dashboard v√† x·ª≠ l√Ω y√™u c·∫ßu t·ª´ ng∆∞·ªùi d√πng.|

### **Ghi ch√∫**

- ƒê·∫£m b·∫£o c√°c c√¥ng c·ª• nh∆∞ Kafka, Zookeeper, HBase, HDFS ƒë∆∞·ª£c c√†i ƒë·∫∑t v√† c·∫•u h√¨nh ƒë√∫ng.
- Ki·ªÉm tra log n·∫øu g·∫∑p l·ªói khi ch·∫°y t·ª´ng b∆∞·ªõc.
D∆∞·ªõi ƒë√¢y l√† danh s√°ch c√°c **port** ƒë·ªÉ ki·ªÉm tra giao di·ªán ng∆∞·ªùi d√πng (UI) c·ªßa c√°c th√†nh ph·∫ßn:

| **Th√†nh ph·∫ßn**           | **Port**        | **URL ƒë·ªÉ truy c·∫≠p UI**           | **M√¥ t·∫£ giao di·ªán**                                                                    |
| ------------------------ | --------------- | -------------------------------- | -------------------------------------------------------------------------------------- |
| **Kafka Control Center** | 9021            | `http://localhost:9021`          | Qu·∫£n l√Ω Kafka, gi√°m s√°t topic, producer v√† consumer.                                   |
| **HDFS (NameNode)**      | 9870            | `http://localhost:9870`          | Theo d√µi tr·∫°ng th√°i c·ªßa HDFS NameNode.                                                 |
| **HBase Thrift Server**  | 9095            | C·∫ßn c·∫•u h√¨nh th√™m n·∫øu mu·ªën UI    | Giao di·ªán qu·∫£n l√Ω HBase qua Thrift (th∆∞·ªùng ph·∫£i tri·ªÉn khai th√™m m·ªôt giao di·ªán c·ª• th·ªÉ). |
| **Airflow Web UI**       | 8080            | `http://localhost:8080`          | Xem v√† qu·∫£n l√Ω DAG trong Apache Airflow.                                               |
| **Spring Boot App**      | 8081            | `http://localhost:8081`          | Giao di·ªán ch√≠nh c·ªßa ·ª©ng d·ª•ng, hi·ªÉn th·ªã dashboard v√† k·∫øt qu·∫£ x·ª≠ l√Ω d·ªØ li·ªáu.             |
| **Spark Master UI**      | 9090            | `http://localhost:9090`          | Gi√°m s√°t c√°c t√°c v·ª• v√† worker c·ªßa Spark Master.                                        |
| **Spark Worker UI**      | 8080 (m·∫∑c ƒë·ªãnh) | `http://localhost:<worker-port>` | Xem th√¥ng tin chi ti·∫øt c√°c worker.                                                     |
| **Cassandra DB**         | Kh√¥ng c√≥ UI     | N/A                              | Cassandra kh√¥ng cung c·∫•p UI m·∫∑c ƒë·ªãnh, c√≥ th·ªÉ s·ª≠ d·ª•ng c√¥ng c·ª• b√™n th·ª© ba nh∆∞ DataStax.  |
|                          |                 |                                  |                                                                                        |
|                          |                 |                                  |                                                                                        |
|                          |                 |                                  |                                                                                        |
|                          |                 |                                  |                                                                                        |

### **L∆∞u √Ω:**

- N·∫øu g·∫∑p l·ªói truy c·∫≠p, ki·ªÉm tra c·∫•u h√¨nh trong file `docker-compose.yml` ƒë·ªÉ ƒë·∫£m b·∫£o c√°c port ƒë√£ ƒë∆∞·ª£c √°nh x·∫° ch√≠nh x√°c.
- ƒê·∫£m b·∫£o r·∫±ng c√°c d·ªãch v·ª• t∆∞∆°ng ·ª©ng ƒë√£ ƒë∆∞·ª£c kh·ªüi ƒë·ªông th√†nh c√¥ng tr∆∞·ªõc khi truy c·∫≠p UI.

D·ª±a v√†o logs, t√¥i th·∫•y c√°c services ƒëang ch·∫°y v√† l·∫Øng nghe tr√™n c√°c ports sau:

1. **HBase Master**:
```
- 16010 (Web UI): "Jetty bound to port 16010"
- 16020 (RPC): "Bind to /172.25.0.5:16020"
```

2. **YARN ResourceManager**:
```
- 8088 (Web UI): "Jetty bound to port 8088"
- 8032 (Client): "IPC Server listener on 8032"
- 8033 (Admin): "IPC Server listener on 8033"
```

3. **YARN NodeManager**:
```
- 8042 (Web UI): "Started ServerConnector...{0.0.0.0:8042}"
```

4. **Hadoop NameNode**:
```
- 9000 (RPC): Datanode ƒëang c·ªë k·∫øt n·ªëi t·ªõi "hadoop-namenode:9000"
```

5. **Zookeeper**:
```
- 2181: C√°c k·∫øt n·ªëi ZK ƒëang ho·∫°t ƒë·ªông
```

So s√°nh v·ªõi file docker-compose:
```yaml
hadoop-namenode:
  ports:
    - "9870:9870"  # Web UI
    - "9000:9000"  # RPC
    - "8020:8020"  # IPC

yarn-resourcemanager:
  ports:
    - "8088:8088"  # Web UI

hbase-master:
  ports:
    - "16000:16000"  # HBase Master RPC
    - "16010:16010"  # HBase Master Web UI
    - "16020:16020"  # HBase RegionServer RPC
    - "16030:16030"  # HBase RegionServer Web UI
```

T·∫•t c·∫£ c√°c port c·∫ßn thi·∫øt ƒë·ªÅu ƒë√£ ƒë∆∞·ª£c expose v√† ƒëang ho·∫°t ƒë·ªông. Tuy nhi√™n, c√≥ m·ªôt s·ªë warning v√† l·ªói c·∫ßn l∆∞u √Ω:

1. HBase Master c√≥ warning:
```
WARN  [master/ec326e803d17:16000:becomeActiveMaster] wal.WALProcedureStore: Unable to read tracker
```

2. Hadoop DataNode c√≥ v·∫•n ƒë·ªÅ k·∫øt n·ªëi:
```
WARN datanode.DataNode: Problem connecting to server: hadoop-namenode/172.25.0.3:9000
```

ƒê·ªÉ kh·∫Øc ph·ª•c, b·∫°n c√≥ th·ªÉ th·ª≠:
1. X√≥a volumes v√† restart:
```bash
docker-compose down -v
docker-compose up --build
```

2. Ki·ªÉm tra network connectivity gi·ªØa c√°c containers:
```bash
docker network inspect stream-network
```


--- 
### Stream layer 

#### **Stream Layer**

[](https://github.com/DoanNgocCuong/Big-Data-Project_2#1-stream-layer)

- Start Apache zookeeper

```batchfile
zookeeper-server-start.bat C:/kafka_2.13_2.6.0/config/zookeeper.properties
```

- Start Kafka server

```batchfile
kafka-server-start.bat C:/kafka_2.13_2.6.0/config/server.properties
```

- Create Kafka topic

```batchfile
kafka-topics.bat --create --topic smartphoneTopic --bootstrap-server localhost:9092
```

- Run the kafka producer

```batchfile
kafka-console-producer.bat --topic smartphoneTopic --bootstrap-server localhost:9092
```

- Run the kafka consumer

```batchfile
kafka-console-consumer.bat --topic smartphoneTopic --from-beginning --bootstrap-server localhost:9092
```

- Start HDFS and yarn (start-all or start-dfs and start-yarn)

```batchfile
start-all  
```

- Start Hbase

```batchfile
start-hbase  
```

- Run thrift server (for Hbase)

```batchfile
hbase thrift start
```

after all this run¬†`stream_pipeline.py`¬†script.

and then open the spring boot appliation in your idea and run it (you can access to the web app locally on¬†`localhost:8081/`)


Ah, ƒë·ªÉ t√¥i gi·∫£i th√≠ch v·ªÅ c√°c ports v√† UI:

1. **Kafka (9092)**:
- Port 9092 l√† port ƒë·ªÉ k·∫øt n·ªëi t·ªõi Kafka broker, KH√îNG c√≥ UI
- ƒê·ªÉ xem UI c·ªßa Kafka, b·∫°n c·∫ßn th√™m Kafka Manager (CMAK) ho·∫∑c Kafka UI:

```yaml
  # Th√™m Kafka UI
  kafka-ui:
    image: provectuslabs/kafka-ui
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
    depends_on:
      - kafka
    networks:
      - stream-network
```

2. **HBase Thrift (9090)**:
- Port 9090 ch·ªâ l√† port API, KH√îNG c√≥ UI
- ƒê·ªÉ xem UI c·ªßa HBase, d√πng port 16010 (HBase Master Web UI)

3. **C√°c UI c√≥ s·∫µn**:
```yaml
# C√°c ports c√≥ UI:
- HBase Master UI: localhost:16010
- HBase RegionServer UI: localhost:16030
- Zookeeper Admin UI: localhost:8080 (n·∫øu ƒë∆∞·ª£c c·∫•u h√¨nh)
```

4. **Th√™m monitoring tools**:
```yaml
  # Grafana ƒë·ªÉ monitoring
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    networks:
      - stream-network
```

B·∫°n c√≥ mu·ªën t√¥i c·∫≠p nh·∫≠t l·∫°i file docker-compose v·ªõi c√°c UI tools kh√¥ng?





OK, ƒë√¢y l√† file docker-compose-stream.yml ho√†n ch·ªânh:

```yaml
version: '3.8'

services:
  # 1. Zookeeper Service
  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    container_name: zookeeper
    ports:
      - "2181:2181"
    networks:
      - stream-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3

  # 2. Kafka Service
  kafka:
    image: wurstmeister/kafka:2.13-2.6.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CREATE_TOPICS: "smartphoneTopic:1:1"
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - stream-network

  # 3. Hadoop NameNode
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    ports:
      - "9870:9870"  # Web UI
      - "9000:9000"  # RPC
    environment:
      - CLUSTER_NAME=test-cluster
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - stream-network

  # 4. Hadoop DataNode
  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=hadoop-namenode:9870
    depends_on:
      - hadoop-namenode
    networks:
      - stream-network

  # 5. YARN ResourceManager
  yarn-resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: yarn-resourcemanager
    ports:
      - "8088:8088"  # Web UI
    depends_on:
      - hadoop-namenode
    networks:
      - stream-network

  # 6. YARN NodeManager
  yarn-nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: yarn-nodemanager
    depends_on:
      - yarn-resourcemanager
    networks:
      - stream-network

  # 7. HBase (All-in-one with Thrift)
  hbase:
    image: dajobe/hbase
    container_name: hbase
    ports:
      - "16000:16000"  # Master RPC
      - "16010:16010"  # Master Web UI
      - "16020:16020"  # RegionServer RPC
      - "16030:16030"  # RegionServer Web UI
      - "9090:9090"    # Thrift Server
    environment:
      HBASE_CONF_hbase_zookeeper_quorum: zookeeper
      HBASE_CONF_hbase_zookeeper_property_clientPort: 2181
      HBASE_MANAGES_ZK: "false"
      HBASE_CONF_hbase_master_port: 16000
      HBASE_CONF_hbase_master_info_port: 16010
      HBASE_CONF_hbase_regionserver_port: 16020
      HBASE_CONF_hbase_regionserver_info_port: 16030
      HBASE_CONF_hbase_thrift_port: 9090
    volumes:
      - hbase_data:/data
    depends_on:
      - zookeeper
      - hadoop-namenode
    networks:
      - stream-network
    command: ["sh", "-c", "start-hbase.sh && hbase thrift start -p 9090 && tail -f /dev/null"]

  # 8. Python Stream Pipeline
  python-app:
    image: python:3.8
    container_name: python-app
    volumes:
      - ./stream_pipeline.py:/app/stream_pipeline.py
      - ./requirements.txt:/app/requirements.txt
    working_dir: /app
    command: >
      bash -c "pip install -r requirements.txt &&
               python stream_pipeline.py"
    depends_on:
      - kafka
      - hbase
    networks:
      - stream-network

  # 9. Spring Boot Application
  spring-boot-app:
    image: openjdk:11-jre-slim
    container_name: spring-boot-app
    ports:
      - "8081:8081"
    volumes:
      - ./app.jar:/app/app.jar
    working_dir: /app
    command: ["java", "-jar", "app.jar"]
    depends_on:
      - kafka
      - hbase
      - python-app
    networks:
      - stream-network

  # 10. Flask Application (Alternative Web UI)
  flask-app:
    image: python:3.8
    container_name: flask-app
    ports:
      - "5000:5000"
    volumes:
      - ./flask_app:/app
    working_dir: /app
    command: >
      bash -c "pip install -r requirements.txt &&
               python app.py"
    depends_on:
      - python-app
    networks:
      - stream-network

networks:
  stream-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hbase_data:
```

ƒê·ªÉ ch·∫°y:

```bash
# Clone repository
git clone https://github.com/aymane-maghouti/Big-Data-Project

# Di chuy·ªÉn v√†o th∆∞ m·ª•c
cd Big-Data-Project

# D·ª´ng v√† x√≥a containers c≈©
docker-compose down -v

# Build v√† ch·∫°y
docker-compose -f docker-compose-stream.yml up --build
```

Sau khi ch·∫°y xong, b·∫°n c√≥ th·ªÉ truy c·∫≠p:
1. Spring Boot UI: `http://localhost:8081`
2. Flask UI: `http://localhost:5000`
3. HBase UI: `http://localhost:16010`
4. Hadoop NameNode UI: `http://localhost:9870`
5. YARN ResourceManager UI: `http://localhost:8088`

L∆∞u √Ω: ƒê·∫£m b·∫£o c√°c file sau ƒë√£ t·ªìn t·∫°i trong th∆∞ m·ª•c:
- `stream_pipeline.py`
- `requirements.txt`
- `app.jar` (Spring Boot application)
- Th∆∞ m·ª•c `flask_app` v·ªõi `app.py`




D·ª±a v√†o c·∫•u tr√∫c project, ta c·∫ßn t·ªï ch·ª©c th∆∞ m·ª•c nh∆∞ sau:

````
Big-Data-Project_2/              # Th∆∞ m·ª•c g·ªëc
‚îÇ
‚îú‚îÄ‚îÄ docker-compose-stream.yml    # File docker-compose
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt             # Dependencies cho Python
‚îÇ
‚îî‚îÄ‚îÄ Main/
    ‚îî‚îÄ‚îÄ Lambda/
        ‚îú‚îÄ‚îÄ producer.py          # Producer script
        ‚îÇ
        ‚îú‚îÄ‚îÄ ML_operations/       # ML models
        ‚îÇ   ‚îî‚îÄ‚îÄ xgb_model.pkl
        ‚îÇ
        ‚îú‚îÄ‚îÄ Stream_data/         # Data streaming
        ‚îÇ   ‚îú‚îÄ‚îÄ stream_data.csv
        ‚îÇ   ‚îî‚îÄ‚îÄ stream_data.py
        ‚îÇ
        ‚îú‚îÄ‚îÄ Stream_layer/        # Stream processing
        ‚îÇ   ‚îú‚îÄ‚îÄ insert_data_hbase.py
        ‚îÇ   ‚îú‚îÄ‚îÄ ML_consumer.py
        ‚îÇ   ‚îú‚îÄ‚îÄ stream_pipeline.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îÇ
        ‚îî‚îÄ‚îÄ real_time_web_app(Flask)/    # Flask web app
            ‚îú‚îÄ‚îÄ app.py
            ‚îú‚îÄ‚îÄ get_Data_from_hbase.py
            ‚îú‚îÄ‚îÄ static/
            ‚îÇ   ‚îú‚îÄ‚îÄ css/
            ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css
            ‚îÇ   ‚îî‚îÄ‚îÄ js/
            ‚îÇ       ‚îî‚îÄ‚îÄ script.js
            ‚îî‚îÄ‚îÄ templates/
                ‚îî‚îÄ‚îÄ index.html
````

T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c:

````bash
# T·∫°o th∆∞ m·ª•c g·ªëc
mkdir -p Big-Data-Project_2

# Di chuy·ªÉn v√†o th∆∞ m·ª•c g·ªëc
cd Big-Data-Project_2

# T·∫°o requirements.txt
cat > requirements.txt << EOF
kafka-python
happybase
pandas
scikit-learn
numpy
thriftpy2
flask
EOF

# T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c
mkdir -p Main/Lambda/{ML_operations,Stream_data,Stream_layer,real_time_web_app\(Flask\)/static/{css,js},real_time_web_app\(Flask\)/templates}

# Copy c√°c file t·ª´ repository g·ªëc
cp -r ../Big-Data-Project/Main/Lambda/* Main/Lambda/

# Copy docker-compose
cp docker-compose-stream.yml .
````

Ki·ªÉm tra c·∫•u tr√∫c:
````bash
tree Big-Data-Project_2
````
```
Main/
‚îî‚îÄ‚îÄ Lambda/
    ‚îú‚îÄ‚îÄ Stream_layer/
    ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.stream
    ‚îÇ   ‚îú‚îÄ‚îÄ requirements.stream.txt
    ‚îÇ   ‚îî‚îÄ‚îÄ stream_pipeline.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ real_time_web_app(Flask)/
        ‚îú‚îÄ‚îÄ Dockerfile.flask
        ‚îú‚îÄ‚îÄ requirements.flask.txt
        ‚îî‚îÄ‚îÄ app.py
```


### **T√≥m t·∫Øt b·∫£ng c√¢u h·ªèi - c√¢u tr·∫£ l·ªùi**

| **Nhi·ªám v·ª•**               | **L·ªánh / H∆∞·ªõng d·∫´n**                                                                                    |
| -------------------------- | ------------------------------------------------------------------------------------------------------- |
| Clone repository           | `git clone https://github.com/aymane-maghouti/Big-Data-Project`                                         |
| Start Zookeeper            | `zookeeper-server-start.bat C:/kafka_2.13_2.6.0/config/zookeeper.properties`                            |
| Start Kafka Server         | `kafka-server-start.bat C:/kafka_2.13_2.6.0/config/server.properties`                                   |
| Create Kafka Topic         | `kafka-topics.bat --create --topic smartphoneTopic --bootstrap-server localhost:9092`                   |
| Run Kafka Producer         | `kafka-console-producer.bat --topic smartphoneTopic --bootstrap-server localhost:9092`                  |
| Run Kafka Consumer         | `kafka-console-consumer.bat --topic smartphoneTopic --from-beginning --bootstrap-server localhost:9092` |
| Start HDFS and YARN        | `start-all` ho·∫∑c `start-dfs` v√† `start-yarn`                                                            |
| Start HBase                | `start-hbase`                                                                                           |
| Start HBase Thrift Server  | `hbase thrift start`                                                                                    |
| Run Stream Pipeline Script | `stream_pipeline.py`                                                                                    |
| Open Spring Boot App       | M·ªü trong IDE v√† truy c·∫≠p `http://localhost:8081/`.                                                      |
| Flask Web App              | ƒê∆∞·ª£c ph√°t tri·ªÉn ri√™ng, xem trong video demo.                                                            |