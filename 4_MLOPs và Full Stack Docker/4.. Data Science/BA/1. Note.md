
Lamba Expression 
Pyspark Word Count example: 
input file = >....

----------
Py 
airports.dat
pySpark with Rƒê

Try: 
data: airports.dat and try do: Create Rƒê from textfile 
COunt the number of airports 
Filter by country 
Group by country 
Count the number of airports in each country 

--------
```
from CSV file: 
filigjData2015 = spark\
.reads\

from p√Ωpark.sql import * 
list =  []```


Dataframe APIs

Dataframe : show, coleect, crearteOReplaceTeamView 
Column: like (), ....



```


### **Short Answers**:
- **Can Spark work with CSV?** ‚Üí **Yes**
- **Can Pandas work with JSON?** ‚Üí **Yes**

---

### **Comparison: Spark vs. Pandas**

| **Feature**            | **Spark**                                  | **Pandas**                                |
|-------------------------|--------------------------------------------|-------------------------------------------|
| **Scale**               | Designed for large-scale distributed data processing across a cluster. Handles terabytes or petabytes of data. | Designed for single-machine, small to medium-sized datasets (typically less than a few GB). |
| **Speed**               | Faster for processing huge datasets (distributed processing). Slower for small data due to overhead. | Faster for small datasets (in-memory operations). Can be slow with large datasets due to memory limits. |
| **Supported Formats**   | CSV, JSON, Parquet, ORC, Avro, SQL, Hive, HDFS, etc. | CSV, JSON, Excel, SQL, Pickle, HDF5, etc. |
| **Parallelism**         | Automatically parallelizes tasks across multiple nodes and cores in a cluster. | Single-threaded by default, though libraries like `modin` or `dask` provide some parallelism. |
| **Ease of Use**         | More complex; requires a Spark session and knowledge of distributed computing. | Simpler and more Pythonic; easy to use for Python developers. |
| **Memory Management**   | Efficiently handles datasets larger than memory by distributing data across nodes. | Operates in-memory; can crash if the dataset exceeds the available memory. |
| **Schema Handling**     | Enforces schema and can infer it automatically. Useful for big data pipelines. | Schema-less by default; relies on Python‚Äôs dynamic typing. |
| **Error Handling**      | Tends to be more robust for malformed or missing data. | May encounter issues with malformed or incomplete data. |
| **Performance Tuning**  | Requires configuration (e.g., partitioning, caching). | Easier to use out-of-the-box but lacks scalability. |
| **Use Cases**           | - Distributed big data processing<br>- ETL pipelines<br>- Data lakes<br>- Machine learning workflows<br>- Streaming data | - Exploratory data analysis<br>- Small-scale data processing<br>- Machine learning prototyping<br>- Data visualization |
| **Environment**         | Typically runs on clusters (Hadoop, AWS EMR, Databricks, etc.). Can also run locally. | Runs locally on a single machine. |
| **APIs**                | API available for Python (PySpark), Java, Scala, R. | Python-only. |

---


### **When to Use Spark or Pandas?**

#### **Use Pandas if:**
- Your dataset fits in memory (a few GBs or less).
- You are doing exploratory data analysis (EDA).
- You are working on a single machine and want simplicity.

#### **Use Spark if:**
- You are working with massive datasets (bigger than RAM or even disk).
- You need distributed processing for speed or scalability.
- You are building data pipelines or integrating with big data tools like Hadoop or Hive.

---

### Example Scenarios:
- **Spark**:
  - A company processing **1TB of log files** daily and storing the results in a data lake.
- **Pandas**:
  - A data analyst analyzing **5MB of sales data** on their local machine.


-------------------------
### **Pair RDD and DataFrame: Definitions and Comparisons**

#### **What is a Pair RDD?**
A **Pair RDD** (Resilient Distributed Dataset) is a special type of RDD in Apache Spark where each element is a key-value pair. It is used to perform operations like grouping, sorting, reducing, or aggregating data by keys.

- **Structure**: `(key, value)`
- **Example**:
  ```python
  rdd = spark.sparkContext.parallelize([("Alice", 25), ("Bob", 30), ("Alice", 28)])
  ```

- **Key Characteristics**:
  - Low-level abstraction.
  - Focused on distributed, functional-style programming.
  - Requires more boilerplate code compared to DataFrames.
  - Good for operations like grouping and aggregations by keys (e.g., `reduceByKey`, `groupByKey`).

---

#### **What is a DataFrame?**
A **DataFrame** is a higher-level abstraction built on top of RDDs in Spark. It represents data in tabular form, similar to a table in a relational database or a DataFrame in Pandas, with named columns and a schema.

- **Structure**: Tabular with rows and named columns.
- **Example**:
  ```python
  data = [("Alice", 25), ("Bob", 30), ("Alice", 28)]
  df = spark.createDataFrame(data, ["Name", "Age"])
  ```

- **Key Characteristics**:
  - Higher-level abstraction with richer APIs.
  - Optimized for performance through Catalyst Optimizer and Tungsten Engine.
  - Easier to work with compared to Pair RDDs due to schema enforcement and SQL-like operations.
  - Supports SQL queries and is widely used for ETL tasks.

---

| **Structure**            | Key-value pairs                      | Tabular with rows and columns        |
| **Ease of Use**          | Requires more code                   | Simpler and SQL-like                 |
| **Performance**          | Less optimized                      | Optimized using Catalyst Optimizer   |
| **Schema**               | No schema enforcement               | Schema enforcement (column names, types) |
| **Preferred Use Cases**  | Complex distributed data processing  | Structured data analytics and ETL    |
| **Operations**           | Key-based transformations            | SQL queries, groupBy, joins          |

---

### **When to Use What?**

- Use **Pair RDDs** if:
  - You need to perform key-value operations (e.g., `reduceByKey`).
  - You want low-level control over distributed data.
  - You are working with unstructured data.

- Use **DataFrames** if:
  - You have structured data (e.g., CSV, JSON, SQL tables).
  - You want better performance and simpler APIs.
  - You need to perform SQL-like operations.

Let me know if you'd like examples for a specific use case! üòä



-----------------------


Classification: 
- X: attriute, preditor, independent, variable, input 
- Y: class, response, dependent variable, output. 
----------
Task : Learn a model that maps each attribute set x into one of the predefined class labels y. 
