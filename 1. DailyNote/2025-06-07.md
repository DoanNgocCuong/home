# 1. Offline Indexing 3000 passages Done Vi√™n M√£n - 16,225 Nodes, 177,939 Edges - 31/05/2025

1. K·∫øt qu·∫£ pha Offline Indexing:
- Corpus: 3000 passages (t·∫°o b·∫±ng c√°ch g·ªôp context c·ªßa 300 c√¢u h·ªèi b·ªô VimQA - t∆∞∆°ng t·ª± nh∆∞ c√°ch t·∫°o corpus khi l√†m v·ªõi b·ªô hotpotQA).
- Ph∆∞∆°ng ph√°p: X√¢y d·ª±ng KG theo ki·∫øn tr√∫c HippoRAG 2 (2 lo·∫°i Node, 3 lo·∫°i Edge).
1.1 Th·ªëng k√™ ƒê·ªì th·ªã (S·ªë li·ªáu ch√≠nh x√°c):
- Nodes:
  - Passage Nodes: 3,000
  - Phrase Nodes: 13225 - ƒë∆∞·ª£c t·∫°o t·ª´: 20662 triples ƒë∆∞·ª£c Detect
(S·ªë l∆∞·ª£ng Phrase Nodes ph·∫£i b·∫±ng 2 l·∫ßn s·ªë l∆∞·ª£ng Triples detect ƒë∆∞·ª£c, tuy nhi√™n c√≥ nhi·ªÅu Phrase Nodes xu·∫•t hi·ªán trong nhi·ªÅu b·ªô Triples)
  - T·ªïng s·ªë Nodes: 16225
- Edges: (m≈©i t√™n ch·ªâ h∆∞·ªõng c·ªßa c·∫°nh)
  - RELATION Edges (Subject ‚Üí Object): 20662     - t∆∞∆°ng ƒë∆∞∆°ng v·ªõi 26,100 Triples.
  - SYNONYM Edges (Phrase ‚Üî Phrase, similarity ‚â• 0.8): 66140
  - CONTAINS Edges (Passage ‚Üí Phrase): 24997     (S·ªë Phrase l√† 13225 sao s·ªë l∆∞·ª£ng c·∫°nh CONTAINS l√† 24997 - v√¨ 1 `Phrase` thu·ªôc c√πng l√∫c nhi·ªÅu `Passage` ).
  - T·ªïng s·ªë Edges: 177939
[Image]

[Image]

TRIPLES (ROBUST EXTRACTION):
   Total: 20662
   Unique Subjects: 3072
   Unique Predicates: 4319
   Unique Objects: 11171

---
KNOWLEDGE GRAPH:
   Total Nodes: 16225
   Total Edges: 177939
   Passage Nodes: 3000
   Phrase Nodes: 13225
   RELATION Edges: 20662
   SYNONYM Edges: 132280
   CONTAINS Edges: 24997
1.2 C√°c th√¥ng tin l∆∞u trong Nodes v√† Edges:
Nodes Phrase, Node Passage ƒë·ªÅu l∆∞u : text v√† embedding ƒë·ªß c·∫£ trong meta data c·ªßa n√≥. => C√°c ph·∫ßn n√†y ƒë·ªÅu ƒë∆∞·ª£c Neo4j h·ªó tr·ª£ BM25, Search Embedding.

[Image]

[Image]

Edges :
- RELATION Edges (Subject ‚Üí Object): type: CONTAINER, metadata ch·ª©a "text" l√† predicate c·ªßa Subject v√† Object.
- SYNONYM Edges (Phrase ‚Üî Phrase, similarity ‚â• 0.8): c·∫°nh 2 h∆∞·ªõng.
- CONTAINS Edges (Passage ‚Üí Phrase): type CONTAINTER.
[Image]

[Image]

[Image]
[Image]

025-05-31 17:40:43,452 - module4_graph_builder - INFO - Created 13225 Phrase nodes with meaningful IDs
2025-05-31 17:40:43,453 - module4_graph_builder - INFO - Phrase ID mapping completed: 13225 phrases processed
2025-05-31 17:40:43,453 - module4_graph_builder - INFO - Phrase examples available in Neo4j database (check Phrase nodes with 'text' property)
2025-05-31 17:40:43,457 - module4_graph_builder - INFO - Building edges...
2025-05-31 17:40:43,458 - module4_graph_builder - INFO - Creating 20662 RELATION edges (no canonical mapping)...
2025-05-31 17:42:35,531 - module4_graph_builder - INFO - Created 20662 RELATION edges
2025-05-31 17:42:35,531 - module4_graph_builder - INFO - Creating 66140 SYNONYM edges (HippoRAG 2 style)...
2025-05-31 17:49:46,713 - module4_graph_builder - INFO - Created 66140 SYNONYM edges
2025-05-31 17:49:46,713 - module4_graph_builder - INFO - SYNONYM edges enable semantic connectivity between phrase variants
2025-05-31 17:49:46,714 - module4_graph_builder - INFO - Creating CONTAINS edges (no canonical mapping)...
2025-05-31 17:52:18,592 - module4_graph_builder - INFO - Created 24997 CONTAINS edges
2025-05-31 17:52:18,596 - module4_graph_builder - INFO - Graph construction completed: 16225 nodes, 111799 edges
2025-05-31 17:52:18,597 - module4_graph_builder - INFO - HippoRAG 2 style: All phrase variants preserved, connected via synonym edges
2025-05-31 17:52:18,598 - module4_graph_builder - INFO - Getting graph statistics...
2025-05-31 17:52:19,357 - module4_graph_builder - INFO - Graph statistics: {'nodes': {'Passage': 3000, 'Phrase': 13225}, 'edges': {'RELATION': 20662, 'SYNONYM': 132280, 'CONTAINS': 24997}, 'total_nodes': 16225, 'total_edges': 177939, 'hipporag_style': True, 'canonical_mapping': False, 'surface_forms_preserved': True, 'meaningful_phrase_ids': True}
2025-05-31 17:52:19,555 - pipeline_orchestrator - INFO - Saved triples with extraction methods to extracted_triples_with_methods.tsv
2025-05-31 17:52:19,567 - module3_synonym_detector - INFO - Saving 66140 synonym pairs to detected_synonyms.tsv
2025-05-31 17:52:19,941 - module3_synonym_detector - INFO - Synonym pairs saved successfully
2025-05-31 17:52:19,943 - pipeline_orchestrator - INFO - Saved synonyms to detected_synonyms.tsv
2025-05-31 17:52:19,944 - pipeline_orchestrator - INFO - Pipeline completed successfully in 11593.08 seconds

============================================================
OFFLINE PIPELINE SUMMARY (HippoRAG 2 Style + GPT Fallback)
============================================================
Execution Time: 11593.08 seconds
Input File: corpus_vimqa_dev_300.xlsx
Pipeline Style: HippoRAG_2_with_GPT_Fallback
Canonical Mapping: False
GPT Fallback: True

EXCEL PROCESSING:
   Documents Loaded: 3000
   Avg Length: 495.9 chars

CHUNKING:
   Total Chunks: 3000
   Avg Chunk Length: 495.9 chars
   Method: keep_as_paragraph

TRIPLES (ROBUST EXTRACTION):
   Total: 20662
   Unique Subjects: 3072
   Unique Predicates: 4319
   Unique Objects: 11171
   Qwen Extracted: 0
   GPT-3.5 Extracted: 20662
   HF Success Rate: 0.0% (0/3000)
   GPT Fallback Success: 2950/3000 failures rescued
   Total Failures: 0
   ‚úÖ All chunks successfully processed!

SYNONYMS (HippoRAG 2 Style):
   Synonym Pairs: 66140
   Avg Similarity: 0.848
   Threshold Used: 0.8
   NOTE: No canonical mapping - all phrase variants preserved

KNOWLEDGE GRAPH:
   Total Nodes: 16225
   Total Edges: 177939
   Passage Nodes: 3000
   Phrase Nodes: 13225
   RELATION Edges: 20662
   SYNONYM Edges: 132280
   CONTAINS Edges: 24997

HIPPORAG 2 + GPT FALLBACK FEATURES:
   - All phrase surface forms preserved
   - Synonym edges connect similar phrases
   - No information loss from canonical mapping
   - Robust extraction with GPT-3.5 fallback
   - Higher completion rate despite API failures
   - Ready for Personalized PageRank traversal
============================================================
Pipeline completed successfully!
Access Neo4j Browser: http://localhost:7474
============================================================
2025-05-31 17:52:19,970 - module4_graph_builder - INFO - Closing Neo4j connection
2025-05-31 17:52:19,971 - utils.utils_neo4j - INFO - Neo4j connection closed
2025-05-31 17:52:19,972 - module4_graph_builder - INFO - Neo4j connection closed
2025-05-31 17:52:19,976 - root - INFO - Pipeline results saved to pipeline_results_hipporag_corpus_vimqa_dev_300.json
üíæ Results saved to: pipeline_results_hipporag_corpus_vimqa_dev_300.json

============================================================
‚úÖ PIPELINE COMPLETED SUCCESSFULLY (HippoRAG 2 Style)
============================================================
üåê Access Neo4j Browser: http://localhost:7474
   Username: neo4j
   Password: graphrag123

üìà Extraction Statistics:
   HuggingFace successful: 0
   HuggingFace failed: 3000
   GPT-3.5 fallback used: 2950
   Total failures: 0

üèóÔ∏è HippoRAG 2 Graph Structure:
   - Passage nodes: Document chunks with embeddings
   - Phrase nodes: All surface forms preserved
   - RELATION edges: Semantic relationships
   - SYNONYM edges: Similarity connections
   - CONTAINS edges: Passage -> Phrase relationships

üìù Query Examples:
   1. Find synonyms: MATCH (p1:Phrase)-[:SYNONYM]-(p2:Phrase) RETURN p1, p2
   2. Explore relations: MATCH (p1:Phrase)-[:RELATION]-(p2:Phrase) RETURN p1, p2
   3. Find passages: MATCH (passage:Passage)-[:CONTAINS]->(phrase:Phrase) RETURN passage, phrase   4. Check extraction methods: MATCH ()-[r:RELATION]->() RETURN r.extraction_method, count(*)

üöÄ Next steps:
   1. Explore graph in Neo4j Browser
   2. Run Personalized PageRank queries
   3. Test semantic search capabilities
   4. Check intermediate results in output files
   5. Verify all passages have CONTAINS edges
============================================================
(.venv) PS D:\GIT\ResearchProject_Memory-AugmentedAIAgents_GraduationProject\src\system\PPDX\OfflineIndexing>



---

# 2. Build xong 1 c√°i Offline -> C√≥ s·ªë ·ªü pha Online - c∆° m√† v·∫´n sai sai.

run_retrieval_pipeline.py

C·∫ßn vi·∫øt code n√†y, b·∫°n vi·∫øt ng·∫Øn g·ªçn cho t√¥i nh√°

input l√†: file excel c√≥ c·ªôt: question -> output l√† copy file g·ªëc ra v√† ƒëi·ªÅn th√™m c√°c th√¥ng tin quan tr·ªçng qua t·ª´ng module

* module 1: top_k_passages, bm25_passages, hybrid_passages, raw_passages, top_n_triples, bm25_triples, hybrid_triples, raw_triples
* module 2: filted_triples
* module 3: filterd_passages
* module 4: expand_triples
* module 5: final_passge, final_triples, ...

(m·ªói c·ªôt ƒë·ªÉ th√¥ng tin ... k√®m score lu√¥n nh√° - d·∫°ng JSON c≈©ng ƒë∆∞·ª£c )

---
Code nh√°p l√™n plan cho t√¥i tr∆∞·ªõc nh√©
T√¥i oke th√¨ code h·∫øt

=====

"""
OnlineRetrievalAndQA/run_retrieval_pipeline.py - PLAN OVERVIEW
X·ª≠ l√Ω file Excel questions v√† th√™m detailed results t·ª´ c√°c modules

PLAN STRUCTURE:
===============

1. INPUT/OUTPUT:
   - Input: questions.xlsx (c·ªôt 'question')
   - Output: questions_results.xlsx (+ nhi·ªÅu c·ªôt m·ªõi)

2. COLUMNS S·∫º ƒê∆Ø·ª¢C TH√äM:

   Module 1 - Dual Retrieval:
   - module1_top_passages: JSON [{id, score, text_preview}, ...]
   - module1_top_triples: JSON [{subject, predicate, object, score}, ...]
   - module1_stats: JSON {bm25_avg, embedding_avg, hybrid_avg}

   Module 2 - Triple Filtering:
   - module2_filtered_triples: JSON [{triple_id, relevance_score, level}, ...]
   - module2_stats: JSON {original_count, filtered_count, avg_relevance}

   Module 3 - Passage Ranking:
   - module3_ranked_passages: JSON [{passage_id, final_score, support_score}, ...]
   - module3_stats: JSON {reranking_changes, support_distribution}

   Module 4 - Context Expansion:
   - module4_expanded_contexts: JSON [{context_id, relevance_score, path}, ...]

   Final Results:
   - final_passages: JSON top passages v·ªõi final scores
   - final_processing_time: float seconds
   - final_pipeline_stats: JSON overall statistics

3. CODE STRUCTURE:

class DetailedPipelineProcessor:
    - Extends RetrievalPipelineOrchestrator
    - Captures detailed outputs t·ª´ m·ªói module
    - Extract functions cho t·ª´ng module

def main():
    - Load Excel file
    - Initialize processor
    - Process each question
    - Add detailed columns
    - Save enhanced Excel

4. USAGE:
   python run_retrieval_pipeline.py --input questions.xlsx --output results.xlsx --top_k 10

5. FEATURES:
   ‚úÖ Excel input/output
   ‚úÖ Detailed module info capture
   ‚úÖ JSON format for complex data
   ‚úÖ Error handling per question
   ‚úÖ Progress tracking
   ‚úÖ Configurable parameters
   ‚úÖ Excel formatting v·ªõi colors
"""
========

## **üéØ Expected Results After Fix:**

### **Current (Before Fix):**
```json
{
  "passage_id": "passage_chunk_ƒê·ªânh Langbiang_19_0",
  "rank": 2,                           // ‚ùå Wrong rank
  "final_score": 0.212,                // ‚ùå Low score
  "retrieval_score": 0.529,
  "support_score": 0.0,                // ‚ùå No support found
  "supporting_triples_count": 0,       // ‚ùå Should be 4!
  "supporting_triples": []             // ‚ùå Empty
}
```

### **After Fix (Expected):**
```json
{
  "passage_id": "passage_chunk_ƒê·ªânh Langbiang_19_0",
  "rank": 1,                           // ‚úÖ Should be #1!
  "final_score": 0.75-0.85,            // ‚úÖ Much higher!
  "retrieval_score": 0.529,            // Same
  "support_score": 0.55-0.65,          // ‚úÖ Strong support!
  "supporting_triples_count": 4,        // ‚úÖ All 4 K'lang+H'biang triples
  "supporting_triples": [               // ‚úÖ Populated!
    "triple_58f6ddbf",  // lang biang ‚Üí named after ‚Üí ch√†ng k'lang v√† n√†ng h'biang
    "triple_3831070f",  // k'lang ‚Üí love ‚Üí h'biang
    "triple_291ed087",  // k'lang ‚Üí save ‚Üí h'biang
    "triple_dfb6b54e"   // h'biang ‚Üí cannot marry ‚Üí k'lang
  ]
}
```

## **üìä Score Calculation Details:**

### **Support Score Calculation:**
```python
# 4 supporting triples v·ªõi relevance scores:
# - Triple 1: 0.51 (lang biang named after...)
# - Triple 2: 0.456 (k'lang love h'biang)
# - Triple 3: 0.456 (k'lang save h'biang)
# - Triple 4: 0.409 (h'biang cannot marry k'lang)

avg_relevance = (0.51 + 0.456 + 0.456 + 0.409) / 4 = 0.458
count_boost = min(1.0, 4/5) = 0.8

support_score = (0.458 √ó 0.7) + (0.8 √ó 0.3) = 0.32 + 0.24 = 0.56
```

### **Final Score Calculation:**
```python
# Current config: 50-50 weights
alpha_retrieval = 0.5
alpha_support = 0.5

base_final_score = (0.5 √ó 0.529) + (0.5 √ó 0.56) = 0.265 + 0.28 = 0.545

# No penalty (has support now)
final_score = 0.545 (no modifiers applied)
```

## **üèÜ Final Ranking Expected:**

### **New Order:**
```
1. passage_chunk_ƒê·ªânh Langbiang_19_0     // ‚úÖ The story passage!
   Score: ~0.75 (with support boost)
   Content: "C√¢u chuy·ªán t√¨nh c·ªßa ch√†ng K'lang v√† n√†ng H'biang..."
   Supporting Triples: 4

2. passage_chunk_Khu d·ª± tr·ªØ sinh quy·ªÉn_14_0
   Score: 0.4 (no support, with penalty)
   Content: "Khu d·ª± tr·ªØ sinh quy·ªÉn th·∫ø gi·ªõi Langbiang..."
   Supporting Triples: 0

3. passage_chunk_Cao nguy√™n L√¢m Vi√™n_17_0
   Score: 0.0 (normalization issue)
   Supporting Triples: 0
```

## **üéâ Key Changes:**

### **‚úÖ What Will Improve:**
1. **Correct passage ranks #1** - The passage with actual K'lang+H'biang story
2. **Support scores populated** - 4 relevant triples found and counted
3. **Final scores realistic** - Reflects both retrieval quality + knowledge support
4. **Logical ranking** - Content-rich passage beats keyword-only matches

### **üìä Statistics Changes:**
```json
// Before:
"support_distribution": {
  "no_support": 3,      // All passages had no support
  "low_support": 0,
  "medium_support": 0,
  "high_support": 0
}

// After:
"support_distribution": {
  "no_support": 2,      // 2 passages still no support
  "low_support": 0,
  "medium_support": 1,  // ‚úÖ ƒê·ªânh Langbiang passage gets support!
  "high_support": 0
}
```

## **üöÄ Answer Quality:**

**Query**: "T√™n n√∫i Lang Biang ƒë∆∞·ª£c gi·∫£i th√≠ch b·∫±ng huy·ªÅn tho·∫°i n√†o"
**Answer**: "ch√†ng K'lang v√† n√†ng H'biang"

**Before**: Wrong passage ranked #1 (administrative info)
**After**: ‚úÖ **Correct passage ranked #1** (contains the actual legend story)

**This demonstrates the power of knowledge-augmented retrieval over pure keyword matching!** üéØ


---
# 3. V·∫•n ƒë·ªÅ embedding c√≥ r·ªìi m√† l√∫c truy v·∫•n l·∫°i ph·∫£i t√≠nh l·∫°i embedding c·ªßa passages ?? (c√≤n triples th√¨ l√∫c t·∫°o qu√™n ch∆∞a t√≠nh embedding cho n√≥)

- M·ª§C TI√äU B√ÇY GI·ªú L√Ä NHANH C√ì S·ªê N√äN L√Ä: T·∫†M D√ôNG MODEL EMBEDDING B√â T√ç NH∆Ø: "sentence-transformers/all-MiniLM-L6-v2"
ƒê·ªÇ ƒê·∫®Y NHANH T·ªêC ƒê·ªò RA S·ªê ƒê√É. FIX TH√îNG LU√íNG
---
Sau update EMBEDDING CHO TRIPLES ƒë√£ c√≥ c√°ch - d·ª± ki·∫øn l√† update embedding cho triples v√†o c·∫°nh RELATION c·ªßa n√≥ v√¨ n√≥ l√† DUY NH·∫§T M√Ä - ƒê√öNG KO NH·ªà :d


---
# 4. Fix l·ªói ·ªü module 3: support_id c·ªßa c√°c passage ƒë·ªÅu b·∫±ng 0 m·∫∑c d√π triples th√¨ ƒë·∫ßy ƒë√≥ ra: nguy√™n nh√¢n do s·ª± ko kh·ªõp c·ªßa t√™n id (do ·∫£nh h∆∞·ªüng c·ªßa ti·ªÅn t·ªë)


def standardize_triple_passage_ids(
    filtered_triples: List[Any],
    passage_id_prefix: str = "passage_chunk_",
    triple_id_prefix: str = "chunk_"
) -> List[Any]:
    """
    Chu·∫©n h√≥a ti·ªÅn t·ªë c·ªßa source_passage_id trong c√°c FilteredTriple ƒë·ªÉ kh·ªõp v·ªõi ƒë·ªãnh d·∫°ng
    passage_id c·ªßa c√°c passage.

    Args:
        filtered_triples (List[Any]): Danh s√°ch c√°c ƒë·ªëi t∆∞·ª£ng FilteredTriple t·ª´ Module 2
        passage_id_prefix (str): Ti·ªÅn t·ªë mong mu·ªën cho passage_id
        triple_id_prefix (str): Ti·ªÅn t·ªë hi·ªán t·∫°i c·ªßa source_passage_id trong triples

    Returns:
        List[Any]: Danh s√°ch c√°c ƒë·ªëi t∆∞·ª£ng FilteredTriple ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a source_passage_id
    """
    if not filtered_triples:
        return []

    logger.info(f"Standardizing triple source_passage_ids from '{triple_id_prefix}' to '{passage_id_prefix}'...")
    standardized_triples = []
    for triple in filtered_triples:
        if triple.source_passage_id.startswith(triple_id_prefix):
            new_source_passage_id = passage_id_prefix + triple.source_passage_id[len(triple_id_prefix):]
            # Create a new triple with updated ID
            triple_dict = triple.__dict__.copy()
            triple_dict['source_passage_id'] = new_source_passage_id
            standardized_triples.append(type(triple)(**triple_dict))
        else:
            logger.warning(f"Triple ID '{triple.source_passage_id}' does not start with '{triple_id_prefix}'. Keeping as is.")
            standardized_triples.append(triple)
    logger.info(f"Finished standardizing {len(standardized_triples)} triples.")
    return standardized_triples


---
# 4. [C√≥ k·∫øt qu·∫£ s·ªë cho t·ª´ng module l√∫c 20h t·ªëi - th·ªü ph√†o nh·∫π nh√µm v√¨ c√≥ s·ªë l√† b√°o c√°o ƒë∆∞·ª£c r·ªìi - l√∫c ƒë√≥ ƒëang ng·ªìi v·ªõi Oanh - c·∫£m gi√°c ƒë·ªëi m·∫∑t v·ªõi deadline ƒë∆∞·ª£c b·∫£o v·ªá/ko m√† m√¨nh b√¨nh tƒ©nh v√£i]11:44 P.M - 31/05/2025 - 01/06/2025 l√† h·∫°n cu·ªëi duy·ªát "ƒê·ªìng √Ω/Ko ƒë·ªìng √Ω" cho b·∫£o v·ªá ---Co can KQ cua Giai doan 1 - Offline - KG va cac buoc cu the cung KQ cua giai doan 2. DA thu nghiem tren  dung 1 dataset nen can nhieu KQ trung gian va phan tich, so sanh

