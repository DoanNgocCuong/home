- CÃ³ cÃ¡ch nÃ o Ä‘á»ƒ ko cáº§n dÃ¹ng nhiá»u time Ä‘á»ƒ train

![[Pasted image 20250808202145.png]]

# 1. Motivation:

- Lazy Learning. Gáº§n thi má»›i há»c vÃ  báº¡n cá»© quyá»ƒn sÃ¡ch 1 vÃ  quyá»ƒn sÃ¡ch 2.
- New Data point => Biáº¿t Ä‘Æ°á»£c nÃ³ nÃªn náº±m á»Ÿ cuá»‘n 1 hay cuá»‘n 2.

![[Pasted image 20250808205712.png]]

Dá»° ÃN THáº¬T + CHIA Sáºº + ÄÆ¯á»¢C COACHING + NETWORKIN\

k = 4, 2 Ä‘á», 2 xanh,

Precision = Sá»‘ dá»± Ä‘oÃ¡n lÃ  ung thÆ° Ä‘Ãºng / tá»•ng sá»‘ dá»± Ä‘oÃ¡n
Recall = Sá»‘ dá»± Ä‘oÃ¡n lÃ  ung thÆ° Ä‘Ãºng / tá»•ng sá»‘ ung thÆ° thá»±c táº¿

```
nhÆ° vÃ­ dá»¥ cÃ¡i auto ra khÃ´ng bá»‹ cancer á»Ÿ data bá»‹ biased tÃ­nh precision mÃ  máº«u sá»‘ = 0 thÃ¬ output tá»« skikit learn lÃ  gÃ¬ áº¡  
  
**You** 9:33 PM  
@Hieu Ngo ChÆ°a hiá»ƒu cÃ¢u há»i cá»§a báº¡n láº¯m áº¡  
  
**Hieu Ngo** 9:34 PM  
nhÆ° vÃ­ dá»¥ auto ra â€œkhÃ´ng bá»‹ cancerâ€ á»Ÿ data bá»‹ biased vá»«a nÃ£y, lÃºc tÃ­nh precision mÃ  máº«u sá»‘ = 0 thÃ¬ output tá»« skikit learn lÃ  gÃ¬ áº¡  
  
**Äá»©c Nguyá»…n** 9:37 PM  
= 0 háº¿t thÃ¬ nÃ³ rÆ¡i vÃ o máº¥y cÃ¡i trÆ°á»ng há»£p False Negative rá»“i váº«n cá»™ng á»Ÿ máº«u Ã¡ báº¡n, nÃªn máº«u kbh = 0 dc  
  
**You** 9:39 PM (Edited)  
Precision = tá»•ng sá»‘ ung thÆ° Ä‘Ãºng / tá»•ng sá»‘ dá»± Ä‘oÃ¡n ung thÆ°  
+, Vá»›i bÃ i vá»«a nÃ£y khi F(x): y = 0 (tá»©c vá»›i má»i x, dá»± Ä‘oÃ¡n luÃ´n lÃ  0)  
thÃ¬ tá»•ng sá»‘ dá»± Ä‘oÃ¡n ung thÆ° = 0  
  
Ã báº¡n lÃ  váº­y áº¡
```

---

# 1. SÃ¡ng chá»§ nháº­t:

## 1.1 Code cháº¡y bÃ i cÃ´ng ty done. 11h30

## 1.2. KNN

- KNN thÆ°á»ng sá»­ dá»¥ng loáº¡i khoáº£ng cÃ¡ch nÃ o máº·c Ä‘á»‹nh. Táº¡i sao???
  =>

---

# **KNN = K-Nearest Neighbors**

> - To know what group you are in, you look at the **k closest people** to you.
> - You see what group most of them are in.
> - Then you join that group.

Weighted Voting:
Suppose k = 3

- Neighbor 1 (distance = 1, class = Cat) â†’ weight = 1/1 = 1.0
- Neighbor 2 (distance = 2, class = Dog) â†’ weight = 1/2 = 0.5
- Neighbor 3 (distance = 3, class = Dog) â†’ weight = 1/3 â‰ˆ 0.33

Votes:

- Cat = 1.0
- Dog = 0.5 + 0.33 = 0.83

Winner = **Cat** ğŸ±

$w = \frac{1}{d}$  hoáº·c  $w = \frac{1}{d^2}$

---

# Bruce Force and KDTree

```bash
            [P2: x=5]
           /           \
   [P7: y=4]         [P6: y=2]
   /      \           /      \
P1(z=4)  P4(z=9)   P5(z=5)  P3(z=7)

```

+, Difference between.
+,
+,

```
1 sá»‘ cÃ¢u há»i:

1. Vá»›i K = 1 thÃ¬ cÃ¡ch quÃ©t Ä‘i qua háº¿t tá»« node root Ä‘áº¿n cÃ¡c node lÃ¡. Sau Ä‘Ã³ chá»n best, right??

2. Vá»›i K = 2, 3 <= log_2_(6) +1 => ???

3. Vá»›i K > log_2_(6) + 1 ???
```

https://www.geeksforgeeks.org/cpp/kd-trees-in-cpp/
https://viblo.asia/p/gioi-thieu-thuat-toan-kd-trees-nearest-neighbour-search-RQqKLvjzl7z
[2004.04523](https://arxiv.org/pdf/2004.04523)

---

![[Pasted image 20250810154215.png]]

---

## **A) Bag-of-Words (BoW)**

- **What it does:** Counts how many times each word appears in a document.
- **Output:** A vector of raw counts.

> Vocabulary = [â€œcatâ€, â€œdogâ€, â€œeatâ€]
> Document: â€œcat eat catâ€ â†’ BoW vector = [2, 0, 1]

---

## **B) TF-IDF**

We need a small corpus to compute IDF. Suppose we have 3 documents:

1. `"cat eat cat"`
2. `"dog eat dog"`
3. `"cat dog eat"`

**Step 1 â€“ Term Frequency (TF)** for document 1:

- cat: 2/3 â‰ˆ 0.6667
- dog: 0/3 = 0
- eat: 1/3 â‰ˆ 0.3333

**Step 2 â€“ Inverse Document Frequency (IDF)**:

IDF(t)=logâ¡(TotalÂ docsDocsÂ containingÂ t)\text{IDF}(t) = \log\left(\frac{\text{Total docs}}{\text{Docs containing t}}\right)

- cat: log(3/2) â‰ˆ 0.1761
- dog: log(3/2) â‰ˆ 0.1761
- eat: log(3/3) = 0

**Step 3 â€“ TF-IDF** (TF Ã— IDF):

- cat: 0.6667 Ã— 0.1761 â‰ˆ 0.1174
- dog: 0 Ã— 0.1761 = 0
- eat: 0.3333 Ã— 0 = 0

**TF-IDF vector** â†’ 0.1174,0,00.1174, 0, 0

---

![[Pasted image 20250810162330.png]]

> Ban Ä‘áº§u mÃ¬nh tÆ°á»Ÿng ráº±ng: Khi cáº­p nháº­t centroids lÃ  má»›i cáº­p nháº­t 1 vÃ i nodes, cÃ²n cÃ¡c nodes khÃ¡c ná»¯a.
> => CÆ¡ mÃ : Ä‘Ãºng ra lÃ : sau khi chá»n 1 centroids thÃ¬ ta cáº§n tÃ­nh toÃ¡n táº¥t cáº£ khoáº£ng cÃ¡ch cÃ¡c Ä‘iá»ƒm cÃ²n láº¡i vá»›i cÃ¡c centroids rá»“i => Sau khi tÃ­nh toÃ¡n chá»‰ cáº§n 1 láº§n centroids khÃ´ng Ä‘á»•i thÃ¬ ta dá»«ng láº¡i ngay .

---

[KNN - K Nearest Neighbor](https://alo2025alconquer.hashnode.dev/knn-k-nearest-neighbor)

---

CÃ¢u há»i : nÃªn dá»±a vÃ o yáº¿u tá»‘ nÃ o Ä‘á»ƒ chá»n 5-folds, 10-folds, 3-folds
+, Báº£n cháº¥t lÃ  train nhiá»u láº§n Ä‘á»ƒ láº¥y average sau Ä‘Ã³ lÃ m Ä‘áº¡i diá»‡n cho K.

---

![[Pasted image 20250810210841.png]]

Má»¥c Ä‘Ã­ch validation Ä‘á»ƒ lá»±a chá»n K tá»‘t nháº¥t. => Ä‘á»ƒ lá»±a chá»n K tá»‘i Æ°u.
K cá»§a KNN cháº¡y tá»« 1-> 15.
+, Test K vá»›i viá»‡c cho cháº¡y trÃªn K-folds (Cross validation) Ä‘á»ƒ nÃ³ tÃ­nh ra Ä‘Æ°á»£c score Ä‘áº¡i diá»‡n cho K

---

TF-IDF
Bag of word
Onehot embedding
Embedding??

---

+, Khi káº¿t quáº£ perform tá»‡.

- Váº¥n Ä‘á» do model hay do data.
  +, Äá»ƒ biáº¿t Ä‘Æ°á»£c data khÃ´ng tá»‘t -> tÃ¡ch thÃ nh táº­p

![[Pasted image 20250810211905.png]]
+, Train vs train-dev: cao trÃªn táº­p train, tá»‡ trÃªn táº¡p train-dev => overfit
+, Náº¿u train tá»‘t, train-dev tá»‘t => oke => mang Ä‘i Ä‘Ã¡nh giÃ¡ mÃ  tá»‡ => data maybe tá»‡.

---

Táº¡i sao: bá»™ tá»« chá»‰ 1000 tá»«

![[Pasted image 20250810212309.png]]

TÃ´i Ä‘i há»c: biá»ƒu diá»…n 200.000 chiá»u => Nhiá»u cÃ¡i bá»‹ 0, ráº¥t Ã­t cÃ¡i cÃ³ sá»‘ => Bá»‹ tá»‘n bá»™ nhá»›.
+,

![[Pasted image 20250810212539.png]]

Vector thÆ°a, toÃ n cÃ¡i bá»‹ = 0

```
Xá»­ lÃ½ váº¥n Ä‘á» khi : hello thÃ¬ model tráº£ lá»i ra pháº§n text wiki feed cho nÃ³ ?? 

```

![[Pasted image 20250810213206.png]]

Ok, mÃ¬nh sáº½ nÃ³i ká»¹ vá» **phÆ°Æ¡ng phÃ¡p Convert Text to Feature â€“ Bag of Words (BoW)** Ä‘á»ƒ báº¡n vá»«a hiá»ƒu báº£n cháº¥t vá»«a hÃ¬nh dung Ä‘Æ°á»£c táº¡i sao code cá»§a báº¡n láº¡i viáº¿t nhÆ° váº­y.

---

## 1. Ã tÆ°á»Ÿng cá»§a BoW

**Bag of Words** lÃ  cÃ¡ch biá»ƒu diá»…n vÄƒn báº£n thÃ nh sá»‘ Ä‘á»ƒ mÃ¡y há»c xá»­ lÃ½.NguyÃªn táº¯c:

- Má»—i **tá»«** (hoáº·c token) trong táº­p dá»¯ liá»‡u Ä‘Æ°á»£c coi nhÆ° má»™t **feature** (Ä‘áº·c trÆ°ng).
- Ta Ä‘áº¿m sá»‘ láº§n tá»« Ä‘Ã³ xuáº¥t hiá»‡n trong má»—i vÄƒn báº£n.
- Bá» qua thá»© tá»± tá»«, ngá»¯ phÃ¡p â†’ chá»‰ quan tÃ¢m Ä‘áº¿n **táº§n suáº¥t**.

VÃ­ dá»¥:

```
VÄƒn báº£n 1: "I love AI"
VÄƒn báº£n 2: "I love Python"

Tá»« vá»±ng (vocab) = ["I", "love", "AI", "Python"]

Biá»ƒu diá»…n BoW:
V1: [1, 1, 1, 0]
V2: [1, 1, 0, 1]
```

á» Ä‘Ã¢y, `[1, 1, 1, 0]` nghÄ©a lÃ :

- "I": xuáº¥t hiá»‡n 1 láº§n
- "love": 1 láº§n
- "AI": 1 láº§n
- "Python": 0 láº§n

---

## 2. CÃ¡c bÆ°á»›c trong code cá»§a báº¡n

```python
vectorizer = CountVectorizer(max_features=1000) 
X_train = vectorizer.fit_transform(imdb_train['text']).toarray()
X_test = vectorizer.transform(imdb_test['text']).toarray()
```

- **`CountVectorizer`**: cÃ´ng cá»¥ cá»§a scikit-learn Ä‘á»ƒ tá»± Ä‘á»™ng:

  1. TÃ¡ch tá»« (tokenize)
  2. XÃ¢y dá»±ng tá»« Ä‘iá»ƒn (vocab)
  3. Äáº¿m táº§n suáº¥t tá»« trong má»—i vÄƒn báº£n
- **`max_features=1000`**: giá»›i háº¡n sá»‘ tá»« Ä‘áº·c trÆ°ng nhiá»u nháº¥t lÃ  1000 tá»« phá»• biáº¿n â†’ giáº£m chiá»u dá»¯ liá»‡u vÃ  trÃ¡nh overfitting.
- **`fit_transform`**:

  - `fit`: há»c vocab tá»« táº­p train.
  - `transform`: Ä‘áº¿m táº§n suáº¥t tá»« cho tá»«ng vÄƒn báº£n.
  - Káº¿t quáº£ tráº£ vá» dáº¡ng **sparse matrix** â†’ `.toarray()` Ä‘á»ƒ Ä‘á»•i sang NumPy array.
- **`transform` cho táº­p test**:

  - KhÃ´ng `fit` láº¡i vÃ¬ vocab pháº£i giá»¯ nguyÃªn nhÆ° lÃºc train â†’ trÃ¡nh "rÃ² rá»‰ dá»¯ liá»‡u".

---

## 3. Äiá»ƒm máº¡nh cá»§a BoW

- ÄÆ¡n giáº£n, dá»… triá»ƒn khai.
- Hiá»‡u quáº£ vá»›i cÃ¡c mÃ´ hÃ¬nh truyá»n thá»‘ng (Naive Bayes, Logistic Regression, SVMâ€¦).
- Dá»… kiá»ƒm soÃ¡t kÃ­ch thÆ°á»›c Ä‘áº·c trÆ°ng (báº±ng `max_features` hoáº·c `min_df`).

---

## 4. Háº¡n cháº¿

- **Máº¥t thÃ´ng tin ngá»¯ cáº£nh**: KhÃ´ng biáº¿t thá»© tá»± tá»«, má»‘i quan há»‡ giá»¯a tá»«.
- **Vector thÆ°a (sparse)**: Nhiá»u sá»‘ 0 â†’ tá»‘n bá»™ nhá»› náº¿u tá»« vá»±ng lá»›n.
- KhÃ´ng xá»­ lÃ½ tá»‘t cÃ¡c tá»« Ä‘á»“ng nghÄ©a hoáº·c Ä‘a nghÄ©a (semantic meaning).
- KhÃ´ng táº­n dá»¥ng thÃ´ng tin tá»« embedding nhÆ° Word2Vec, BERT.

---

## 5. Má»™t sá»‘ cáº£i tiáº¿n trÃªn BoW

- **TF-IDF**: thay vÃ¬ chá»‰ Ä‘áº¿m, cÃ²n tÃ­nh trá»ng sá»‘ dá»±a trÃªn má»©c Ä‘á»™ Ä‘áº·c trÆ°ng cá»§a tá»« trong toÃ n bá»™ corpus.
- **N-gram**: thay vÃ¬ chá»‰ láº¥y 1 tá»« (unigram), cÃ³ thá»ƒ láº¥y cá»¥m tá»« 2 tá»« (bigram), 3 tá»« (trigram)â€¦ Ä‘á»ƒ giá»¯ thÃªm chÃºt ngá»¯ cáº£nh.
- **Embedding**: dÃ¹ng Word2Vec, GloVe, FastText, BERTâ€¦ Ä‘á»ƒ biá»ƒu diá»…n tá»«/vÄƒn báº£n thÃ nh vector dense, mang thÃ´ng tin ngá»¯ nghÄ©a.

---

Náº¿u báº¡n muá»‘n, mÃ¬nh cÃ³ thá»ƒ váº½ cho báº¡n **sÆ¡ Ä‘á»“ flow tá»« vÄƒn báº£n â†’ BoW â†’ model ML** Ä‘á»ƒ nhÃ¬n trá»±c quan hÆ¡n.
Báº¡n cÃ³ muá»‘n mÃ¬nh váº½ luÃ´n khÃ´ng?

---

Ok, mÃ¬nh sáº½ so sÃ¡nh **BoW** vÃ  **TF-IDF** ngáº¯n gá»n, rá»“i giáº£i thÃ­ch vÃ¬ sao dÃ¹ khÃ´ng hiá»ƒu ngá»¯ nghÄ©a chÃºng váº«n dÃ¹ng tá»‘t.

---

## 1. BoW vs. TF-IDF

| TiÃªu chÃ­                       | **BoW**                                                                                 | **TF-IDF**                                                                                                                 |
| -------------------------------- | --------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **CÃ¡ch tÃ­nh**            | Äáº¿m sá»‘ láº§n má»—i tá»« xuáº¥t hiá»‡n trong vÄƒn báº£n.                                          | Äáº¿m sá»‘ láº§n tá»« xuáº¥t hiá»‡n, nhÆ°ng**giáº£m trá»ng sá»‘** náº¿u tá»« xuáº¥t hiá»‡n nhiá»u trong toÃ n bá»™ corpus (dÃ¹ng IDF). |
| **Trá»ng sá»‘ tá»«**         | Chá»‰ dá»±a vÃ o táº§n suáº¥t xuáº¥t hiá»‡n (TF).                                                   | TF Ã— IDF â†’ tÄƒng trá»ng sá»‘ cho tá»« Ä‘áº·c trÆ°ng, giáº£m cho tá»« phá»• biáº¿n nhÆ° â€œtheâ€, â€œisâ€.                               |
| **ThÃ´ng tin ngá»¯ nghÄ©a** | KhÃ´ng giá»¯ ngá»¯ cáº£nh, khÃ´ng biáº¿t nghÄ©a tá»«.                                              | CÅ©ng khÃ´ng giá»¯ ngá»¯ cáº£nh, nhÆ°ng lá»c bá»›t tá»« khÃ´ng quan trá»ng.                                                           |
| **KÃ­ch thÆ°á»›c vector**   | Phá»¥ thuá»™c sá»‘ lÆ°á»£ng tá»« vá»±ng.                                                            | Giá»‘ng BoW, nhÆ°ng giÃ¡ trá»‹ trong vector lÃ  trá»ng sá»‘ TF-IDF.                                                                 |
| **Khi nÃªn dÃ¹ng**         | Khi dá»¯ liá»‡u nhá», tá»« quan trá»ng lÃ  cÃ¡c tá»« khÃ³a láº·p láº¡i nhiá»u trong má»™t vÄƒn báº£n. | Khi cáº§n phÃ¢n biá»‡t tá»« Ä‘áº·c trÆ°ng so vá»›i tá»« phá»• biáº¿n trong táº­p dá»¯ liá»‡u.                                               |

---

## 2. VÃ¬ sao khÃ´ng hiá»ƒu ngá»¯ nghÄ©a váº«n dÃ¹ng tá»‘t?

1. **Nhiá»u tÃ¡c vá»¥ phÃ¢n loáº¡i chá»‰ cáº§n táº§n suáº¥t tá»«**VÃ­ dá»¥: vá»›i phÃ¢n loáº¡i sentiment (tÃ­ch cá»±c/tiÃªu cá»±c), sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c tá»« â€œgoodâ€, â€œexcellentâ€ hay â€œbadâ€, â€œterribleâ€ Ä‘Ã£ Ä‘á»§ Ä‘á»ƒ phÃ¢n biá»‡t.
2. **Ngá»¯ nghÄ©a nhiá»u khi Ä‘Æ°á»£c â€œáº©nâ€ trong thá»‘ng kÃª tá»«**CÃ¡c tá»« xuáº¥t hiá»‡n nhiá»u trong má»™t nhÃ£n vÃ  Ã­t á»Ÿ nhÃ£n khÃ¡c â†’ tá»± Ä‘á»™ng trá»Ÿ thÃ nh dáº¥u hiá»‡u phÃ¢n loáº¡i.
3. **Thuáº­t toÃ¡n há»c mÃ¡y sáº½ tá»± há»c quy luáº­t tá»« vector nÃ y**DÃ¹ vector chá»‰ lÃ  sá»‘ Ä‘áº¿m, model nhÆ° Logistic Regression, Naive Bayesâ€¦ sáº½ tÃ¬m ra trá»ng sá»‘ phÃ¹ há»£p Ä‘á»ƒ phÃ¢n biá»‡t.
4. **ÄÆ¡n giáº£n vÃ  hiá»‡u quáº£ cho baseline**
   BoW vÃ  TF-IDF dá»… tÃ­nh, nhanh, Ã­t tÃ i nguyÃªn â†’ phÃ¹ há»£p Ä‘á»ƒ thá»­ nghiá»‡m nhanh hoáº·c khi dá»¯ liá»‡u khÃ´ng Ä‘á»§ Ä‘á»ƒ train embedding phá»©c táº¡p.

---

Náº¿u báº¡n muá»‘n, mÃ¬nh cÃ³ thá»ƒ lÃ m thÃªm **má»™t hÃ¬nh minh há»a flow: VÄƒn báº£n â†’ BoW & TF-IDF â†’ model ML**, nhÃ¬n phÃ¡t hiá»ƒu ngay luÃ´n.
Báº¡n cÃ³ muá»‘n mÃ¬nh váº½ khÃ´ng?

![[Pasted image 20250810214752.png]]

![[Pasted image 20250810214959.png]]

![[Pasted image 20250810220248.png]]

Khi nÃ o dÃ¹ng Min - Max / Standard
+, Min max nháº¡y cáº£m vá»›i nhiá»…u
+, Standard
+<

| gender | male | female |
| ------ | ---- | ------ |
| male   | 1    | 0      |
| female | 0    | 1      |
| male   | 1    | 0      |

Ä‘iá»ƒm:  giá»i khÃ¡ trung bÃ¬nh -> encod: 0, 1, 2, 3, 4,

![[Pasted image 20250810221418.png]]

ANN, HNSW. (FAISS - FB - HNSW)

![[Pasted image 20250810221850.png]]
