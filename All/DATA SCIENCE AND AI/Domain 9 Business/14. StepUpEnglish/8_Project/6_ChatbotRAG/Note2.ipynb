{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. How Run "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cài database local Milvus bằng Docker - https://milvus.io/docs/install_milvus_docker.md\n",
    "```bash\n",
    "docker-compose up --build\n",
    "# or \n",
    "docker-compose build and docker-compose up  \n",
    "docker run -p 8000:3000 -e MILVUS_URL=localhost:19530 zilliz/attu:v2.4 # Check Milvus running at http://localhost:8000/#/databases/default/colletions\n",
    "```\n",
    "- Lệnh này dùng để chạy Attu - một GUI tool để quản lý và theo dõi Milvus database. Hãy phân tích từng phần:\n",
    "    - `-p 8000:3000`: Mở port 8000 trên máy host để kết nối với Attu\n",
    "    - `-e MILVUS_URL=localhost:19530`: Chỉ định URL của Milvus server (localhost là IP của máy host, 19530 là port mà Milvus sử dụng)\n",
    "    - `zilliz/attu:v2.4`: Image Docker của Attu, version 2.4\n",
    "2. Ctrl Shift P -> cài .venv\n",
    "- Cài requirements.txt\n",
    "```markdown\n",
    "HÃY CHO TÔI BIẾT\n",
    "1. các lỗi gì đã xảy ra\n",
    "2. Tại sao xảy ra\n",
    "3. Bạn đã fix  như nào ???\n",
    "```\n",
    "\n",
    "4. `.env`\n",
    "```bash\n",
    "OPENAI_API_KEY=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\n",
    "\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_API_KEY=\"your-langchain-api-key-here\"\n",
    "LANGCHAIN_PROJECT=\"project-name\"\n",
    "```\n",
    "\n",
    "3. Chạy ứng dụng: \n",
    "\n",
    "- Crawl data về local trước khi bắt đầu chạy ứng dụng. Mở Terminal/Command Prompt, di chuyển vào thư mục src và chạy:\n",
    "```python\n",
    "cd src\n",
    "python crawl.py\n",
    "```\n",
    "\n",
    "- Seed data vào Milvus:\n",
    "```python\n",
    "python seed_data.py\n",
    "```\n",
    "Data được seed tới Milvus database với tên collection là `data_test`\n",
    "Milvus vector database running locally at http://localhost:19530. Milvus is a vector database used for storing and searching vector embeddings.\n",
    "\n",
    "- Chạy ứng dụng:\n",
    "```python\n",
    "streamlit run main.py\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 lỗi cài thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tôi sẽ phân tích chi tiết các vấn đề:\n",
    "\n",
    "### 1. Các lỗi đã xảy ra:\n",
    "- **Lỗi 1**: Conflict giữa `pydantic` và `chromadb`\n",
    "- **Lỗi 2**: Không thể build wheel cho `chroma-hnswlib` vì thiếu Microsoft Visual C++ Build Tools\n",
    "\n",
    "### 2. Nguyên nhân:\n",
    "- **Lỗi 1**: `chromadb` phiên bản cũ yêu cầu `pydantic<2.0` trong khi file requirements.txt yêu cầu `pydantic>=2.0.0`\n",
    "- **Lỗi 2**: `chromadb` cần phải được biên dịch từ source code trên Windows, điều này yêu cầu C++ compiler\n",
    "\n",
    "### 3. Cách fix:\n",
    "Tôi đã đề xuất thay đổi sang sử dụng FAISS thay vì Chroma vì:\n",
    "1. FAISS có sẵn pre-built wheels cho Windows\n",
    "2. Không cần C++ compiler\n",
    "3. Hiệu năng tốt và được sử dụng rộng rãi\n",
    "\n",
    "Các thay đổi cụ thể trong requirements.txt:\n",
    "```requirements.txt\n",
    "# Thay đổi trong phần LangChain & AI\n",
    "- langchain-chroma>=0.0.1\n",
    "+ langchain-faiss>=0.0.1\n",
    "\n",
    "# Thay đổi trong phần Vector Database & Search\n",
    "- chromadb>=0.4.0\n",
    "+ faiss-cpu>=1.7.4\n",
    "```\n",
    "\n",
    "Đây là giải pháp đơn giản nhất vì:\n",
    "1. Tránh phải cài đặt Visual Studio Build Tools\n",
    "2. Không cần xử lý conflicts về dependencies\n",
    "3. FAISS là một lựa chọn tốt cho vector store, được sử dụng rộng rãi trong production\n",
    "\n",
    "Nếu bạn vẫn muốn sử dụng Chroma, bạn sẽ cần:\n",
    "1. Cài đặt Microsoft Visual C++ Build Tools\n",
    "2. Đảm bảo version compatibility giữa pydantic và chromadb\n",
    "3. Rebuild lại virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Tìm hiểu về crawl data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Tôi thấy bạn đang cố chạy file `crawl_data.py` nhưng file thực tế là `crawl.py`. Đây là cách để chạy đúng quy trình:\n",
    "\n",
    "1. **Đầu tiên, chạy crawl.py để lấy dữ liệu**:\n",
    "```bash\n",
    "python crawl.py\n",
    "```\n",
    "\n",
    "File này sẽ:\n",
    "- Crawl dữ liệu từ `https://www.stack-ai.com/docs`\n",
    "- Lưu vào `data/stack.json`\n",
    "- Tham khảo hàm `main()` trong `crawl.py`:\n",
    "\n",
    "```81:92:src/crawl.py\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Hàm chính điều khiển luồng chương trình:\n",
    "    1. Crawl dữ liệu từ trang web stack-ai\n",
    "    2. Lưu dữ liệu đã crawl vào file JSON\n",
    "    3. In kết quả crawl để kiểm tra\n",
    "    \"\"\"\n",
    "    # Crawl dữ liệu từ trang docs của stack-ai\n",
    "    data = crawl_web('https://www.stack-ai.com/docs')\n",
    "    # Lưu dữ liệu vào thư mục data_v2\n",
    "    save_data_locally(data, 'stack.json', 'data')\n",
    "    print('data: ', data)  # In dữ liệu đã crawl\n",
    "```\n",
    "\n",
    "\n",
    "2. **Sau khi có dữ liệu, chạy seed_data.py để đưa vào Milvus**:\n",
    "```bash\n",
    "python seed_data.py\n",
    "```\n",
    "\n",
    "File này sẽ:\n",
    "- Đọc dữ liệu từ `data/stack.json`\n",
    "- Tạo embeddings và lưu vào Milvus\n",
    "- Tham khảo hàm `main()` trong `seed_data.py`:\n",
    "\n",
    "```150:167:src/seed_data.py\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Hàm chính để kiểm thử các chức năng của module\n",
    "    Thực hiện:\n",
    "        1. Test seed_milvus với dữ liệu từ file local 'stack.json'\n",
    "        2. (Đã comment) Test seed_milvus_live với dữ liệu từ trang web stack-ai\n",
    "    Chú ý:\n",
    "        - Đảm bảo Milvus server đang chạy tại localhost:19530\n",
    "        - Các biến môi trường cần thiết (như OPENAI_API_KEY) đã được cấu hình\n",
    "    \"\"\"\n",
    "    # Test seed_milvus với dữ liệu local\n",
    "    seed_milvus('http://localhost:19530', 'data_test', 'stack.json', 'data', use_ollama=False)\n",
    "    # Test seed_milvus_live với URL trực tiếp\n",
    "    # seed_milvus_live('https://www.stack-ai.com/docs', 'http://localhost:19530', 'data_test_live', 'stack-ai', use_ollama=False)\n",
    "# Chạy main() nếu file được thực thi trực tiếp\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "\n",
    "3. **Cuối cùng mới chạy ứng dụng Streamlit**:\n",
    "```bash\n",
    "streamlit run main.py\n",
    "```\n",
    "\n",
    "Lỗi \"Không tìm thấy documents trong collection 'data_test'\" xảy ra vì bạn chưa thực hiện bước 1 và 2 để tạo dữ liệu trước khi chạy ứng dụng.\n",
    "\n",
    "Tóm lại quy trình đầy đủ là:\n",
    "1. Crawl dữ liệu (`crawl.py`)\n",
    "2. Seed dữ liệu vào Milvus (`seed_data.py`)\n",
    "3. Chạy ứng dụng (`main.py) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Langchain \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run any LLM, Chat model, or Chain. Its trace will be sent to this project.\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Seed data vào Milvus trước khi run main.py\n",
    "\n",
    " Seed data vào Milvus:\n",
    " \n",
    "```python \n",
    "python seed_data.py\n",
    "```\n",
    "(Kiểm tra data đã aào Milvus chưa bằng cách truy cập: http://localhost:8000/#/databases/default/colletions\n",
    "<Nhớ để ý `docker run -p 8000:3000 -e MILVUS_URL={milvus server IP}:19530 zilliz/attu:v2.4` để chắc chắn Milvus đang hoạt động >)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "File `seed_data.py` có 4 nhiệm vụ chính:\n",
    "\n",
    "1. **Đọc dữ liệu từ file JSON local**:\n",
    "\n",
    "```13:29:src/seed_data.py\n",
    "def load_data_from_local(filename: str, directory: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Hàm đọc dữ liệu từ file JSON local\n",
    "    Args:\n",
    "        filename (str): Tên file JSON cần đọc (ví dụ: 'data.json')\n",
    "        directory (str): Thư mục chứa file (ví dụ: 'data_v3')\n",
    "    Returns:\n",
    "        tuple: Trả về (data, doc_name) trong đó:\n",
    "            - data: Dữ liệu JSON đã được parse\n",
    "            - doc_name: Tên tài liệu đã được xử lý (bỏ đuôi .json và thay '_' bằng khoảng trắng)\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    print(f'Data loaded from {file_path}')\n",
    "    # Chuyển tên file thành tên tài liệu (bỏ đuôi .json và thay '_' bằng khoảng trắng)\n",
    "    return data, filename.rsplit('.', 1)[0].replace('_', ' ')\n",
    "```\n",
    "\n",
    "- Đọc dữ liệu từ file JSON trong thư mục được chỉ định\n",
    "- Trả về dữ liệu và tên tài liệu đã được xử lý\n",
    "\n",
    "2. **Tạo vector embeddings từ dữ liệu local**:\n",
    "\n",
    "```31:84:src/seed_data.py\n",
    "def seed_milvus(URI_link: str, collection_name: str, filename: str, directory: str, use_ollama: bool = False) -> Milvus:\n",
    "    \"\"\"\n",
    "    Hàm tạo và lưu vector embeddings vào Milvus từ dữ liệu local\n",
    "    Args:\n",
    "        URI_link (str): Đường dẫn kết nối đến Milvus\n",
    "        collection_name (str): Tên collection trong Milvus để lưu dữ liệu\n",
    "        filename (str): Tên file JSON chứa dữ liệu nguồn\n",
    "        directory (str): Thư mục chứa file dữ liệu\n",
    "        use_ollama (bool): Sử dụng Ollama embeddings thay vì OpenAI\n",
    "    \"\"\"\n",
    "    # Khởi tạo model embeddings tùy theo lựa chọn\n",
    "    if use_ollama:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            model=\"llama2\"  # hoặc model khác mà bạn đã cài đặt\n",
    "        )\n",
    "    else:\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    \n",
    "    # Đọc dữ liệu từ file local\n",
    "    local_data, doc_name = load_data_from_local(filename, directory)\n",
    "\n",
    "    # Chuyển đổi dữ liệu thành danh sách các Document với giá trị mặc định cho các trường\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=doc.get('page_content') or '',\n",
    "            metadata={\n",
    "                'source': doc['metadata'].get('source') or '',\n",
    "                'content_type': doc['metadata'].get('content_type') or 'text/plain',\n",
    "                'title': doc['metadata'].get('title') or '',\n",
    "                'description': doc['metadata'].get('description') or '',\n",
    "                'language': doc['metadata'].get('language') or 'en',\n",
    "                'doc_name': doc_name,\n",
    "                'start_index': doc['metadata'].get('start_index') or 0\n",
    "            }\n",
    "        )\n",
    "        for doc in local_data\n",
    "    ]\n",
    "    print('documents: ', documents)\n",
    "\n",
    "    # Tạo ID duy nhất cho mỗi document\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "    # Khởi tạo và cấu hình Milvus\n",
    "    vectorstore = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        connection_args={\"uri\": URI_link},\n",
    "        collection_name=collection_name,\n",
    "        drop_old=True  # Xóa data đã tồn tại trong collection\n",
    "    )\n",
    "    # Thêm documents vào Milvus\n",
    "    vectorstore.add_documents(documents=documents, ids=uuids)\n",
    "    print('vector: ', vectorstore)\n",
    "    return vectorstore\n",
    "```\n",
    "\n",
    "- Chuyển đổi dữ liệu JSON thành các Document\n",
    "- Tạo embeddings bằng OpenAI hoặc Ollama\n",
    "- Lưu embeddings vào Milvus database\n",
    "\n",
    "3. **Crawl và tạo embeddings trực tiếp từ URL**:\n",
    "\n",
    "```86:129:src/seed_data.py\n",
    "def seed_milvus_live(URL: str, URI_link: str, collection_name: str, doc_name: str, use_ollama: bool = False) -> Milvus:\n",
    "    \"\"\"\n",
    "    Hàm crawl dữ liệu trực tiếp từ URL và tạo vector embeddings trong Milvus\n",
    "    Args:\n",
    "        URL (str): URL của trang web cần crawl dữ liệu\n",
    "        URI_link (str): Đường dẫn kết nối đến Milvus\n",
    "        collection_name (str): Tên collection trong Milvus\n",
    "        doc_name (str): Tên định danh cho tài liệu được crawl\n",
    "        use_ollama (bool): Sử dụng Ollama embeddings thay vì OpenAI\n",
    "    \"\"\"\n",
    "    if use_ollama:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            model=\"llama2\"  # hoặc model khác mà bạn đã cài đặt\n",
    "        )\n",
    "    else:\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    \n",
    "    documents = crawl_web(URL)\n",
    "\n",
    "    # Cập nhật metadata cho mỗi document với giá trị mặc định\n",
    "    for doc in documents:\n",
    "        metadata = {\n",
    "            'source': doc.metadata.get('source') or '',\n",
    "            'content_type': doc.metadata.get('content_type') or 'text/plain',\n",
    "            'title': doc.metadata.get('title') or '',\n",
    "            'description': doc.metadata.get('description') or '',\n",
    "            'language': doc.metadata.get('language') or 'en',\n",
    "            'doc_name': doc_name,\n",
    "            'start_index': doc.metadata.get('start_index') or 0\n",
    "        }\n",
    "        doc.metadata = metadata\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "    vectorstore = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        connection_args={\"uri\": URI_link},\n",
    "        collection_name=collection_name,\n",
    "        drop_old=True\n",
    "    )\n",
    "    vectorstore.add_documents(documents=documents, ids=uuids)\n",
    "    print('vector: ', vectorstore)\n",
    "    return vectorstore\n",
    "```\n",
    "\n",
    "- Crawl dữ liệu từ URL được chỉ định\n",
    "- Xử lý metadata và tạo embeddings\n",
    "- Lưu vào Milvus database\n",
    "\n",
    "4. **Kết nối tới Milvus database**:\n",
    "\n",
    "```130:148:src/seed_data.py\n",
    "def connect_to_milvus(URI_link: str, collection_name: str) -> Milvus:\n",
    "    \"\"\"\n",
    "    Hàm kết nối đến collection có sẵn trong Milvus\n",
    "    Args:\n",
    "        URI_link (str): Đường dẫn kết nối đến Milvus\n",
    "        collection_name (str): Tên collection cần kết nối\n",
    "    Returns:\n",
    "        Milvus: Đối tượng Milvus đã được kết nối, sẵn sàng để truy vấn\n",
    "    Chú ý:\n",
    "        - Không tạo collection mới hoặc xóa dữ liệu cũ\n",
    "        - Sử dụng model 'text-embedding-3-large' cho việc tạo embeddings khi truy vấn\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vectorstore = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        connection_args={\"uri\": URI_link},\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    return vectorstore\n",
    "```\n",
    "\n",
    "- Tạo kết nối tới collection có sẵn trong Milvus\n",
    "- Sử dụng để truy vấn dữ liệu\n",
    "\n",
    "Tóm lại, file này đóng vai trò quan trọng trong việc:\n",
    "- Chuẩn bị dữ liệu cho chatbot\n",
    "- Tạo và quản lý vector database (Milvus)\n",
    "- Cung cấp các hàm tiện ích để xử lý dữ liệu\n",
    "- Là cầu nối giữa dữ liệu thô và hệ thống truy vấn vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu bỏ qua bước seed data:'\n",
    "\n",
    "\n",
    "- Thiếu dữ liệu cho vector search:\n",
    "- Hàm get_retriever() trong local_ollama.py sẽ không tìm thấy documents trong Milvus\n",
    "- Dẫn đến lỗi \"Không tìm thấy documents trong collection 'data_test'\"\n",
    "\n",
    "- Chatbot không có dữ liệu để tìm kiếm thông tin liên quan\n",
    "- Không thể trả lời các câu hỏi dựa trên context\n",
    "- Chỉ có thể trả lời dựa trên kiến thức có sẵn của model, không có khả năng RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tôi sẽ giải thích luồng hoạt động chính của code:\n",
    "\n",
    "1. **Khởi động ứng dụng** (trong main.py):\n",
    "\n",
    "```199:220:src/main.py\n",
    "# === HÀM CHÍNH ===\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Hàm chính điều khiển luồng chương trình\n",
    "    \"\"\"\n",
    "    initialize_app()\n",
    "    model_choice = setup_sidebar()  # Chỉ cần trả về model choice để xử lý chat\n",
    "    msgs = setup_chat_interface(model_choice)\n",
    "    \n",
    "    # Khởi tạo AI dựa trên lựa chọn model để trả lời\n",
    "    if model_choice == \"OpenAI GPT\":\n",
    "        retriever = get_openai_retriever()\n",
    "        agent_executor = get_openai_agent(retriever)\n",
    "    else:\n",
    "        retriever = get_ollama_retriever()\n",
    "        agent_executor = get_ollama_agent(retriever)\n",
    "    \n",
    "    handle_user_input(msgs, agent_executor)\n",
    "\n",
    "# Chạy ứng dụng\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "- Khởi tạo app với `initialize_app()`\n",
    "- Tạo sidebar để cấu hình với `setup_sidebar()`\n",
    "- Tạo giao diện chat với `setup_chat_interface()`\n",
    "- Khởi tạo AI agent dựa vào lựa chọn model (OpenAI hoặc Ollama)\n",
    "\n",
    "2. **Xử lý tin nhắn người dùng** (trong main.py):\n",
    "\n",
    "```160:197:src/main.py\n",
    "# === XỬ LÝ TIN NHẮN NGƯỜI DÙNG ===\n",
    "def handle_user_input(msgs, agent_executor):\n",
    "    \"\"\"\n",
    "    Xử lý khi người dùng gửi tin nhắn:\n",
    "    1. Hiển thị tin nhắn người dùng\n",
    "    2. Gọi AI xử lý và trả lời\n",
    "    3. Lưu vào lịch sử chat\n",
    "    \"\"\"\n",
    "    if prompt := st.chat_input(\"Hãy hỏi tôi bất cứ điều gì về Stack AI!\"):\n",
    "        # Lưu và hiển thị tin nhắn người dùng\n",
    "        st.session_state.messages.append({\"role\": \"human\", \"content\": prompt})\n",
    "        st.chat_message(\"human\").write(prompt)\n",
    "        msgs.add_user_message(prompt)\n",
    "\n",
    "        # Xử lý và hiển thị câu trả lời\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st_callback = StreamlitCallbackHandler(st.container())\n",
    "            \n",
    "            # Lấy lịch sử chat\n",
    "            chat_history = [\n",
    "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
    "                for msg in st.session_state.messages[:-1]\n",
    "            ]\n",
    "\n",
    "            # Gọi AI xử lý\n",
    "            response = agent_executor.invoke(\n",
    "                {\n",
    "                    \"input\": prompt,\n",
    "                    \"chat_history\": chat_history\n",
    "                },\n",
    "                {\"callbacks\": [st_callback]}\n",
    "            )\n",
    "\n",
    "            # Lưu và hiển thị câu trả lời\n",
    "            output = response[\"output\"]\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": output})\n",
    "            msgs.add_ai_message(output)\n",
    "            st.write(output)\n",
    "```\n",
    "\n",
    "- Khi người dùng nhập tin nhắn\n",
    "- Lưu tin nhắn vào lịch sử\n",
    "- Gọi AI agent để xử lý với context từ lịch sử chat\n",
    "- Hiển thị câu trả lời\n",
    "\n",
    "3. **Cấu hình AI Agent** (trong agent.py và local_ollama.py):\n",
    "\n",
    "Với OpenAI:\n",
    "\n",
    "```64:95:src/agent.py\n",
    "def get_llm_and_agent(_retriever) -> AgentExecutor:\n",
    "    \"\"\"\n",
    "    Khởi tạo Language Model và Agent với cấu hình cụ thể\n",
    "    Args:\n",
    "        _retriever: Retriever đã được cấu hình để tìm kiếm thông tin\n",
    "    Returns:\n",
    "        AgentExecutor: Agent đã được cấu hình với:\n",
    "            - Model: GPT-4\n",
    "            - Temperature: 0\n",
    "            - Streaming: Enabled\n",
    "            - Custom system prompt\n",
    "    Chú ý:\n",
    "        - Yêu cầu OPENAI_API_KEY đã được cấu hình\n",
    "        - Agent được thiết lập với tên \"ChatchatAI\"\n",
    "        - Sử dụng chat history để duy trì ngữ cảnh hội thoại\n",
    "    \"\"\"\n",
    "    # Khởi tạo ChatOpenAI với chế độ streaming\n",
    "    llm = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4o-mini\")\n",
    "    tools = [tool]\n",
    "    \n",
    "    # Thiết lập prompt template cho agent\n",
    "    system = \"\"\"You are an expert at AI. Your name is ChatchatAI.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "\n",
    "    # Tạo và trả về agent\n",
    "    agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    return AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "```\n",
    "\n",
    "- Sử dụng GPT-4\n",
    "- Temperature = 0 (ít sáng tạo, tập trung vào thông tin chính xác)\n",
    "- Streaming = True (hiển thị từng từ một)\n",
    "\n",
    "Với Ollama:\n",
    "\n",
    "```62:93:src/local_ollama.py\n",
    "def get_llm_and_agent(retriever):\n",
    "    \"\"\"\n",
    "    Khởi tạo LLM và agent với Ollama\n",
    "    \"\"\"\n",
    "    # Tạo retriever tool\n",
    "    tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"find_documents\",\n",
    "        \"Search for information of Stack AI.\"\n",
    "    )\n",
    "\n",
    "    # Khởi tạo ChatOllama\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama2\",  # hoặc model khác tùy chọn\n",
    "        temperature=0,\n",
    "        streaming=True\n",
    "    )\n",
    "\n",
    "    tools = [tool]\n",
    "\n",
    "    # Thiết lập prompt template\n",
    "    system = \"\"\"You are an expert at AI. Your name is ChatchatAI. For Stack AI questions call the find_document tool\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "\n",
    "    # Tạo agent\n",
    "    agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    return AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "```\n",
    "\n",
    "- Sử dụng LLaMA2 \n",
    "- Cấu hình tương tự OpenAI\n",
    "- Chạy locally\n",
    "\n",
    "4. **Xử lý dữ liệu** (trong seed_data.py và crawl.py):\n",
    "\n",
    "Crawl dữ liệu:\n",
    "\n",
    "```21:39:src/crawl.py\n",
    "def crawl_web(url_data):\n",
    "    \"\"\"\n",
    "    Hàm crawl dữ liệu từ URL với chế độ đệ quy\n",
    "    Args:\n",
    "        url_data (str): URL gốc để bắt đầu crawl\n",
    "    Returns:\n",
    "        list: Danh sách các Document object, mỗi object chứa nội dung đã được chia nhỏ\n",
    "              và metadata tương ứng\n",
    "    \"\"\"\n",
    "    # Tạo loader với độ sâu tối đa là 4 cấp\n",
    "    loader = RecursiveUrlLoader(url=url_data, extractor=bs4_extractor, max_depth=4)\n",
    "    docs = loader.load()  # Tải nội dung\n",
    "    print('length: ', len(docs))  # In số lượng tài liệu đã tải\n",
    "    \n",
    "    # Chia nhỏ văn bản thành các đoạn 10000 ký tự, với 500 ký tự chồng lấp\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=500)\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "    print('length_all_splits: ', len(all_splits))  # In số lượng đoạn văn bản sau khi chia\n",
    "    return all_splits\n",
    "```\n",
    "\n",
    "- Crawl từ URL với độ sâu tối đa 4 cấp\n",
    "- Chia nhỏ văn bản thành các đoạn 10000 ký tự\n",
    "- Overlap 500 ký tự để duy trì ngữ cảnh\n",
    "\n",
    "Lưu vào Milvus:\n",
    "\n",
    "```130:148:src/seed_data.py\n",
    "def connect_to_milvus(URI_link: str, collection_name: str) -> Milvus:\n",
    "    \"\"\"\n",
    "    Hàm kết nối đến collection có sẵn trong Milvus\n",
    "    Args:\n",
    "        URI_link (str): Đường dẫn kết nối đến Milvus\n",
    "        collection_name (str): Tên collection cần kết nối\n",
    "    Returns:\n",
    "        Milvus: Đối tượng Milvus đã được kết nối, sẵn sàng để truy vấn\n",
    "    Chú ý:\n",
    "        - Không tạo collection mới hoặc xóa dữ liệu cũ\n",
    "        - Sử dụng model 'text-embedding-3-large' cho việc tạo embeddings khi truy vấn\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vectorstore = Milvus(\n",
    "        embedding_function=embeddings,\n",
    "        connection_args={\"uri\": URI_link},\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    return vectorstore\n",
    "```\n",
    "\n",
    "- Tạo embeddings với OpenAI hoặc Ollama\n",
    "- Lưu vào Milvus database để tìm kiếm vector\n",
    "- Kết nối qua localhost:19530\n",
    "\n",
    "Toàn bộ hệ thống hoạt động theo mô hình RAG (Retrieval Augmented Generation):\n",
    "1. Tìm thông tin liên quan từ Milvus\n",
    "2. Kết hợp với prompt và lịch sử chat\n",
    "3. Gửi cho AI model để tạo câu trả lời\n",
    "4. Hiển thị kết quả theo dạng streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Chunk and Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk như nào ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa vào codebase, tôi thấy việc chunk dataset trong RAG được thực hiện qua RecursiveCharacterTextSplitter với các thông số cụ thể:\n",
    "```python\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,  # Mỗi chunk có độ dài 10000 ký tự\n",
    "    chunk_overlap=500  # Overlap 500 ký tự giữa các chunk liền kề\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "```\n",
    "Copy\n",
    "\n",
    "Apply\n",
    "\n",
    "src\\crawl.py\n",
    "Các điểm chính về chunking strategy:\n",
    "\n",
    "Sử dụng RecursiveCharacterTextSplitter - một phương pháp chunk thông minh theo ký tự, đảm bảo không cắt giữa câu/đoạn văn\n",
    "\n",
    "Kích thước mỗi chunk là 10000 ký tự - đủ lớn để giữ ngữ cảnh nhưng không quá lớn để tối ưu việc embedding\n",
    "\n",
    "Overlap 500 ký tự - giúp duy trì tính liên tục của thông tin giữa các chunk\n",
    "\n",
    "Sau khi chunk, dữ liệu được lưu vào Milvus vector store với metadata đầy đủ như source, content_type, title, description, language, doc_name và start_index\n",
    "\n",
    "Đây là cách tiếp cận hiệu quả để chuẩn bị dữ liệu cho RAG system.\n",
    "\n",
    "Try again with different c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding lưu ở đâu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa vào codebase, tôi thấy việc chunk dataset trong RAG được thực hiện qua RecursiveCharacterTextSplitter với các thông số cụ thể:\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,  # Mỗi chunk có độ dài 10000 ký tự\n",
    "    chunk_overlap=500  # Overlap 500 ký tự giữa các chunk liền kề\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "Copy\n",
    "\n",
    "Apply\n",
    "\n",
    "src\\crawl.py\n",
    "Các điểm chính về chunking strategy:\n",
    "\n",
    "Sử dụng RecursiveCharacterTextSplitter - một phương pháp chunk thông minh theo ký tự, đảm bảo không cắt giữa câu/đoạn văn\n",
    "\n",
    "Kích thước mỗi chunk là 10000 ký tự - đủ lớn để giữ ngữ cảnh nhưng không quá lớn để tối ưu việc embedding\n",
    "\n",
    "Overlap 500 ký tự - giúp duy trì tính liên tục của thông tin giữa các chunk\n",
    "\n",
    "Sau khi chunk, dữ liệu được lưu vào Milvus vector store với metadata đầy đủ như source, content_type, title, description, language, doc_name và start_index\n",
    "\n",
    "Đây là cách tiếp cận hiệu quả để chuẩn bị dữ liệu cho RAG system.\n",
    "\n",
    "Try again with different c"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
