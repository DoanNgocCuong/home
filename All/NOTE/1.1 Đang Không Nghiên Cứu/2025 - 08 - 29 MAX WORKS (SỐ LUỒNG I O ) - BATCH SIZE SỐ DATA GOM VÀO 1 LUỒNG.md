
|TiÃªu chÃ­|Batch size|Max workers|
|---|---|---|
|Báº£n cháº¥t|KÃ­ch thÆ°á»›c nhÃ³m dá»¯ liá»‡u xá»­ lÃ½ trong 1 láº§n|Sá»‘ worker xá»­ lÃ½ Ä‘á»“ng thá»i|
|á»¨ng dá»¥ng|Training ML, xá»­ lÃ½ dá»¯ liá»‡u batch|Parallelism trong há»‡ thá»‘ng|
|TÃ¡c Ä‘á»™ng|áº¢nh hÆ°á»Ÿng Ä‘áº¿n memory vÃ  convergence|áº¢nh hÆ°á»Ÿng Ä‘áº¿n throughput, CPU core usage|
|LiÃªn quan|Gradient update, I/O batch|Thread, process, concurrency|


```PYTHON 
# @title OPENAI

import os

import json

import pandas as pd

import time

import logging

import re

import concurrent.futures

from typing import List, Dict

import math

from openpyxl import load_workbook

import psutil

import multiprocessing

import threading

from pathlib import Path

import argparse

from dotenv import load_dotenv

  

# ============= FIX PROXY ISSUE =============

# BÆ°á»›c 1: XÃ³a táº¥t cáº£ proxy environment variables

proxy_vars = [

Â  Â  'HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy',

Â  Â  'ALL_PROXY', 'all_proxy', 'NO_PROXY', 'no_proxy'

]

  

for var in proxy_vars:

Â  Â  if var in os.environ:

Â  Â  Â  Â  del os.environ[var]

Â  Â  Â  Â  print(f"Removed proxy variable: {var}")

  

# BÆ°á»›c 2: Set explicit no proxy

os.environ['NO_PROXY'] = '*'

  

# BÆ°á»›c 3: Import OpenAI sau khi clear proxy

try:

Â  Â  from openai import OpenAI

Â  Â  from openai import OpenAIError

Â  Â  print("âœ“ OpenAI imported successfully")

except ImportError as e:

Â  Â  print(f"âœ— Error importing OpenAI: {e}")

Â  Â  print("Run: pip install openai>=1.0.0")

Â  Â  exit(1)

  

# Setup logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

logger = logging.getLogger(__name__)

  

# Load environment variables

load_dotenv()

  

# BÆ°á»›c 4: Khá»Ÿi táº¡o OpenAI client vá»›i error handling

def create_openai_client():

Â  Â  api_key = os.getenv('OPENAI_API_KEY')

Â  Â  if not api_key:

Â  Â  Â  Â  raise ValueError("OPENAI_API_KEY not found in environment variables")

Â  Â  print(f"API Key: {api_key[:10]}...")

Â  Â  try:

Â  Â  Â  Â  # Táº¡o client chá»‰ vá»›i api_key (khÃ´ng cÃ³ parameters khÃ¡c)

Â  Â  Â  Â  client = OpenAI(api_key=api_key)

Â  Â  Â  Â  print("âœ“ OpenAI client created successfully")

Â  Â  Â  Â  return client

Â  Â  except Exception as e:

Â  Â  Â  Â  print(f"âœ— Error creating OpenAI client: {e}")

Â  Â  Â  Â  print("Trying alternative initialization...")

Â  Â  Â  Â  # Alternative: Táº¡o client vá»›i httpx explicit

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  import httpx

Â  Â  Â  Â  Â  Â  http_client = httpx.Client(proxies=None)

Â  Â  Â  Â  Â  Â  client = OpenAI(api_key=api_key, http_client=http_client)

Â  Â  Â  Â  Â  Â  print("âœ“ OpenAI client created with custom HTTP client")

Â  Â  Â  Â  Â  Â  return client

Â  Â  Â  Â  except Exception as e2:

Â  Â  Â  Â  Â  Â  print(f"âœ— Alternative initialization failed: {e2}")

Â  Â  Â  Â  Â  Â  raise e

  

# Khá»Ÿi táº¡o client

client = create_openai_client()

# ============= END FIX =============

  

sheet_name = 'Trang tÃ­nh1'

  

def process_conversation(order, base_prompt, inputs, conversation_history=None):

Â  Â  print(f"\n=== Processing Conversation ===")

Â  Â  print(f"Order: {order}")

Â  Â  print(f"Base Prompt: {base_prompt[:100]}...")

Â  Â  # Log conversation history

Â  Â  if conversation_history:

Â  Â  Â  Â  logger.info(f"Conversation history: {conversation_history}")

  

Â  Â  # Táº¡o model config dÆ°á»›i dáº¡ng JSON

Â  Â  model_config = {

Â  Â  Â  Â  "model": "gpt-4o-mini-2024-07-18",

Â  Â  Â  Â  "temperature": 0,

Â  Â  Â  Â  "max_tokens": 2048,

Â  Â  Â  Â  "top_p": 1,

Â  Â  Â  Â  "frequency_penalty": 0.0,

Â  Â  Â  Â  "presence_penalty": 0.0

Â  Â  }

Â  Â  model_config_json = json.dumps(model_config)

Â  Â  responses = []

Â  Â  response_times = []

Â  Â  chat_messages = []

Â  Â  # 1. System message

Â  Â  chat_messages.append({"role": "system", "content": base_prompt})

Â  Â  print("\nSau khi thÃªm system message:")

Â  Â  print(chat_messages)

Â  Â  # 2. History handling

Â  Â  if conversation_history and not pd.isna(conversation_history):

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  # Parse conversation history from JSON string

Â  Â  Â  Â  Â  Â  history_messages = json.loads(conversation_history)

Â  Â  Â  Â  Â  Â  # Validate format of history messages

Â  Â  Â  Â  Â  Â  if isinstance(history_messages, list):

Â  Â  Â  Â  Â  Â  Â  Â  for msg in history_messages:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if isinstance(msg, dict) and 'role' in msg and 'content' in msg:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chat_messages.append(msg)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Warning: Skipping invalid message format in history: {msg}")

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  print(f"Warning: conversation_history is not a list: {history_messages}")

Â  Â  Â  Â  Â  Â  print("\nSau khi thÃªm history:")

Â  Â  Â  Â  Â  Â  print(json.dumps(chat_messages, indent=2, ensure_ascii=False))

Â  Â  Â  Â  except json.JSONDecodeError as e:

Â  Â  Â  Â  Â  Â  print(f"Error parsing conversation history: {e}")

Â  Â  Â  Â  Â  Â  print(f"Raw conversation history: {conversation_history}")

Â  Â  Â  Â  Â  Â  logger.error(f"Error parsing conversation history: {e}")

Â  Â  Â  Â  Â  Â  logger.error(f"Raw conversation history: {conversation_history}")

Â  Â  # 3. New input

Â  Â  for user_input in inputs:

Â  Â  Â  Â  chat_messages.append({"role": "user", "content": user_input})

Â  Â  Â  Â  print("\nTrÆ°á»›c khi gá»i API:")

Â  Â  Â  Â  print(json.dumps(chat_messages, indent=2, ensure_ascii=False))

Â  Â  Â  Â  start_time = time.time()

Â  Â  Â  Â  try_count = 0

Â  Â  Â  Â  while try_count < 3:

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  print(f"DEBUG - Attempt {try_count + 1} to call OpenAI API")

Â  Â  Â  Â  Â  Â  Â  Â  completion = client.chat.completions.create(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model=model_config["model"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  messages=chat_messages, Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temperature=model_config["temperature"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_tokens=model_config["max_tokens"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  top_p=model_config["top_p"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  frequency_penalty=model_config["frequency_penalty"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  presence_penalty=model_config["presence_penalty"]

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  end_time = time.time()

Â  Â  Â  Â  Â  Â  Â  Â  response_content = completion.choices[0].message.content

Â  Â  Â  Â  Â  Â  Â  Â  chat_messages.append({"role": "assistant", "content": response_content})

  

Â  Â  Â  Â  Â  Â  Â  Â  responses.append(response_content)

Â  Â  Â  Â  Â  Â  Â  Â  response_times.append(end_time - start_time)

  

Â  Â  Â  Â  Â  Â  Â  Â  # Print the completion output here

Â  Â  Â  Â  Â  Â  Â  Â  print(f"Order {order}, Input: '{user_input}', Response: '{response_content[:100]}...', Time: {end_time - start_time:.2f}s\n====")

Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  try_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  print(f"DEBUG - Error on attempt {try_count}: {str(e)}")

Â  Â  Â  Â  Â  Â  Â  Â  if try_count >= 3:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  responses.append("Request failed after 3 retries.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_times.append("-")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Order {order}, Input: '{user_input}', Response: 'Request failed after 3 retries.', Time: -")

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"DEBUG - Waiting 3 seconds before retry...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(3)

  

Â  Â  return responses, response_times, chat_messages, model_config_json

  

# Add argument parser

def parse_arguments():

Â  Â  parser = argparse.ArgumentParser(description='Process conversations with OpenAI API')

Â  Â  parser.add_argument('--num-rows', type=int, default=None,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help='Number of rows to process (default: all rows)')

Â  Â  parser.add_argument('--input-file', type=str, default='input_data.xlsx',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help='Input Excel file path (default: input_data.xlsx)')

Â  Â  parser.add_argument('--output-file', type=str, default='output_data_v2.xlsx',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help='Output Excel file path (default: output_data_v2.xlsx)')

Â  Â  parser.add_argument('--sheet', type=str, default='Trang tÃ­nh1',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help='Excel sheet name to process (default: Trang tÃ­nh1)')

Â  Â  parser.add_argument('--batch-size', type=int, default=4,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help='Number of items to process in each batch (default: 4)')

Â  Â  parser.add_argument('--max-workers', type=int, default=4,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help='Maximum number of worker threads (default: 4)')

Â  Â  return parser.parse_args()

  

def process_batch(batch_rows: List[Dict]):

Â  Â  output_rows = []

Â  Â  for row in batch_rows:

Â  Â  Â  Â  order = row.get('order', 'default_order')

Â  Â  Â  Â  prompt = row['system_prompt']

Â  Â  Â  Â  conversation_history = row.get('conversation_history', None)

Â  Â  Â  Â  inputs = [row['user_input']]

Â  Â  Â  Â  logger.info(f"Processing order {order}")

Â  Â  Â  Â  responses, response_times, chat_messages, model_config = process_conversation(

Â  Â  Â  Â  Â  Â  order, prompt, inputs, conversation_history

Â  Â  Â  Â  )

Â  Â  Â  Â  # Copy all columns and add new data

Â  Â  Â  Â  new_row = row.copy()

Â  Â  Â  Â  new_row.update({

Â  Â  Â  Â  Â  Â  'assistant_response': responses[0] if responses else None,

Â  Â  Â  Â  Â  Â  'response_time': response_times[0] if response_times else None,

Â  Â  Â  Â  Â  Â  'model_config': model_config

Â  Â  Â  Â  })

Â  Â  Â  Â  output_rows.append(new_row)

Â  Â  return output_rows

  

def append_to_excel(filename, df):

Â  Â  """Append DataFrame to existing Excel file without reading entire file."""

Â  Â  try:

Â  Â  Â  Â  with pd.ExcelWriter(filename, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:

Â  Â  Â  Â  Â  Â  workbook = writer.book

Â  Â  Â  Â  Â  Â  worksheet = workbook['Sheet1']

Â  Â  Â  Â  Â  Â  start_row = worksheet.max_row

Â  Â  Â  Â  Â  Â  df.to_excel(writer,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â index=False,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â header=False,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â startrow=start_row)

Â  Â  Â  Â  return True

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"Error appending to Excel: {str(e)}")

Â  Â  Â  Â  return False

  

def get_system_resources():

Â  Â  """Láº¥y thÃ´ng tin tÃ i nguyÃªn há»‡ thá»‘ng."""

Â  Â  try:

Â  Â  Â  Â  cpu_count = multiprocessing.cpu_count()

Â  Â  Â  Â  memory = psutil.virtual_memory()

Â  Â  Â  Â  available_memory_gb = memory.available / (1024 * 1024 * 1024)

Â  Â  Â  Â  cpu_percent = psutil.cpu_percent(interval=1)

Â  Â  Â  Â  logger.info(f"System resources:")

Â  Â  Â  Â  logger.info(f"- CPU cores: {cpu_count}")

Â  Â  Â  Â  logger.info(f"- CPU usage: {cpu_percent}%")

Â  Â  Â  Â  logger.info(f"- Available memory: {available_memory_gb:.2f}GB")

Â  Â  Â  Â  logger.info(f"- Memory usage: {memory.percent}%")

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  'cpu_count': cpu_count,

Â  Â  Â  Â  Â  Â  'available_memory_gb': available_memory_gb,

Â  Â  Â  Â  Â  Â  'cpu_percent': cpu_percent,

Â  Â  Â  Â  Â  Â  'memory_percent': memory.percent

Â  Â  Â  Â  }

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"Error getting system resources: {str(e)}")

Â  Â  Â  Â  return None

  

def optimize_batch_parameters(total_rows: int, args):

Â  Â  """Tá»‘i Æ°u batch_size vÃ  max_workers dá»±a trÃªn tÃ i nguyÃªn há»‡ thá»‘ng."""

Â  Â  resources = get_system_resources()

Â  Â  if not resources:

Â  Â  Â  Â  logger.warning("Using default parameters due to resource check failure")

Â  Â  Â  Â  return args.batch_size, args.max_workers

Â  Â  # Tá»‘i Æ°u max_workers

Â  Â  recommended_workers = min(

Â  Â  Â  Â  resources['cpu_count'],

Â  Â  Â  Â  int(8 * (100 - resources['cpu_percent']) / 100),

Â  Â  Â  Â  int(resources['available_memory_gb'])

Â  Â  )

Â  Â  max_workers = max(1, min(recommended_workers, args.max_workers))

Â  Â  # Tá»‘i Æ°u batch_size

Â  Â  memory_based_batch = int(resources['available_memory_gb'] * 1024)

Â  Â  cpu_based_batch = int(20 * (100 - resources['cpu_percent']) / 100)

Â  Â  recommended_batch = min(

Â  Â  Â  Â  memory_based_batch,

Â  Â  Â  Â  cpu_based_batch,

Â  Â  Â  Â  max(1, total_rows // (max_workers * 2))

Â  Â  )

Â  Â  batch_size = max(1, min(recommended_batch, args.batch_size))

Â  Â  logger.info(f"Optimized parameters:")

Â  Â  logger.info(f"- Original batch_size: {args.batch_size}, max_workers: {args.max_workers}")

Â  Â  logger.info(f"- Recommended batch_size: {batch_size}, max_workers: {max_workers}")

Â  Â  return batch_size, max_workers

  

def monitor_resources(stop_event, interval=30):

Â  Â  """Monitor tÃ i nguyÃªn há»‡ thá»‘ng trong quÃ¡ trÃ¬nh xá»­ lÃ½."""

Â  Â  while not stop_event.is_set():

Â  Â  Â  Â  resources = get_system_resources()

Â  Â  Â  Â  if resources:

Â  Â  Â  Â  Â  Â  if resources['cpu_percent'] > 90:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning("High CPU usage detected!")

Â  Â  Â  Â  Â  Â  if resources['memory_percent'] > 90:

Â  Â  Â  Â  Â  Â  Â  Â  logger.warning("High memory usage detected!")

Â  Â  Â  Â  time.sleep(interval)

  

def main():

Â  Â  args = parse_arguments()

Â  Â  SCRIPTS_FOLDER = Path(__file__).parent

Â  Â  INPUT_FILE = SCRIPTS_FOLDER / args.input_file

Â  Â  OUTPUT_FILE = SCRIPTS_FOLDER / args.output_file

  

Â  Â  # Kiá»ƒm tra file input tá»“n táº¡i

Â  Â  if not INPUT_FILE.exists():

Â  Â  Â  Â  logger.error(f"Input file not found: {INPUT_FILE}")

Â  Â  Â  Â  return

  

Â  Â  df_input = pd.read_excel(INPUT_FILE, sheet_name=args.sheet)

Â  Â  rows_to_process = df_input if args.num_rows is None else df_input.head(args.num_rows)

Â  Â  logger.info(f"Processing {len(rows_to_process)} rows from {INPUT_FILE}")

Â  Â  # Tá»‘i Æ°u parameters

Â  Â  batch_size, max_workers = optimize_batch_parameters(len(rows_to_process), args)

Â  Â  all_rows = rows_to_process.to_dict('records')

Â  Â  batches = [all_rows[i:i + batch_size] for i in range(0, len(all_rows), batch_size)]

Â  Â  logger.info(f"Created {len(batches)} batches with batch_size={batch_size}, max_workers={max_workers}")

Â  Â  # Táº¡o file output vá»›i headers

Â  Â  cols_order = list(df_input.columns) + ['assistant_response', 'response_time', 'model_config']

Â  Â  pd.DataFrame(columns=cols_order).to_excel(OUTPUT_FILE, index=False)

Â  Â  # Khá»Ÿi Ä‘á»™ng monitor thread

Â  Â  stop_monitoring = threading.Event()

Â  Â  monitor_thread = threading.Thread(target=monitor_resources, args=(stop_monitoring,))

Â  Â  monitor_thread.start()

Â  Â  try:

Â  Â  Â  Â  processed_count = 0

Â  Â  Â  Â  failed_batches = []

  

Â  Â  Â  Â  with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:

Â  Â  Â  Â  Â  Â  futures = [executor.submit(process_batch, batch) for batch in batches]

Â  Â  Â  Â  Â  Â  for i, future in enumerate(concurrent.futures.as_completed(futures)):

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_results = future.result()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_df = pd.DataFrame(batch_results)[cols_order]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for attempt in range(3):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if append_to_excel(OUTPUT_FILE, batch_df):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processed_count += len(batch_results)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Appended batch {i+1}/{len(batches)}. Total processed: {processed_count}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if attempt < 2:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.warning(f"Retry {attempt + 1} for batch {i+1}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(2)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Failed to append batch {i+1} after 3 attempts")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  failed_batches.append((i, batch_results))

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Error processing batch {i+1}: {str(e)}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  failed_batches.append((i, None))

  

Â  Â  Â  Â  # Xá»­ lÃ½ láº¡i cÃ¡c batch tháº¥t báº¡i

Â  Â  Â  Â  if failed_batches:

Â  Â  Â  Â  Â  Â  logger.info(f"Retrying {len(failed_batches)} failed batches...")

Â  Â  Â  Â  Â  Â  for batch_index, batch_results in failed_batches:

Â  Â  Â  Â  Â  Â  Â  Â  if batch_results:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_df = pd.DataFrame(batch_results)[cols_order]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if append_to_excel(OUTPUT_FILE, batch_df):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processed_count += len(batch_results)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Successfully retried batch {batch_index+1}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logger.error(f"Permanently failed to append batch {batch_index+1}")

  

Â  Â  except KeyboardInterrupt:

Â  Â  Â  Â  logger.info("Process interrupted by user")

Â  Â  except Exception as e:

Â  Â  Â  Â  logger.error(f"Unexpected error: {str(e)}")

Â  Â  finally:

Â  Â  Â  Â  # Dá»«ng monitor thread

Â  Â  Â  Â  stop_monitoring.set()

Â  Â  Â  Â  monitor_thread.join()

Â  Â  Â  Â  # Log káº¿t quáº£ cuá»‘i cÃ¹ng

Â  Â  Â  Â  logger.info(f"Processing completed. Total rows processed: {processed_count}")

Â  Â  Â  Â  logger.info(f"Output saved to: {OUTPUT_FILE}")

Â  Â  Â  Â  get_system_resources()

  

if __name__ == "__main__":

Â  Â  main()

```

```python 
#!/usr/bin/env python3

# -*- coding: utf-8 -*-

"""

Parallel Processing Module for Main Pipeline

===========================================

  

Features:

- Adaptive batch sizing based on data complexity

- Smart worker management with performance monitoring

- Thread-safe result collection

- Progress tracking and ETA calculation

- Error handling and retry mechanisms

- Memory usage optimization

"""

  

import threading

import time

from concurrent.futures import ThreadPoolExecutor, as_completed

from datetime import datetime, timedelta

from typing import List, Dict, Any, Callable, Optional

import psutil

import math

  

class ParallelProcessor:

Â  Â  """

Â  Â  Advanced parallel processor with adaptive optimization

Â  Â  """

Â  Â  def __init__(self,

Â  Â  Â  Â  Â  Â  Â  Â  Â max_workers: Optional[int] = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â batch_size: Optional[int] = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â memory_limit_mb: int = 1024,

Â  Â  Â  Â  Â  Â  Â  Â  Â enable_adaptive: bool = True,

Â  Â  Â  Â  Â  Â  Â  Â  Â timeout_seconds: int = 300):

Â  Â  Â  Â  """

Â  Â  Â  Â  Initialize parallel processor

Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  max_workers: Maximum number of worker threads (None = auto-detect)

Â  Â  Â  Â  Â  Â  batch_size: Items per batch (None = auto-calculate)

Â  Â  Â  Â  Â  Â  memory_limit_mb: Memory limit in MB

Â  Â  Â  Â  Â  Â  enable_adaptive: Enable adaptive optimization

Â  Â  Â  Â  """

Â  Â  Â  Â  self.max_workers = max_workers or self._auto_detect_workers()

Â  Â  Â  Â  self.batch_size = batch_size

Â  Â  Â  Â  self.memory_limit_mb = memory_limit_mb

Â  Â  Â  Â  self.enable_adaptive = enable_adaptive

Â  Â  Â  Â  self.timeout_seconds = timeout_seconds

Â  Â  Â  Â  # Performance tracking

Â  Â  Â  Â  self.start_time = None

Â  Â  Â  Â  self.completed_items = 0

Â  Â  Â  Â  self.total_items = 0

Â  Â  Â  Â  self.error_count = 0

Â  Â  Â  Â  self.retry_count = 0

Â  Â  Â  Â  # Thread safety

Â  Â  Â  Â  self.lock = threading.Lock()

Â  Â  Â  Â  self.results = []

Â  Â  Â  Â  # Memory monitoring

Â  Â  Â  Â  self.process = psutil.Process()

Â  Â  def _auto_detect_workers(self) -> int:

Â  Â  Â  Â  """Auto-detect optimal number of workers based on system"""

Â  Â  Â  Â  cpu_count = psutil.cpu_count(logical=True)

Â  Â  Â  Â  memory_gb = psutil.virtual_memory().total / (1024**3)

Â  Â  Â  Â  # Conservative approach: don't overwhelm the system

Â  Â  Â  Â  if memory_gb < 4:

Â  Â  Â  Â  Â  Â  return min(2, cpu_count)

Â  Â  Â  Â  elif memory_gb < 8:

Â  Â  Â  Â  Â  Â  return min(4, cpu_count)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  return min(8, cpu_count)

Â  Â  def _calculate_optimal_batch_size(self, total_items: int, avg_time_per_item: float = None) -> int:

Â  Â  Â  Â  """Calculate optimal batch size based on data characteristics"""

Â  Â  Â  Â  if self.batch_size:

Â  Â  Â  Â  Â  Â  return self.batch_size

Â  Â  Â  Â  # Adaptive batch sizing

Â  Â  Â  Â  if self.enable_adaptive:

Â  Â  Â  Â  Â  Â  # Estimate based on available memory

Â  Â  Â  Â  Â  Â  available_memory_mb = psutil.virtual_memory().available / (1024**2)

Â  Â  Â  Â  Â  Â  estimated_memory_per_item = 50 Â # MB per item (conservative estimate)

Â  Â  Â  Â  Â  Â  memory_based_batch = max(1, int(available_memory_mb / estimated_memory_per_item))

Â  Â  Â  Â  Â  Â  # Time-based optimization

Â  Â  Â  Â  Â  Â  if avg_time_per_item:

Â  Â  Â  Â  Â  Â  Â  Â  # Aim for batches that take 30-60 seconds to process

Â  Â  Â  Â  Â  Â  Â  Â  target_batch_time = 45 Â # seconds

Â  Â  Â  Â  Â  Â  Â  Â  time_based_batch = max(1, int(target_batch_time / avg_time_per_item))

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  time_based_batch = 10

Â  Â  Â  Â  Â  Â  # Worker-based optimization

Â  Â  Â  Â  Â  Â  worker_based_batch = max(1, total_items // (self.max_workers * 2))

Â  Â  Â  Â  Â  Â  # Take the minimum to be conservative

Â  Â  Â  Â  Â  Â  optimal_batch = min(memory_based_batch, time_based_batch, worker_based_batch)

Â  Â  Â  Â  Â  Â  return max(1, min(optimal_batch, 50)) Â # Between 1 and 50

Â  Â  Â  Â  return 10 Â # Default fallback

Â  Â  def _create_batches(self, items: List[Any], batch_size: int) -> List[List[Any]]:

Â  Â  Â  Â  """Create batches from items list"""

Â  Â  Â  Â  return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]

Â  Â  def _monitor_memory(self) -> float:

Â  Â  Â  Â  """Monitor current memory usage"""

Â  Â  Â  Â  memory_info = self.process.memory_info()

Â  Â  Â  Â  memory_mb = memory_info.rss / (1024**2)

Â  Â  Â  Â  return memory_mb

Â  Â  def _should_pause_for_memory(self) -> bool:

Â  Â  Â  Â  """Check if we should pause for memory management"""

Â  Â  Â  Â  current_memory = self._monitor_memory()

Â  Â  Â  Â  return current_memory > self.memory_limit_mb

Â  Â  def process_parallel(self,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  items: List[Any],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  process_func: Callable,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_callback: Optional[Callable] = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **kwargs) -> List[Any]:

Â  Â  Â  Â  """

Â  Â  Â  Â  Process items in parallel with advanced optimization

Â  Â  Â  Â  Args:

Â  Â  Â  Â  Â  Â  items: List of items to process

Â  Â  Â  Â  Â  Â  process_func: Function to process each batch

Â  Â  Â  Â  Â  Â  progress_callback: Optional callback for progress updates

Â  Â  Â  Â  Â  Â  **kwargs: Additional arguments for process_func

Â  Â  Â  Â  Returns:

Â  Â  Â  Â  Â  Â  List of results from all batches

Â  Â  Â  Â  """

Â  Â  Â  Â  self.total_items = len(items)

Â  Â  Â  Â  self.completed_items = 0

Â  Â  Â  Â  self.start_time = datetime.now()

Â  Â  Â  Â  self.results = []

Â  Â  Â  Â  if not items:

Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  # Calculate optimal batch size

Â  Â  Â  Â  batch_size = self._calculate_optimal_batch_size(len(items))

Â  Â  Â  Â  batches = self._create_batches(items, batch_size)

Â  Â  Â  Â  print(f"ğŸš€ Parallel Processing Setup:")

Â  Â  Â  Â  print(f" Â  ğŸ“Š Total items: {self.total_items}")

Â  Â  Â  Â  print(f" Â  ğŸ“¦ Batch size: {batch_size}")

Â  Â  Â  Â  print(f" Â  ğŸ§µ Max workers: {self.max_workers}")

Â  Â  Â  Â  print(f" Â  ğŸ“‹ Total batches: {len(batches)}")

Â  Â  Â  Â  print(f" Â  ğŸ’¾ Memory limit: {self.memory_limit_mb}MB")

Â  Â  Â  Â  print(f" Â  ğŸ”§ Adaptive mode: {'âœ…' if self.enable_adaptive else 'âŒ'}")

Â  Â  Â  Â  # Process batches in parallel

Â  Â  Â  Â  with ThreadPoolExecutor(max_workers=self.max_workers) as executor:

Â  Â  Â  Â  Â  Â  # Submit all batches - filter out timeout_seconds from kwargs

Â  Â  Â  Â  Â  Â  process_kwargs = {k: v for k, v in kwargs.items() if k != 'timeout_seconds'}

Â  Â  Â  Â  Â  Â  future_to_batch = {

Â  Â  Â  Â  Â  Â  Â  Â  executor.submit(self._process_batch_with_retry, batch, process_func, **process_kwargs): batch

Â  Â  Â  Â  Â  Â  Â  Â  for batch in batches

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  # Process completed batches with timeout

Â  Â  Â  Â  Â  Â  for future in as_completed(future_to_batch):

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Add timeout to prevent hanging

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_result = future.result(timeout=self.timeout_seconds)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with self.lock:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.results.extend(batch_result)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.completed_items += len(batch_result)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Progress update

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if progress_callback:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  progress_callback(self.completed_items, self.total_items)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Memory management

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if self._should_pause_for_memory():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âš ï¸ Â High memory usage ({self._monitor_memory():.1f}MB), pausing...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(1) Â # Brief pause

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Performance monitoring

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self._log_progress()

Â  Â  Â  Â  Â  Â  Â  Â  except TimeoutError:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.error_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"â° Batch processing timeout after {self.timeout_seconds}s, skipping...")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.error_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"âŒ Batch processing error: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  # Final summary

Â  Â  Â  Â  self._log_final_summary()

Â  Â  Â  Â  return self.results

Â  Â  def _process_batch_with_retry(self,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â batch: List[Any],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â process_func: Callable,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â max_retries: int = 2,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â **kwargs) -> List[Any]:

Â  Â  Â  Â  """Process a batch with retry mechanism"""

Â  Â  Â  Â  for attempt in range(max_retries + 1):

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  return process_func(batch, **kwargs)

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  if attempt < max_retries:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.retry_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"ğŸ”„ Retry {attempt + 1}/{max_retries} for batch due to: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(2 ** attempt) Â # Exponential backoff

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"ğŸ’¥ Batch failed after {max_retries} retries: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return [] Â # Return empty result for failed batch

Â  Â  def _log_progress(self):

Â  Â  Â  Â  """Log current progress with ETA"""

Â  Â  Â  Â  if self.completed_items == 0:

Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  elapsed_time = (datetime.now() - self.start_time).total_seconds()

Â  Â  Â  Â  avg_time_per_item = elapsed_time / self.completed_items

Â  Â  Â  Â  remaining_items = self.total_items - self.completed_items

Â  Â  Â  Â  estimated_remaining_time = avg_time_per_item * remaining_items

Â  Â  Â  Â  progress_percent = (self.completed_items / self.total_items) * 100

Â  Â  Â  Â  eta = datetime.now() + timedelta(seconds=estimated_remaining_time)

Â  Â  Â  Â  memory_usage = self._monitor_memory()

Â  Â  Â  Â  print(f"ğŸ“Š Progress: {self.completed_items}/{self.total_items} ({progress_percent:.1f}%)")

Â  Â  Â  Â  print(f" Â  â±ï¸ Â Elapsed: {elapsed_time/60:.1f}min | ETA: {eta.strftime('%H:%M:%S')}")

Â  Â  Â  Â  print(f" Â  ğŸš€ Speed: {avg_time_per_item:.1f}s/item | ğŸ’¾ Memory: {memory_usage:.1f}MB")

Â  Â  Â  Â  print(f" Â  âŒ Errors: {self.error_count} | ğŸ”„ Retries: {self.retry_count}")

Â  Â  def _log_final_summary(self):

Â  Â  Â  Â  """Log final processing summary"""

Â  Â  Â  Â  total_time = (datetime.now() - self.start_time).total_seconds()

Â  Â  Â  Â  success_rate = ((self.total_items - self.error_count) / self.total_items) * 100

Â  Â  Â  Â  print(f"\nğŸ‰ Parallel Processing Complete!")

Â  Â  Â  Â  print(f" Â  â±ï¸ Â Total time: {total_time/60:.1f} minutes")

Â  Â  Â  Â  print(f" Â  ğŸ“Š Success rate: {success_rate:.1f}%")

Â  Â  Â  Â  print(f" Â  âŒ Errors: {self.error_count}")

Â  Â  Â  Â  print(f" Â  ğŸ”„ Retries: {self.retry_count}")

Â  Â  Â  Â  print(f" Â  ğŸ’¾ Peak memory: {self._monitor_memory():.1f}MB")

Â  Â  Â  Â  if self.total_items > 0:

Â  Â  Â  Â  Â  Â  throughput = self.total_items / total_time

Â  Â  Â  Â  Â  Â  print(f" Â  ğŸš€ Throughput: {throughput:.2f} items/second")

  
  

# Convenience functions for easy integration

def process_situations_parallel(situations: List[Dict],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â process_func: Callable,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â max_workers: int = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â batch_size: int = None,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â timeout_seconds: int = 300,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â **kwargs) -> List[Dict]:

Â  Â  """

Â  Â  Convenience function to process situations in parallel

Â  Â  Args:

Â  Â  Â  Â  situations: List of situation dictionaries

Â  Â  Â  Â  process_func: Function to process each situation

Â  Â  Â  Â  max_workers: Maximum worker threads

Â  Â  Â  Â  batch_size: Items per batch

Â  Â  Â  Â  **kwargs: Additional arguments for process_func

Â  Â  Returns:

Â  Â  Â  Â  List of processed results

Â  Â  """

Â  Â  processor = ParallelProcessor(

Â  Â  Â  Â  max_workers=max_workers,

Â  Â  Â  Â  batch_size=batch_size,

Â  Â  Â  Â  enable_adaptive=True

Â  Â  )

Â  Â  def progress_callback(completed, total):

Â  Â  Â  Â  if completed % 10 == 0: Â # Update every 10 items

Â  Â  Â  Â  Â  Â  print(f"ğŸ”„ Processed {completed}/{total} situations...")

Â  Â  return processor.process_parallel(

Â  Â  Â  Â  items=situations,

Â  Â  Â  Â  process_func=process_func,

Â  Â  Â  Â  progress_callback=progress_callback,

Â  Â  Â  Â  timeout_seconds=timeout_seconds,

Â  Â  Â  Â  **kwargs

Â  Â  )

  
  

if __name__ == "__main__":

Â  Â  # Demo usage

Â  Â  def demo_process_func(batch, **kwargs):

Â  Â  Â  Â  """Demo processing function"""

Â  Â  Â  Â  time.sleep(0.1) Â # Simulate work

Â  Â  Â  Â  return [f"Processed: {item}" for item in batch]

Â  Â  # Test data

Â  Â  test_items = list(range(100))

Â  Â  print("ğŸ§ª Testing Parallel Processor...")

Â  Â  processor = ParallelProcessor(max_workers=4, batch_size=10)

Â  Â  results = processor.process_parallel(test_items, demo_process_func)

Â  Â  print(f"âœ… Demo completed with {len(results)} results")
Â  Â  
```




[Skip to content](https://chatgpt.com/c/68b16357-eb00-8326-99aa-aea1e392e223#main)

## Chat history

[](https://chatgpt.com/)

##### You said:

TÃ¬m kiáº¿m cÃ¡c nguá»“n tÃ i liá»‡u vÃ  chÆ°Æ¡ng trÃ¬nh Ä‘á»ƒ luyá»‡n giáº£i thuáº­t song song

###### ChatGPT said:

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘Â **nguá»“n tÃ i liá»‡u vÃ  chÆ°Æ¡ng trÃ¬nh há»c luyá»‡n giáº£i thuáº­t song song (parallel algorithms)**Â mÃ  báº¡n cÃ³ thá»ƒ tham kháº£o, bao gá»“m tá»« ná»n táº£ng lÃ½ thuyáº¿t Ä‘áº¿n cÃ¡c framework láº­p trÃ¬nh thá»±c tiá»…n:

---

## 1. TÃ i liá»‡u há»c lÃ½ thuyáº¿t vÃ  slides giáº£ng dáº¡y

- **Introduction to Parallel Algorithms (UMD)**  
    Báº£n PDF giá»›i thiá»‡u cÃ¡c khÃ¡i niá»‡m ná»n táº£ng nhÆ° mÃ´ hÃ¬nh PRAM, workâ€‘depth, scheduling, v.v. Ná»™i dung Ä‘Ã£ tá»•ng há»£p tinh hoa tá»« hÆ¡n 40 nÄƒm nghiÃªn cá»©u trong lÄ©nh vá»±c nÃ y.Â [tcpp.cs.gsu.edu+14software-performance-engineering.github.io+14TutorialsPoint+14](https://software-performance-engineering.github.io/spring25/resources/parallel.pdf?utm_source=chatgpt.com)[math.gordon.edu+6Computer Science at UMD+6CMU School of Computer Science+6](https://www.cs.umd.edu/~laxman/notes/parallel.pdf?utm_source=chatgpt.com)
    
- **Unit 7: Parallel Algorithms â€“ wildâ€‘inter.net**  
    Slide bÃ i giáº£ng cá»§a Sebastian Wild, trÃ¬nh bÃ y cÃ¡c mÃ´ hÃ¬nh mÃ¡y, PRAM, Ä‘áº·t váº¥n Ä‘á» vá» prefix sum, phÃ¢n Ä‘oáº¡n song song, quicksort vÃ  mergesort song song, cÃ¹ng giá»›i háº¡n hiá»‡u nÄƒng tá»‘c Ä‘á»™...Â [umiacs.umd.edu+4wild-inter.net+4wild-inter.net+4](https://www.wild-inter.net/teaching/comp526/07-parallel-algorithms?utm_source=chatgpt.com)
    
- **Notesâ€‘10â€‘Parallelâ€‘Algorithms â€“ Philippsâ€‘UniversitÃ¤t Marburg**  
    TÃ i liá»‡u cáº­p nháº­t cuá»‘i nÄƒm 2024, vá»›i Ä‘á»‹nh hÆ°á»›ng há»c vá» chiáº¿n lÆ°á»£c parallel hÃ³a, phÃ¢n loáº¡i bÃ i toÃ¡n embarrassingly parallel, PRAM, prefix sum, quicksort/mergesort song songâ€¦Â [Computer Science and Engineering+15wild-inter.net+15Computer Science at UMD+15](https://www.wild-inter.net/teaching/ea/notes-10-parallel-algorithms.pdf?utm_source=chatgpt.com)
    
- **Lectures on Parallel Computing â€“ Jesper Larsson TrÃ¤ff (arXiv 2024)**  
    Bá»™ lecture notes toÃ n diá»‡n cho cáº£ Ä‘áº¡i há»c vÃ  sau Ä‘áº¡i há»c, Ä‘i sÃ¢u vÃ o nguyÃªn lÃ½ thiáº¿t káº¿ parallel algorithms, Ä‘á»“ng thá»i hÆ°á»›ng dáº«n thá»±c hÃ nh vá»›i MPI, OpenMP vÃ  pthreads.Â [arXiv](https://arxiv.org/abs/2407.18795?utm_source=chatgpt.com)
    

---

## 2. HÆ°á»›ng dáº«n trá»±c tuyáº¿n (tutorials)

- **TutorialsPoint â€“Â _Parallel Algorithm Tutorial_**  
    ÄÃ¢y lÃ  pháº§n hÆ°á»›ng dáº«n trá»±c tuyáº¿n lÃ½ thuyáº¿t cÆ¡ báº£n vá» cÃ¡c mÃ´ hÃ¬nh, cáº¥u trÃºc vÃ  cÃ¡ch triá»ƒn khai thuáº­t toÃ¡n song song. PhÃ¹ há»£p vá»›i sinh viÃªn tá»« cÆ¡ báº£n Ä‘áº¿n nÃ¢ng cao.Â [My Lecture Library+15TutorialsPoint+15TutorialsPoint+15](https://www.tutorialspoint.com/parallel_algorithm/index.htm?utm_source=chatgpt.com)
    
- **GeeksforGeeks â€“Â _Parallel Algorithm Models in Parallel Computing_**  
    Giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh song song phá»• biáº¿n nhÆ° Dataâ€‘Parallel, chiáº¿n lÆ°á»£c phÃ¢n chia dá»¯ liá»‡u/tasks, tá»‘i Æ°u tÆ°Æ¡ng tÃ¡c giá»¯a chÃºng.Â [GeeksforGeeks+2math.gordon.edu+2](https://www.geeksforgeeks.org/mobile-computing/parallel-algorithm-models-in-parallel-computing/?utm_source=chatgpt.com)
    
- **NumberAnalytics â€“Â _Mastering Parallel Algorithms_**  
    BÃ i blog má»›i thÃ¡ng 6/2025, giÃºp báº¡n hiá»ƒu tá»•ng quan vá» á»©ng dá»¥ng, thÃ¡ch thá»©c (synchronization, load balancing, debugging) vÃ  cÃ¡ch tá»‘i Æ°u scalability trong thuáº­t toÃ¡n song song.Â [TutorialsPoint+11Number Analytics+11umiacs.umd.edu+11](https://www.numberanalytics.com/blog/ultimate-guide-to-parallel-algorithms?utm_source=chatgpt.com)
    

---

## 3. NgÃ´n ngá»¯ vÃ  framework láº­p trÃ¬nh song song

- **Intel oneAPI Threading Building Blocks (oneTBB)**  
    Má»™t thÆ° viá»‡n C++ phá»• biáº¿n cho láº­p trÃ¬nh Ä‘a luá»“ng, há»— trá»£ cÃ¡c cáº¥u trÃºc nhÆ°Â `parallel_for`,Â `parallel_reduce`,Â `parallel_sort`Â cÃ¹ng workâ€‘stealing schedulerâ€¦Â [Wikipedia](https://en.wikipedia.org/wiki/Threading_Building_Blocks?utm_source=chatgpt.com)
    
- **SYCL (Khronos Group)**  
    MÃ´ hÃ¬nh láº­p trÃ¬nh song song cho há»‡ thá»‘ng tÃ­nh toÃ¡n Ä‘a ná»n táº£ng (CPU, GPU, accelerators) theo phong cÃ¡ch singleâ€‘source, dÃ¹ng C++17. CÃ³ nhiá»u implementation nhÆ° DPC++, AdaptiveCppâ€¦Â [Wikipedia+2CMU School of Computer Science+2](https://en.wikipedia.org/wiki/SYCL?utm_source=chatgpt.com)
    
- **NESL**  
    Má»™t ngÃ´n ngá»¯ song song chuyÃªn vá» láº­p trÃ¬nh hÃ m (functional), há»— trá»£ nested data parallelism vÃ  mÃ´ hÃ¬nh workâ€‘depth rÃµ rÃ ng. Táº¡o cáº§u ná»‘i giá»¯a thuáº­t toÃ¡n lÃ½ thuyáº¿t vÃ  thá»±c thi hiá»‡u quáº£.Â [Wikipedia](https://en.wikipedia.org/wiki/NESL?utm_source=chatgpt.com)
    

---

## 4. KhÃ³a há»c vÃ  bÃ i giáº£ng video trá»±c tuyáº¿n

- **Playlist â€œParallel Algorithm Tutorialâ€ â€“ YouTube (Last Minute Study)**  
    Gá»“m 10 video ngáº¯n hÆ°á»›ng dáº«n cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n Ä‘áº¿n trung cáº¥p vá» song song hÃ³a thuáº­t toÃ¡n.Â [tcpp.cs.gsu.edu+10YouTube+10TutorialsPoint+10](https://www.youtube.com/playlist?list=PLiKFau7eKJlqoHxgx_hjLN-xFup7a13Va&utm_source=chatgpt.com)
    

NgoÃ i ra, báº¡n cÃ³ thá»ƒ tÃ¬m cÃ¡c khÃ³a há»c má»Ÿ (MOOC) trÃªn Coursera, edX, hoáº·c cÃ¡c giáº£ng dáº¡y táº¡i cÃ¡c trÆ°á»ng Ä‘áº¡i há»c nhÆ° UMD, CMU, ETH Zurich, v.v., Ä‘á»ƒ cÃ³ thÃªm bÃ i giáº£ng video vÃ  bÃ i táº­p trá»±c quan.

---

## 5. Gá»£i Ã½ phÆ°Æ¡ng phÃ¡p há»c

|Giai Ä‘oáº¡n|Ná»™i dung nÃªn táº­p trung|Gá»£i Ã½ cá»¥ thá»ƒ|
|---|---|---|
|**LÃ½ thuyáº¿t ná»n táº£ng**|Hiá»ƒu PRAM, work-depth model, phÃ¢n tÃ­ch hiá»‡u nÄƒng|UMD PDF, Sebastian Wild slides, TrÃ¤ff lectures|
|**Thá»±c hÃ nh code**|CÃ i Ä‘áº·t peer-to-peer, shared memory, MPI/OpenMP, oneTBB, SYCL|Sá»­ dá»¥ng lecture notes thá»±c hÃ nh, oneTBB, SYCL tutorials|
|**Chuyá»ƒn sang láº­p trÃ¬nh**|Viáº¿t cÃ¡c bÃ i toÃ¡n quen thuá»™c nhÆ° prefix sum, merge sort song song|Dá»± Ã¡n nhá» theo lecture/tron sá»‘, Ã¡p dá»¥ng STL parallel hoáº·c oneTBB|
|**Tá»‘i Æ°u vÃ  gá»¡ lá»—i**|Xá»­ lÃ½ cáº¡nh tranh, khá»‘i lÆ°á»£ng khÃ´ng cÃ¢n báº±ng, overhead|Äá»c blog Mastering Parallel Algorithms, debug tools|

---

### Tá»•ng káº¿t nhanh

- **TÃ i liá»‡u lÃ½ thuyáº¿t**: UMD, Sebastian Wild, Marburg, TrÃ¤ff (arXiv)
    
- **Tutorials trá»±c tuyáº¿n**: TutorialsPoint, GeeksforGeeks, blog NumberAnalytics
    
- **Framework thá»±c hÃ nh**: oneTBB (C++), SYCL (C++ single-source), NESL (functional)
    
- **Video há»c nhanh**: Playlist YouTube â€œParallel Algorithm Tutorialâ€ (Last Minute Study)
    

---

Náº¿u báº¡n cáº§n tÃ i liá»‡u tiáº¿ng Viá»‡t hoáº·c cÃ¡c khÃ³a há»c cá»¥ thá»ƒ á»Ÿ Viá»‡t Nam, mÃ¬nh cÃ³ thá»ƒ tÃ¬m thÃªm. CÃ²n náº¿u báº¡n quan tÃ¢m Ä‘áº¿n á»©ng dá»¥ng song song trÃªn GPU, HPC hoáº·c dÃ¹ng Python (nhÆ° multiprocessing hoáº·c CUDA Python), cá»© nÃ³i nhÃ© mÃ¬nh sáº½ gá»£i Ã½ chi tiáº¿t hÆ¡n!

##### You said:

láº­p trÃ¬nh song song máº£ng phthon nhÃ©

###### ChatGPT said:

Thought for 17s

Tuyá»‡t! DÆ°á»›i Ä‘Ã¢y lÃ  â€œsá»• tay nhanhâ€ Ä‘á»ƒÂ **láº­p trÃ¬nh song song trÃªn máº£ng báº±ng Python**, Ä‘i tá»« CPU (Ä‘a tiáº¿n trÃ¬nh/Ä‘a luá»“ng, JIT) Ä‘áº¿n phÃ¢n tÃ¡n vÃ  GPU, kÃ¨m vÃ­ dá»¥ ngáº¯n + nguá»“n chÃ­nh thá»‘ng Ä‘á»ƒ báº¡n Ä‘Ã o sÃ¢u.

# 1) Song song CPU â€œcÆ¡ báº£nâ€ cho máº£ng

**A.Â `multiprocessing`Â /Â `concurrent.futures`Â (ProcessPool) â€“ nÃ© GIL, chia dá»¯ liá»‡u theo block**

- DÃ¹ng khi má»—i task xá»­ lÃ½ Ä‘á»™c láº­p trÃªn cÃ¡c lÃ¡t máº£ng (chunk).
    
- API cao cáº¥p, dá»… dÃ¹ng:Â `ProcessPoolExecutor`/`Pool.map`.Â [Python documentation+1](https://docs.python.org/3/library/concurrent.futures.html?utm_source=chatgpt.com)
    

`# Sum bÃ¬nh phÆ°Æ¡ng theo khá»‘i báº±ng ProcessPool import numpy as np from concurrent.futures import ProcessPoolExecutor  def work(chunk):     x = np.asarray(chunk, dtype=np.float64)     return np.sum(x*x)  def parallel_sum_squares(a, n_workers=4):     chunks = np.array_split(a, n_workers)     with ProcessPoolExecutor(max_workers=n_workers) as ex:         return sum(ex.map(work, chunks))  a = np.random.rand(10_000_000) print(parallel_sum_squares(a))`

> LÆ°u Ã½: Äá»‘i tÆ°á»£ng pháº£i â€œpicklableâ€; code nÃªn Ä‘áº·t trongÂ `if __name__ == "__main__":`Â trÃªn Windows.Â [stackless.readthedocs.io](https://stackless.readthedocs.io/en/3.6-slp/library/concurrent.futures.html?utm_source=chatgpt.com)

**B. Thread + NumPy (Ä‘Ãºng ngá»¯ cáº£nh)**  
Nhiá»u phÃ©p NumPyÂ **tá»± giáº£i phÃ³ng GIL**Â vÃ /hoáº·c gá»i BLAS/LAPACK Ä‘a luá»“ng â†’ Ä‘Ã´i khiÂ **thread**Â váº«n tÄƒng tá»‘c náº¿u má»—i thread xá»­ lÃ½Â **máº£ng riÃªng**. Äá»«ng â€œbá»câ€ thÃªm thread quanh phÃ©p Ä‘Ã£ Ä‘a luá»“ng vÃ¬ cÃ³ thá»ƒ cháº­m hÆ¡n.Â [NumPy](https://numpy.org/doc/stable/reference/thread_safety.html?utm_source=chatgpt.com)[Super Fast Python](https://superfastpython.com/numpy-threads-worse-performance/?utm_source=chatgpt.com)

# 2) JIT & vector hÃ³a nÃ¢ng cao cho máº£ng

**A. NumbaÂ `@njit(parallel=True)`Â +Â `prange`**

- Dá»‹ch JIT sang mÃ¡y, song song hÃ³a vÃ²ng láº·p kiá»ƒu OpenMP, cá»±c há»£p cho vÃ²ng for trÃªn máº£ng.Â [numba.readthedocs.io](https://numba.readthedocs.io/en/stable/user/parallel.html?utm_source=chatgpt.com)[numba.pydata.org](https://numba.pydata.org/numba-doc/0.11/prange.html?utm_source=chatgpt.com)
    

`import numpy as np from numba import njit, prange  @njit(parallel=True, fastmath=True) def axpy(a, x, y):     out = np.empty_like(x)     for i in prange(x.size):         out[i] = a * x[i] + y[i]     return out  x = np.random.rand(10_000_000) y = np.random.rand(10_000_000) z = axpy(2.0, x, y)  # cháº¡y song song trÃªn CPU`

**B. NumExpr â€“ â€œmÃ¡y áº£oâ€ Ä‘Ã¡nh giÃ¡ biá»ƒu thá»©c máº£ng, Ä‘a luá»“ng**

- Viáº¿t gá»n kiá»ƒuÂ `"3*a + 4*b + sin(c)"`, giáº£m dÃ¹ng bá»™ nhá»› táº¡m, tá»± dÃ¹ng nhiá»u core.Â [numexpr.readthedocs.io](https://numexpr.readthedocs.io/en/latest/user_guide.html?utm_source=chatgpt.com)[GitHub](https://github.com/pydata/numexpr?utm_source=chatgpt.com)
    

`import numexpr as ne z = ne.evaluate("3*a + 4*b + sin(c)")`

# 3) Máº£ng â€œquÃ¡ toâ€ & pipeline song song/Ä‘a mÃ¡y

**Dask Array**Â â€“ â€œNumPy chia khá»‘iâ€ (blocked), cháº¡y song song trÃªn nhiá»u core/mÃ¡y, xá»­ lÃ½ dá»¯ liá»‡u lá»›n hÆ¡n RAM.Â [tutorial.dask.org](https://tutorial.dask.org/02_array.html?utm_source=chatgpt.com)[intro-to-dask.readthedocs.io](https://intro-to-dask.readthedocs.io/en/latest/tutorial/Dask/collections.html?utm_source=chatgpt.com)

`import dask.array as da x = da.random.random((100_000, 10_000), chunks=(10_000, 10_000)) y = (x - x.mean(axis=0)) / x.std(axis=0)   # Ä‘á»“ thá»‹ tÃ­nh toÃ¡n lÆ°á»i print(y.mean().compute())                  # .compute() -> thá»±c thi song song`

# 4) GPU cho máº£ng (náº¿u cÃ³ CUDA/ROCm)

**CuPy**Â â€“ gáº§n nhÆ° â€œdrop-inâ€ NumPy nhÆ°ng cháº¡y trÃªn GPU; cá»±c nhanh khi máº£ng lá»›n/tÃ­nh toÃ¡n náº·ng.Â [docs.cupy.dev+1](https://docs.cupy.dev/?utm_source=chatgpt.com)[learningsys.org](https://learningsys.org/nips17/assets/papers/paper_16.pdf?utm_source=chatgpt.com)

`import cupy as cp x = cp.random.rand(10_000_000, dtype=cp.float32) y = cp.sin(x) * 2 + x**2        # cháº¡y trÃªn GPU cp.cuda.Stream.null.synchronize()`

> Máº¹o hiá»‡u nÄƒng GPU: giá»¯ dá»¯ liá»‡uÂ **trÃªn GPU**Â (háº¡n cháº¿ copy), dÃ¹ngÂ `float32`Â khi Ä‘Æ°á»£c, vÃ  cÃ¢n nháº¯cÂ **CUDA streams**Â cho task Ä‘á»™c láº­p.Â [SparkCodehub](https://www.sparkcodehub.com/numpy/modern-applications/gpu-computing-cupy?utm_source=chatgpt.com)

# 5) Khi nÃ o dÃ¹ng cÃ¡i gÃ¬?

- **Vector hÃ³a NumPy thuáº§n**: Ä‘Æ¡n giáº£n & nhanh (nhiá»u hÃ m Ä‘Ã£ Ä‘a luá»“ng nhá» BLAS).
    
- **Numba (prange)**: vÃ²ng for tÃ­nh toÃ¡n pháº§n tá»­ â†’ tÄƒng tá»‘c lá»›n trÃªn CPU.Â [numba.readthedocs.io](https://numba.readthedocs.io/en/stable/user/parallel.html?utm_source=chatgpt.com)
    
- **NumExpr**: nhiá»u biá»ƒu thá»©c máº£ng liÃªn tiáº¿p â†’ Ã­t táº¡m, cháº¡y Ä‘a luá»“ng tá»± Ä‘á»™ng.Â [numexpr.readthedocs.io](https://numexpr.readthedocs.io/en/latest/user_guide.html?utm_source=chatgpt.com)
    
- **ProcessPool**: tÃ¡c vá»¥ Ä‘á»™c láº­p náº·ng CPU / cáº§n nÃ© GIL hoáº·c gá»i NumPy khÃ´ng Ä‘a luá»“ng.Â [Python documentation](https://docs.python.org/3/library/concurrent.futures.html?utm_source=chatgpt.com)
    
- **Dask Array**: dá»¯ liá»‡uÂ **quÃ¡ lá»›n**Â hoáº·c cáº§nÂ **song song phÃ¢n tÃ¡n**.Â [tutorial.dask.org](https://tutorial.dask.org/02_array.html?utm_source=chatgpt.com)
    
- **CuPy**: cÃ³ GPU vÃ  thao tÃ¡c máº£ng lá»›n/náº·ng (FFT, linalg, ufunc).Â [docs.cupy.dev](https://docs.cupy.dev/?utm_source=chatgpt.com)
    

# 6) Báº«y thÆ°á»ng gáº·p & tá»‘i Æ°u

- **ÄÃ¨ thread lÃªn BLAS**: phÃ©p linalg (dot, svd, â€¦) vá»‘n Ä‘Ã£ Ä‘a luá»“ng; thÃªm ThreadPool cÃ³ thá»ƒÂ **cháº­m hÆ¡n**. Äiá»u chá»‰nh sá»‘ thread BLAS (`OMP_NUM_THREADS`,Â `MKL_NUM_THREADS`) khi cáº§n.Â [Scikit-learn](https://scikit-learn.org/stable/computing/parallelism.html?utm_source=chatgpt.com)[NumPy](https://numpy.org/devdocs/reference/global_state.html?utm_source=chatgpt.com)[GitHub](https://github.com/OpenMathLib/OpenBLAS/issues/1881?utm_source=chatgpt.com)
    
- **Pickle overhead**Â vá»›iÂ `multiprocessing`: gá»­i máº£ng khá»•ng lá»“ giá»¯a tiáº¿n trÃ¬nh tá»‘n thá»i gian/bá»™ nhá»› â†’ chia block Ã­t láº§n hÆ¡n, dÃ¹ng shared memory khi phÃ¹ há»£p.Â [Python documentation](https://docs.python.org/3/library/multiprocessing.html?utm_source=chatgpt.com)
    
- **Thread an toÃ n vá»›i NumPy**: hiá»‡u quáº£ nháº¥t khi má»—i thread sá»Ÿ há»¯u máº£ng riÃªng (khÃ´ng chia sáº» ghi/Ä‘á»c nhiá»u).Â [NumPy](https://numpy.org/doc/stable/reference/thread_safety.html?utm_source=chatgpt.com)
    

---

## Lá»™ trÃ¬nh luyá»‡n táº­p gá»£i Ã½ (4 tuáº§n)

1. **Tuáº§n 1**: Ã”n NumPy vector hÃ³a + hiá»ƒu BLAS Ä‘a luá»“ng; Ä‘o thá»­Â `np.dot`Â khi Ä‘á»•iÂ `OMP_NUM_THREADS`.Â [Scikit-learn](https://scikit-learn.org/stable/computing/parallelism.html?utm_source=chatgpt.com)
    
2. **Tuáº§n 2**: Viáº¿t 3 bÃ i báº±ngÂ **Numba prange**Â (AXPY, prefix-sum/scan, 2D stencil). So sÃ¡nh vá»›i NumPy.Â [numba.readthedocs.io](https://numba.readthedocs.io/en/stable/user/parallel.html?utm_source=chatgpt.com)
    
3. **Tuáº§n 3**: BÃ i lá»›n vá»›iÂ **ProcessPool**Â (chia máº£ng theo block) vÃ Â **NumExpr**Â cho biá»ƒu thá»©c dÃ i.Â [Python documentation](https://docs.python.org/3/library/concurrent.futures.html?utm_source=chatgpt.com)[numexpr.readthedocs.io](https://numexpr.readthedocs.io/en/latest/user_guide.html?utm_source=chatgpt.com)
    
4. **Tuáº§n 4**:Â **Dask Array**Â cho máº£ng vÆ°á»£t RAM; náº¿u cÃ³ GPU thÃ¬ port sangÂ **CuPy**.Â [tutorial.dask.org](https://tutorial.dask.org/02_array.html?utm_source=chatgpt.com)[docs.cupy.dev](https://docs.cupy.dev/?utm_source=chatgpt.com)
    

---

## Nguá»“n cháº¥t lÆ°á»£ng Ä‘á»ƒ tra cá»©u thÃªm

- Python docs:Â `multiprocessing`,Â `concurrent.futures`.Â [Python documentation+1](https://docs.python.org/3/library/multiprocessing.html?utm_source=chatgpt.com)
    
- Numba parallel (`@jit(parallel=True)`,Â `prange`).Â [numba.readthedocs.io](https://numba.readthedocs.io/en/stable/user/parallel.html?utm_source=chatgpt.com)[numba.pydata.org](https://numba.pydata.org/numba-doc/0.11/prange.html?utm_source=chatgpt.com)
    
- NumExpr user guide.Â [numexpr.readthedocs.io](https://numexpr.readthedocs.io/en/latest/user_guide.html?utm_source=chatgpt.com)
    
- Dask Array tutorial.Â [tutorial.dask.org](https://tutorial.dask.org/02_array.html?utm_source=chatgpt.com)
    
- NumPy thread-safety & BLAS threading.Â [NumPy+1](https://numpy.org/doc/stable/reference/thread_safety.html?utm_source=chatgpt.com)
    
- CuPy docs (NumPy trÃªn GPU).Â [docs.cupy.dev](https://docs.cupy.dev/?utm_source=chatgpt.com)
    

Báº¡n muá»‘n mÃ¬nh viáº¿t má»™tÂ **template project**Â nhá» (CLI) Ä‘á»ƒ benchmark 5 cÃ¡ch trÃªn cÃ¹ng má»™t bÃ i toÃ¡n máº£ng (CPU vs GPU) khÃ´ng? MÃ¬nh cÃ³ thá»ƒ Ä‘áº·t sáºµn cáº¥u hÃ¬nh thread/BLAS vÃ  report thá»i gian cháº¡y Ä‘á»ƒ báº¡n Ã¡p dá»¥ng ngay.

ChatGPT can make mistakes. OpenAI doesn't use SU workspace data to train its models.

![](https://cdn.oaistatic.com/assets/favicon-eex17e9e.ico)

---

Memo

Highlight

Explain

Beta

1093Â /Â 5000used queries

![](chrome-extension://amfojhdiedpdnlijjbhjnhokbnohfdfb/img/logo/icon@2x.png)

1


---
# VÃ­ dá»¥ vá» láº­p trÃ¬nh song song

### 2.2.1 BÃ i Prompt Tool : 

Link: [BasicTasks_Prompting/5_TuningPrompting/PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py at main Â· DoanNgocCuong/BasicTasks_Prompting](https://github.com/DoanNgocCuong/BasicTasks_Prompting/blob/main/5_TuningPrompting/PromptTuning_OpenAI_v5_BatchSize_NumWorkers.py)



```
# @title OPENAI
import os
import json
import pandas as pd
import time
import logging
import re
import concurrent.futures
from typing import List, Dict
import math
from openpyxl import load_workbook
import psutil
import multiprocessing
import threading
from pathlib import Path
import argparse
from dotenv import load_dotenv

# ============= FIX PROXY ISSUE =============
# BÆ°á»›c 1: XÃ³a táº¥t cáº£ proxy environment variables
proxy_vars = [
    'HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy', 
    'ALL_PROXY', 'all_proxy', 'NO_PROXY', 'no_proxy'
]

for var in proxy_vars:
    if var in os.environ:
        del os.environ[var]
        print(f"Removed proxy variable: {var}")

# BÆ°á»›c 2: Set explicit no proxy
os.environ['NO_PROXY'] = '*'

# BÆ°á»›c 3: Import OpenAI sau khi clear proxy
try:
    from openai import OpenAI
    from openai import OpenAIError
    print("âœ“ OpenAI imported successfully")
except ImportError as e:
    print(f"âœ— Error importing OpenAI: {e}")
    print("Run: pip install openai>=1.0.0")
    exit(1)

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# BÆ°á»›c 4: Khá»Ÿi táº¡o OpenAI client vá»›i error handling
def create_openai_client():
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found in environment variables")
    
    print(f"API Key: {api_key[:10]}...")
    
    try:
        # Táº¡o client chá»‰ vá»›i api_key (khÃ´ng cÃ³ parameters khÃ¡c)
        client = OpenAI(api_key=api_key)
        print("âœ“ OpenAI client created successfully")
        return client
    except Exception as e:
        print(f"âœ— Error creating OpenAI client: {e}")
        print("Trying alternative initialization...")
        
        # Alternative: Táº¡o client vá»›i httpx explicit
        try:
            import httpx
            http_client = httpx.Client(proxies=None)
            client = OpenAI(api_key=api_key, http_client=http_client)
            print("âœ“ OpenAI client created with custom HTTP client")
            return client
        except Exception as e2:
            print(f"âœ— Alternative initialization failed: {e2}")
            raise e

# Khá»Ÿi táº¡o client
client = create_openai_client()
# ============= END FIX =============

sheet_name = 'Trang tÃ­nh1'

def process_conversation(order, base_prompt, inputs, conversation_history=None):
    print(f"\n=== Processing Conversation ===")
    print(f"Order: {order}")
    print(f"Base Prompt: {base_prompt[:100]}...")
    
    # Log conversation history
    if conversation_history:
        logger.info(f"Conversation history: {conversation_history}")

    # Táº¡o model config dÆ°á»›i dáº¡ng JSON
    model_config = {
        "model": "gpt-4o-mini-2024-07-18",
        "temperature": 0,
        "max_tokens": 2048,
        "top_p": 1,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0
    }
    model_config_json = json.dumps(model_config)
    
    responses = []
    response_times = []
    chat_messages = []
    
    # 1. System message
    chat_messages.append({"role": "system", "content": base_prompt})
    print("\nSau khi thÃªm system message:")
    print(chat_messages)
    
    # 2. History handling
    if conversation_history and not pd.isna(conversation_history):
        try:
            # Parse conversation history from JSON string
            history_messages = json.loads(conversation_history)
            
            # Validate format of history messages
            if isinstance(history_messages, list):
                for msg in history_messages:
                    if isinstance(msg, dict) and 'role' in msg and 'content' in msg:
                        chat_messages.append(msg)
                    else:
                        print(f"Warning: Skipping invalid message format in history: {msg}")
            else:
                print(f"Warning: conversation_history is not a list: {history_messages}")
            
            print("\nSau khi thÃªm history:")
            print(json.dumps(chat_messages, indent=2, ensure_ascii=False))
            
        except json.JSONDecodeError as e:
            print(f"Error parsing conversation history: {e}")
            print(f"Raw conversation history: {conversation_history}")
            logger.error(f"Error parsing conversation history: {e}")
            logger.error(f"Raw conversation history: {conversation_history}")
    
    # 3. New input
    for user_input in inputs:
        chat_messages.append({"role": "user", "content": user_input})
        print("\nTrÆ°á»›c khi gá»i API:")
        print(json.dumps(chat_messages, indent=2, ensure_ascii=False))
        
        start_time = time.time()
        try_count = 0
        while try_count < 3:
            try:
                print(f"DEBUG - Attempt {try_count + 1} to call OpenAI API")
                completion = client.chat.completions.create(
                    model=model_config["model"],
                    messages=chat_messages,   
                    temperature=model_config["temperature"],
                    max_tokens=model_config["max_tokens"],
                    top_p=model_config["top_p"],
                    frequency_penalty=model_config["frequency_penalty"],
                    presence_penalty=model_config["presence_penalty"]
                )
                end_time = time.time()
                response_content = completion.choices[0].message.content
                chat_messages.append({"role": "assistant", "content": response_content})

                responses.append(response_content)
                response_times.append(end_time - start_time)

                # Print the completion output here
                print(f"Order {order}, Input: '{user_input}', Response: '{response_content[:100]}...', Time: {end_time - start_time:.2f}s\n====")
                break
                
            except Exception as e:
                try_count += 1
                print(f"DEBUG - Error on attempt {try_count}: {str(e)}")
                if try_count >= 3:
                    responses.append("Request failed after 3 retries.")
                    response_times.append("-")
                    print(f"Order {order}, Input: '{user_input}', Response: 'Request failed after 3 retries.', Time: -")
                else:
                    print(f"DEBUG - Waiting 3 seconds before retry...")
                    time.sleep(3)

    return responses, response_times, chat_messages, model_config_json

# Add argument parser
def parse_arguments():
    parser = argparse.ArgumentParser(description='Process conversations with OpenAI API')
    parser.add_argument('--num-rows', type=int, default=None,
                      help='Number of rows to process (default: all rows)')
    parser.add_argument('--input-file', type=str, default='input_data.xlsx',
                      help='Input Excel file path (default: input_data.xlsx)')
    parser.add_argument('--output-file', type=str, default='output_data_v2.xlsx',
                      help='Output Excel file path (default: output_data_v2.xlsx)')
    parser.add_argument('--sheet', type=str, default='Trang tÃ­nh1',
                      help='Excel sheet name to process (default: Trang tÃ­nh1)')
    parser.add_argument('--batch-size', type=int, default=4,
                      help='Number of items to process in each batch (default: 4)')
    parser.add_argument('--max-workers', type=int, default=4,
                      help='Maximum number of worker threads (default: 4)')
    return parser.parse_args()

def process_batch(batch_rows: List[Dict]):
    output_rows = []
    for row in batch_rows:
        order = row.get('order', 'default_order')
        prompt = row['system_prompt']
        conversation_history = row.get('conversation_history', None)
        inputs = [row['user_input']]
        
        logger.info(f"Processing order {order}")
        
        responses, response_times, chat_messages, model_config = process_conversation(
            order, prompt, inputs, conversation_history
        )
        
        # Copy all columns and add new data
        new_row = row.copy()
        new_row.update({
            'assistant_response': responses[0] if responses else None,
            'response_time': response_times[0] if response_times else None,
            'model_config': model_config
        })
        output_rows.append(new_row)
    
    return output_rows

def append_to_excel(filename, df):
    """Append DataFrame to existing Excel file without reading entire file."""
    try:
        with pd.ExcelWriter(filename, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:
            workbook = writer.book
            worksheet = workbook['Sheet1']
            start_row = worksheet.max_row
            
            df.to_excel(writer, 
                       index=False, 
                       header=False,
                       startrow=start_row)
            
        return True
    except Exception as e:
        logger.error(f"Error appending to Excel: {str(e)}")
        return False

def get_system_resources():
    """Láº¥y thÃ´ng tin tÃ i nguyÃªn há»‡ thá»‘ng."""
    try:
        cpu_count = multiprocessing.cpu_count()
        memory = psutil.virtual_memory()
        available_memory_gb = memory.available / (1024 * 1024 * 1024)
        cpu_percent = psutil.cpu_percent(interval=1)
        
        logger.info(f"System resources:")
        logger.info(f"- CPU cores: {cpu_count}")
        logger.info(f"- CPU usage: {cpu_percent}%")
        logger.info(f"- Available memory: {available_memory_gb:.2f}GB")
        logger.info(f"- Memory usage: {memory.percent}%")
        
        return {
            'cpu_count': cpu_count,
            'available_memory_gb': available_memory_gb,
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent
        }
    except Exception as e:
        logger.error(f"Error getting system resources: {str(e)}")
        return None

def optimize_batch_parameters(total_rows: int, args):
    """Tá»‘i Æ°u batch_size vÃ  max_workers dá»±a trÃªn tÃ i nguyÃªn há»‡ thá»‘ng."""
    resources = get_system_resources()
    if not resources:
        logger.warning("Using default parameters due to resource check failure")
        return args.batch_size, args.max_workers
    
    # Tá»‘i Æ°u max_workers
    recommended_workers = min(
        resources['cpu_count'],
        int(8 * (100 - resources['cpu_percent']) / 100),
        int(resources['available_memory_gb'])
    )
    max_workers = max(1, min(recommended_workers, args.max_workers))
    
    # Tá»‘i Æ°u batch_size
    memory_based_batch = int(resources['available_memory_gb'] * 1024)
    cpu_based_batch = int(20 * (100 - resources['cpu_percent']) / 100)
    
    recommended_batch = min(
        memory_based_batch,
        cpu_based_batch,
        max(1, total_rows // (max_workers * 2))
    )
    batch_size = max(1, min(recommended_batch, args.batch_size))
    
    logger.info(f"Optimized parameters:")
    logger.info(f"- Original batch_size: {args.batch_size}, max_workers: {args.max_workers}")
    logger.info(f"- Recommended batch_size: {batch_size}, max_workers: {max_workers}")
    
    return batch_size, max_workers

def monitor_resources(stop_event, interval=30):
    """Monitor tÃ i nguyÃªn há»‡ thá»‘ng trong quÃ¡ trÃ¬nh xá»­ lÃ½."""
    while not stop_event.is_set():
        resources = get_system_resources()
        if resources:
            if resources['cpu_percent'] > 90:
                logger.warning("High CPU usage detected!")
            if resources['memory_percent'] > 90:
                logger.warning("High memory usage detected!")
        time.sleep(interval)

def main():
    args = parse_arguments()
    
    SCRIPTS_FOLDER = Path(__file__).parent
    INPUT_FILE = SCRIPTS_FOLDER / args.input_file
    OUTPUT_FILE = SCRIPTS_FOLDER / args.output_file

    # Kiá»ƒm tra file input tá»“n táº¡i
    if not INPUT_FILE.exists():
        logger.error(f"Input file not found: {INPUT_FILE}")
        return

    df_input = pd.read_excel(INPUT_FILE, sheet_name=args.sheet)
    rows_to_process = df_input if args.num_rows is None else df_input.head(args.num_rows)
    
    logger.info(f"Processing {len(rows_to_process)} rows from {INPUT_FILE}")
    
    # Tá»‘i Æ°u parameters
    batch_size, max_workers = optimize_batch_parameters(len(rows_to_process), args)
    
    all_rows = rows_to_process.to_dict('records')
    batches = [all_rows[i:i + batch_size] for i in range(0, len(all_rows), batch_size)]
    
    logger.info(f"Created {len(batches)} batches with batch_size={batch_size}, max_workers={max_workers}")
    
    # Táº¡o file output vá»›i headers
    cols_order = list(df_input.columns) + ['assistant_response', 'response_time', 'model_config']
    pd.DataFrame(columns=cols_order).to_excel(OUTPUT_FILE, index=False)
    
    # Khá»Ÿi Ä‘á»™ng monitor thread
    stop_monitoring = threading.Event()
    monitor_thread = threading.Thread(target=monitor_resources, args=(stop_monitoring,))
    monitor_thread.start()
    
    try:
        processed_count = 0
        failed_batches = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_batch, batch) for batch in batches]
            
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    batch_results = future.result()
                    batch_df = pd.DataFrame(batch_results)[cols_order]
                    
                    for attempt in range(3):
                        if append_to_excel(OUTPUT_FILE, batch_df):
                            processed_count += len(batch_results)
                            logger.info(f"Appended batch {i+1}/{len(batches)}. Total processed: {processed_count}")
                            break
                        else:
                            if attempt < 2:
                                logger.warning(f"Retry {attempt + 1} for batch {i+1}")
                                time.sleep(2)
                            else:
                                logger.error(f"Failed to append batch {i+1} after 3 attempts")
                                failed_batches.append((i, batch_results))
                                
                except Exception as e:
                    logger.error(f"Error processing batch {i+1}: {str(e)}")
                    failed_batches.append((i, None))

        # Xá»­ lÃ½ láº¡i cÃ¡c batch tháº¥t báº¡i
        if failed_batches:
            logger.info(f"Retrying {len(failed_batches)} failed batches...")
            for batch_index, batch_results in failed_batches:
                if batch_results:
                    batch_df = pd.DataFrame(batch_results)[cols_order]
                    if append_to_excel(OUTPUT_FILE, batch_df):
                        processed_count += len(batch_results)
                        logger.info(f"Successfully retried batch {batch_index+1}")
                    else:
                        logger.error(f"Permanently failed to append batch {batch_index+1}")

    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
    finally:
        # Dá»«ng monitor thread
        stop_monitoring.set()
        monitor_thread.join()
        
        # Log káº¿t quáº£ cuá»‘i cÃ¹ng
        logger.info(f"Processing completed. Total rows processed: {processed_count}")
        logger.info(f"Output saved to: {OUTPUT_FILE}")
        get_system_resources()

if __name__ == "__main__":
    main()

```


## 2.2.2 Gen lá»™ trÃ¬nh há»c - 20-50 luá»“ng Situations 1 lÃºc (trong 1 situations thÃ¬ cÃ³ 5 questions, má»—i questions thÃ¬ call 1 lÃºc 1 lesson detail (8 luá»“ng audio bÃªn trong), 1 lÃºc call 5 questions)

Link: [MiniProd_Web4_ContentEngFlow_StepUpE_T102024/1000Nghe_copyFrom_LS_ai_automation/utils_genLessonDetail1000Nghe_22082025_v2/batch_size_max_worker.py at main Â· DoanNgocCuong/MiniProd_Web4_ContentEngFlow_StepUpE_T102024](https://github.com/DoanNgocCuong/MiniProd_Web4_ContentEngFlow_StepUpE_T102024/blob/main/1000Nghe_copyFrom_LS_ai_automation/utils_genLessonDetail1000Nghe_22082025_v2/batch_size_max_worker.py)


```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Parallel Processing Module for Main Pipeline
===========================================

Features:
- Adaptive batch sizing based on data complexity
- Smart worker management with performance monitoring
- Thread-safe result collection
- Progress tracking and ETA calculation
- Error handling and retry mechanisms
- Memory usage optimization
"""

import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from typing import List, Dict, Any, Callable, Optional
import psutil
import math

class ParallelProcessor:
    """
    Advanced parallel processor with adaptive optimization
    """
    
    def __init__(self, 
                 max_workers: Optional[int] = None,
                 batch_size: Optional[int] = None,
                 memory_limit_mb: int = 1024,
                 enable_adaptive: bool = True,
                 timeout_seconds: int = 300):
        """
        Initialize parallel processor
        
        Args:
            max_workers: Maximum number of worker threads (None = auto-detect)
            batch_size: Items per batch (None = auto-calculate)
            memory_limit_mb: Memory limit in MB
            enable_adaptive: Enable adaptive optimization
        """
        self.max_workers = max_workers or self._auto_detect_workers()
        self.batch_size = batch_size
        self.memory_limit_mb = memory_limit_mb
        self.enable_adaptive = enable_adaptive
        self.timeout_seconds = timeout_seconds
        
        # Performance tracking
        self.start_time = None
        self.completed_items = 0
        self.total_items = 0
        self.error_count = 0
        self.retry_count = 0
        
        # Thread safety
        self.lock = threading.Lock()
        self.results = []
        
        # Memory monitoring
        self.process = psutil.Process()
        
    def _auto_detect_workers(self) -> int:
        """Auto-detect optimal number of workers based on system"""
        cpu_count = psutil.cpu_count(logical=True)
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # Conservative approach: don't overwhelm the system
        if memory_gb < 4:
            return min(2, cpu_count)
        elif memory_gb < 8:
            return min(4, cpu_count)
        else:
            return min(8, cpu_count)
    
    def _calculate_optimal_batch_size(self, total_items: int, avg_time_per_item: float = None) -> int:
        """Calculate optimal batch size based on data characteristics"""
        if self.batch_size:
            return self.batch_size
            
        # Adaptive batch sizing
        if self.enable_adaptive:
            # Estimate based on available memory
            available_memory_mb = psutil.virtual_memory().available / (1024**2)
            estimated_memory_per_item = 50  # MB per item (conservative estimate)
            
            memory_based_batch = max(1, int(available_memory_mb / estimated_memory_per_item))
            
            # Time-based optimization
            if avg_time_per_item:
                # Aim for batches that take 30-60 seconds to process
                target_batch_time = 45  # seconds
                time_based_batch = max(1, int(target_batch_time / avg_time_per_item))
            else:
                time_based_batch = 10
            
            # Worker-based optimization
            worker_based_batch = max(1, total_items // (self.max_workers * 2))
            
            # Take the minimum to be conservative
            optimal_batch = min(memory_based_batch, time_based_batch, worker_based_batch)
            
            return max(1, min(optimal_batch, 50))  # Between 1 and 50
        
        return 10  # Default fallback
    
    def _create_batches(self, items: List[Any], batch_size: int) -> List[List[Any]]:
        """Create batches from items list"""
        return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]
    
    def _monitor_memory(self) -> float:
        """Monitor current memory usage"""
        memory_info = self.process.memory_info()
        memory_mb = memory_info.rss / (1024**2)
        return memory_mb
    
    def _should_pause_for_memory(self) -> bool:
        """Check if we should pause for memory management"""
        current_memory = self._monitor_memory()
        return current_memory > self.memory_limit_mb
    
    def process_parallel(self, 
                        items: List[Any], 
                        process_func: Callable,
                        progress_callback: Optional[Callable] = None,
                        **kwargs) -> List[Any]:
        """
        Process items in parallel with advanced optimization
        
        Args:
            items: List of items to process
            process_func: Function to process each batch
            progress_callback: Optional callback for progress updates
            **kwargs: Additional arguments for process_func
            
        Returns:
            List of results from all batches
        """
        self.total_items = len(items)
        self.completed_items = 0
        self.start_time = datetime.now()
        self.results = []
        
        if not items:
            return []
        
        # Calculate optimal batch size
        batch_size = self._calculate_optimal_batch_size(len(items))
        batches = self._create_batches(items, batch_size)
        
        print(f"ğŸš€ Parallel Processing Setup:")
        print(f"   ğŸ“Š Total items: {self.total_items}")
        print(f"   ğŸ“¦ Batch size: {batch_size}")
        print(f"   ğŸ§µ Max workers: {self.max_workers}")
        print(f"   ğŸ“‹ Total batches: {len(batches)}")
        print(f"   ğŸ’¾ Memory limit: {self.memory_limit_mb}MB")
        print(f"   ğŸ”§ Adaptive mode: {'âœ…' if self.enable_adaptive else 'âŒ'}")
        
        # Process batches in parallel
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all batches - filter out timeout_seconds from kwargs
            process_kwargs = {k: v for k, v in kwargs.items() if k != 'timeout_seconds'}
            future_to_batch = {
                executor.submit(self._process_batch_with_retry, batch, process_func, **process_kwargs): batch
                for batch in batches
            }
            
            # Process completed batches with timeout
            for future in as_completed(future_to_batch):
                try:
                    # Add timeout to prevent hanging
                    batch_result = future.result(timeout=self.timeout_seconds)
                    with self.lock:
                        self.results.extend(batch_result)
                        self.completed_items += len(batch_result)
                    
                    # Progress update
                    if progress_callback:
                        progress_callback(self.completed_items, self.total_items)
                    
                    # Memory management
                    if self._should_pause_for_memory():
                        print(f"âš ï¸  High memory usage ({self._monitor_memory():.1f}MB), pausing...")
                        time.sleep(1)  # Brief pause
                    
                    # Performance monitoring
                    self._log_progress()
                    
                except TimeoutError:
                    self.error_count += 1
                    print(f"â° Batch processing timeout after {self.timeout_seconds}s, skipping...")
                    continue
                except Exception as e:
                    self.error_count += 1
                    print(f"âŒ Batch processing error: {e}")
                    continue
        
        # Final summary
        self._log_final_summary()
        return self.results
    
    def _process_batch_with_retry(self, 
                                 batch: List[Any], 
                                 process_func: Callable,
                                 max_retries: int = 2,
                                 **kwargs) -> List[Any]:
        """Process a batch with retry mechanism"""
        for attempt in range(max_retries + 1):
            try:
                return process_func(batch, **kwargs)
            except Exception as e:
                if attempt < max_retries:
                    self.retry_count += 1
                    print(f"ğŸ”„ Retry {attempt + 1}/{max_retries} for batch due to: {e}")
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    print(f"ğŸ’¥ Batch failed after {max_retries} retries: {e}")
                    return []  # Return empty result for failed batch
    
    def _log_progress(self):
        """Log current progress with ETA"""
        if self.completed_items == 0:
            return
            
        elapsed_time = (datetime.now() - self.start_time).total_seconds()
        avg_time_per_item = elapsed_time / self.completed_items
        remaining_items = self.total_items - self.completed_items
        estimated_remaining_time = avg_time_per_item * remaining_items
        
        progress_percent = (self.completed_items / self.total_items) * 100
        eta = datetime.now() + timedelta(seconds=estimated_remaining_time)
        
        memory_usage = self._monitor_memory()
        
        print(f"ğŸ“Š Progress: {self.completed_items}/{self.total_items} ({progress_percent:.1f}%)")
        print(f"   â±ï¸  Elapsed: {elapsed_time/60:.1f}min | ETA: {eta.strftime('%H:%M:%S')}")
        print(f"   ğŸš€ Speed: {avg_time_per_item:.1f}s/item | ğŸ’¾ Memory: {memory_usage:.1f}MB")
        print(f"   âŒ Errors: {self.error_count} | ğŸ”„ Retries: {self.retry_count}")
    
    def _log_final_summary(self):
        """Log final processing summary"""
        total_time = (datetime.now() - self.start_time).total_seconds()
        success_rate = ((self.total_items - self.error_count) / self.total_items) * 100
        
        print(f"\nğŸ‰ Parallel Processing Complete!")
        print(f"   â±ï¸  Total time: {total_time/60:.1f} minutes")
        print(f"   ğŸ“Š Success rate: {success_rate:.1f}%")
        print(f"   âŒ Errors: {self.error_count}")
        print(f"   ğŸ”„ Retries: {self.retry_count}")
        print(f"   ğŸ’¾ Peak memory: {self._monitor_memory():.1f}MB")
        
        if self.total_items > 0:
            throughput = self.total_items / total_time
            print(f"   ğŸš€ Throughput: {throughput:.2f} items/second")


# Convenience functions for easy integration
def process_situations_parallel(situations: List[Dict], 
                               process_func: Callable,
                               max_workers: int = None,
                               batch_size: int = None,
                               timeout_seconds: int = 300,
                               **kwargs) -> List[Dict]:
    """
    Convenience function to process situations in parallel
    
    Args:
        situations: List of situation dictionaries
        process_func: Function to process each situation
        max_workers: Maximum worker threads
        batch_size: Items per batch
        **kwargs: Additional arguments for process_func
        
    Returns:
        List of processed results
    """
    processor = ParallelProcessor(
        max_workers=max_workers,
        batch_size=batch_size,
        enable_adaptive=True
    )
    
    def progress_callback(completed, total):
        if completed % 10 == 0:  # Update every 10 items
            print(f"ğŸ”„ Processed {completed}/{total} situations...")
    
    return processor.process_parallel(
        items=situations,
        process_func=process_func,
        progress_callback=progress_callback,
        timeout_seconds=timeout_seconds,
        **kwargs
    )


if __name__ == "__main__":
    # Demo usage
    def demo_process_func(batch, **kwargs):
        """Demo processing function"""
        time.sleep(0.1)  # Simulate work
        return [f"Processed: {item}" for item in batch]
    
    # Test data
    test_items = list(range(100))
    
    print("ğŸ§ª Testing Parallel Processor...")
    processor = ParallelProcessor(max_workers=4, batch_size=10)
    results = processor.process_parallel(test_items, demo_process_func)
    
    print(f"âœ… Demo completed with {len(results)} results")
```


### 2.3 ÄÃ³ng API lá»™ trÃ¬nh há»c 

```
/**

Â * ================================================================================

Â * OPTIMIZED LEARNING PIPELINE CONTROLLER - A12 + PARALLEL A3 IMPLEMENTATION

Â * ================================================================================

Â *

Â * Author: Doan Ngoc Cuong (Optimized Version)

Â * Date: 2025-09-11

Â * Version: 4.0.0 - Performance Optimized

Â *

Â * Description:

Â * API tá»‘i Æ°u hÃ³a vá»›i 2-step approach thay vÃ¬ 3-step sequential:

Â * Â - A12: generateA1andA2_Combined (job_role â†’ topics/situations trá»±c tiáº¿p)

Â * Â - A3_PARALLEL: generateFullLearningPipeline song song cho tá»«ng situation

Â * Â - MERGE: Tá»•ng há»£p káº¿t quáº£ cuá»‘i cÃ¹ng

Â *

Â * Performance Benefits:

Â * - Giáº£m tá»« 3 steps sequential xuá»‘ng 2 steps (A12 + parallel A3)

Â * - Parallel processing thay vÃ¬ sequential cho A3 calls

Â * - Time complexity: O(1) + O(max_parallel) thay vÃ¬ O(n) sequential

Â * - Estimated speedup: 60-80% cho scenarios >= 3

Â *

Â * ===================================================================================

Â * OPTIMIZED WORKFLOW COMPARISON

Â * ===================================================================================

Â *

Â * OLD APPROACH (Sequential):

Â * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â * â”‚ INPUT â†’ [A1: 3s] â†’ [A2: 4s] â†’ [A3: 15s Ã— N situations] Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”‚ Total: 7s + (15s Ã— N) = 7s + 75s (for 5 situations) = 82s Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â *

Â * NEW APPROACH (Optimized):

Â * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â * â”‚ INPUT â†’ [A12: 5s] â†’ [A3 Parallel: max(15s)] â†’ [MERGE: 1s] Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”‚ Total: 5s + 15s + 1s = 21s (for 5 situations in parallel) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”‚ Time Saved: 82s - 21s = 61s (74% faster!) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â *

Â * ===================================================================================

Â * ARCHITECTURE OVERVIEW

Â * ===================================================================================

Â *

Â * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â * â”‚ STEP A12: COMBINED JOB_ROLE â†’ TOPICS/SITUATIONS Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

Â * â”‚ Function: generateA1andA2_UserInput2JTBD2TopicAndSituation Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”‚ Input: { data: { job_role: "AI Engineer" } } Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”‚ Output: Direct topics with situations (bypasses JTBD intermediate step) Â  Â  Â  Â â”‚

Â * â”‚ Performance: Single API call thay vÃ¬ 2 calls (A1â†’A2) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”‚ Model: Combined OpenAI + Gemini logic in one optimized prompt Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

Â * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â *

Â * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â * â”‚ STEP EXTRACT: SITUATION EXTRACTION & PARALLELIZATION PREP Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

Â * â”‚ Logic: Extract all situations from all topics into flat array Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

Â * â”‚ Input: A12 output (topics with nested situations) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

Â * â”‚ Output: Flat array of situations with metadata for parallel processing Â  Â  Â  Â  â”‚

Â * â”‚ Features: ID generation, batch grouping, load balancing preparation Â  Â  Â  Â  Â  Â â”‚

Â * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â *

Â * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â * â”‚ STEP A3_PARALLEL: CONCURRENT FULL PIPELINE EXECUTION Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

Â * â”‚ Function: generateFullLearningPipeline (parallel execution) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

Â * â”‚ Input: Individual situation + generateQuestions4Inputs format Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

Â * â”‚ Output: Complete learning pipeline per situation Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”‚ Concurrency: Controlled parallel execution with configurable limits Â  Â  Â  Â  Â  â”‚

Â * â”‚ Error Handling: Individual failures don't affect other situations Â  Â  Â  Â  Â  Â  Â â”‚

Â * â”‚ Performance: Processing time = max(individual_time) vs sum(individual_time) Â  Â â”‚

Â * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â *

Â * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

Â * â”‚ STEP MERGE: INTELLIGENT RESULT AGGREGATION Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

Â * â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

Â * â”‚ Logic: Organize parallel results back into topic/situation hierarchy Â  Â  Â  Â  Â  â”‚

Â * â”‚ Input: Array of individual A3 results Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

Â * â”‚ Output: Structured response with comprehensive metadata Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

Â * â”‚ Features: Success/failure tracking, performance metrics, partial success Â  Â  Â  â”‚

Â * â”‚ Quality: Error isolation, detailed reporting, optimization statistics Â  Â  Â  Â  Â â”‚

Â * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Â *

Â * ===================================================================================

Â * PERFORMANCE OPTIMIZATIONS

Â * ===================================================================================

Â *

Â * 1. Reduced API Round-trips:

Â * Â  Â - A1+A2 combined into single A12 call

Â * Â  Â - Eliminates intermediate JTBD step

Â * Â  Â - Reduces latency overhead

Â *

Â * 2. Parallel Processing:

Â * Â  Â - Concurrent A3 execution for all situations

Â * Â  Â - Configurable parallelism (default: 5 concurrent)

Â * Â  Â - Resource-efficient batch processing

Â *

Â * 3. Error Isolation:

Â * Â  Â - Individual situation failures don't cascade

Â * Â  Â - Partial success handling

Â * Â  Â - Detailed error reporting per situation

Â *

Â * 4. Memory Optimization:

Â * Â  Â - Streaming result processing

Â * Â  Â - Configurable batch sizes

Â * Â  Â - Garbage collection friendly

Â *

Â * 5. Smart Resource Management:

Â * Â  Â - Adaptive concurrency based on system load

Â * Â  Â - Rate limiting for external APIs

Â * Â  Â - Connection pooling optimization

Â *

Â * ===================================================================================

Â * SCALABILITY FEATURES

Â * ===================================================================================

Â *

Â * - Horizontal scaling: Easy to distribute across multiple servers

Â * - Load balancing: Intelligent work distribution

Â * - Circuit breaker: Automatic failure handling

Â * - Monitoring: Comprehensive metrics and logging

Â * - Caching: Result caching for repeated requests

Â *

Â * ===================================================================================

Â * API ENDPOINTS

Â * ===================================================================================

Â *

Â * Main Endpoint:

Â * POST /api/generate-learning-path-user-input-to-done-full

Â *

Â * Health Check:

Â * GET /api/health/optimized-pipeline-status

Â *

Â * Performance Metrics:

Â * GET /api/metrics/pipeline-performance

Â *

Â */

  

// ===================================================================================

// DEPENDENCIES AND IMPORTS

// ===================================================================================

  

const { generateA1andA2_UserInput2JTBD2TopicAndSituation } = require('./generateLearningPathV4T92025_A1andA2_UserInput2JTBD2TopicAndSituation.js');

const { generateFullLearningPipeline } = require('./generateLearningPathV4T92025_A3_Question4Inputs2FinalLessonDetail.js');

const { call_api_with_retry } = require('@utils/utils_call_api_with_retry');

  

// ===================================================================================

// CONFIGURATION CONSTANTS

// ===================================================================================

  

/**

Â * Performance and concurrency configuration

Â *

Â * Clarification: Batch size vs Max workers (this implementation uses Max workers)

Â * - Batch size (data-centric): kÃ­ch thÆ°á»›c má»™t nhÃ³m dá»¯ liá»‡u xá»­ lÃ½ trong 1 lÆ°á»£t.

Â * Â  - ThÆ°á»ng dÃ¹ng trong training ML hoáº·c xá»­ lÃ½ I/O theo lÃ´; áº£nh hÆ°á»Ÿng memory vÃ  convergence.

Â * - Max workers (parallelism-centric): sá»‘ tÃ¡c vá»¥ cháº¡y Ä‘á»“ng thá»i (Ä‘á»™ song song).

Â * Â  - DÃ¹ng Ä‘á»ƒ kiá»ƒm soÃ¡t throughput vÃ  má»©c sá»­ dá»¥ng CPU core trong há»‡ thá»‘ng song song.

Â *

Â * Trong file nÃ y, cáº¥u hÃ¬nh Ä‘iá»u khiá»ƒn TRá»°C TIáº¾P sá»‘ situation cháº¡y Ä‘á»“ng thá»i â†’ dÃ¹ng tÃªn MAX_WORKERS

Â * Ä‘á»ƒ trÃ¡nh nháº§m láº«n vá»›i khÃ¡i niá»‡m "batch size" mang tÃ­nh cháº¥t dá»¯ liá»‡u.

---

1 Situation gen khoáº£ng 1.5min (cÃ¡c cÆ¡ cháº¿ song song bÃªn trong)

Gen theo Situation vá»›i luá»“ng 10-20-30-50 situations.

(vá»›i 20 luá»“ng ko lá»—i (do cÃ³ retry audio vÃ  check Audio, retry OpenAI, retry tá»«ng API con, retry Ä‘áº¿m xem Ä‘á»§ scenario chÆ°a trong 1 situation)

, cÃ²n 40-50 luá»“ng tá»‰ lá»‡ lá»—i khoáº£ng 2%)

  

---

- KhÃ´ng. Trong code hiá»‡n táº¡i:

Â  - Thá»±c thi song song thá»±c táº¿ = MAX_WORKERS.

Â  - Batches cháº¡y tuáº§n tá»±; MAX_CONCURRENT_BATCHES chÆ°a Ä‘Æ°á»£c dÃ¹ng.

Â  - MAX_CONCURRENT_A3 chá»‰ log, chÆ°a cháº·n gÃ¬.

  

Muá»‘n Ä‘Ãºng nhÆ° báº¡n nÃ³i: tá»•ng song song = min(MAX_CONCURRENT_A3, MAX_CONCURRENT_BATCHES Ã— MAX_WORKERS). Cáº§n:

- Cháº¡y nhiá»u batch Ä‘á»“ng thá»i theo MAX_CONCURRENT_BATCHES.

- ThÃªm limiter tá»•ng Ä‘á»ƒ cap theo MAX_CONCURRENT_A3.

Â */

const OPTIMIZED_CONFIG = {

Â  Â  // Parallel processing limits

Â  Â  MAX_CONCURRENT_A3: 5, Â  Â  Â  Â  Â  Â  Â // Maximum parallel A3 calls

Â  Â  MAX_CONCURRENT_BATCHES: 2, Â  Â  Â  Â  // Maximum parallel batches

Â  Â  MAX_WORKERS: 30, Â  Â  Â  Â  Â  Â  Â  Â  Â  Â // Sá»‘ situation cháº¡y Ä‘á»“ng thá»i trong 1 lÆ°á»£t

Â  Â  // Timeouts and retries

Â  Â  A12_TIMEOUT_MS: 300000, Â  Â  Â  Â  Â  Â // A12 generation timeout (5 minutes)

Â  Â  A3_TIMEOUT_MS: 300000, Â  Â  Â  Â  Â  Â  Â // A3 individual timeout

Â  Â  MAX_RETRIES: 3, Â  Â  Â  Â  Â  Â  Â  Â  Â  Â // Retry attempts per operation

Â  Â  // Memory and performance

Â  Â  MEMORY_LIMIT_MB: 512, Â  Â  Â  Â  Â  Â  Â // Memory limit per process

Â  Â  GC_INTERVAL_MS: 30000, Â  Â  Â  Â  Â  Â  // Garbage collection interval

Â  Â  // Monitoring and logging

Â  Â  ENABLE_DETAILED_LOGGING: true, Â  Â  // Detailed execution logging

Â  Â  ENABLE_PERFORMANCE_TRACKING: true, // Performance metrics collection

Â  Â  ENABLE_ERROR_TRACKING: true, Â  Â  Â  // Error analytics

Â  Â  // Collection ID generation

Â  Â  COLLECTION_PREFIX: 'OPT', Â  Â  Â  Â  Â // Optimized pipeline prefix

Â  Â  TOPIC_PREFIX: 'TOP', Â  Â  Â  Â  Â  Â  Â  // Topic ID prefix

Â  Â  SITUATION_PREFIX: 'SIT' Â  Â  Â  Â  Â  Â // Situation ID prefix

};

  

/**

Â * Error handling configuration

Â */

const ERROR_CONFIG = {

Â  Â  // Retry strategies

Â  Â  EXPONENTIAL_BACKOFF: true, Â  Â  Â  Â  // Use exponential backoff for retries

Â  Â  MAX_BACKOFF_MS: 5000, Â  Â  Â  Â  Â  Â  // Maximum backoff time

Â  Â  // Error isolation

Â  Â  CONTINUE_ON_PARTIAL_FAILURE: true, // Continue processing despite individual failures

Â  Â  MIN_SUCCESS_RATE: 0.7, Â  Â  Â  Â  Â  Â // Minimum success rate to consider pipeline successful

Â  Â  // Fallback options

Â  Â  ENABLE_GRACEFUL_DEGRADATION: true, // Enable fallback to sequential processing

Â  Â  FALLBACK_THRESHOLD: 0.5 Â  Â  Â  Â  Â  // Failure rate threshold for fallback

};

  

// ===================================================================================

// UTILITY FUNCTIONS

// ===================================================================================

  

/**

Â * Enhanced controller caller for A12 with comprehensive error handling

Â *

Â * @param {Function} controllerFn - A12 controller function

Â * @param {Object} body - Request body

Â * @param {string} operation - Operation name for logging

Â * @returns {Promise<Object>} A12 execution result

Â */

const callA12Controller = (controllerFn, body, operation = 'A12_Generation') => {

Â  Â  return new Promise((resolve) => {

Â  Â  Â  Â  const startTime = Date.now();

Â  Â  Â  Â  if (OPTIMIZED_CONFIG.ENABLE_DETAILED_LOGGING) {

Â  Â  Â  Â  Â  Â  console.log(`ğŸš€ [${operation}] Starting A12 generation...`);

Â  Â  Â  Â  Â  Â  console.log(`ğŸ“Š [${operation}] Input: job_role="${body?.data?.job_role}"`);

Â  Â  Â  Â  }

  

Â  Â  Â  Â  const req = { body };

Â  Â  Â  Â  const res = {

Â  Â  Â  Â  Â  Â  status(code) {

Â  Â  Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  json(payload) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const duration = Date.now() - startTime;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.error(`âŒ [${operation}] Failed with status ${code} in ${duration}ms`);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  resolve({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  success: false,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  httpStatus: code,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  payload,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  operation,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  duration_ms: duration,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: `A12 failed with status ${code}`

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  json(payload) {

Â  Â  Â  Â  Â  Â  Â  Â  const duration = Date.now() - startTime;

Â  Â  Â  Â  Â  Â  Â  Â  if (OPTIMIZED_CONFIG.ENABLE_DETAILED_LOGGING) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.log(`âœ… [${operation}] Completed successfully in ${duration}ms`);

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  resolve({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  success: true,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  httpStatus: 200,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  payload,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  operation,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  duration_ms: duration

Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  };

  

Â  Â  Â  Â  // Execute with timeout protection

Â  Â  Â  Â  const timeoutId = setTimeout(() => {

Â  Â  Â  Â  Â  Â  const duration = Date.now() - startTime;

Â  Â  Â  Â  Â  Â  console.error(`â° [${operation}] Timeout after ${duration}ms`);

Â  Â  Â  Â  Â  Â  resolve({

Â  Â  Â  Â  Â  Â  Â  Â  success: false,

Â  Â  Â  Â  Â  Â  Â  Â  httpStatus: 408,

Â  Â  Â  Â  Â  Â  Â  Â  payload: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status: false,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: `${operation} timed out after ${OPTIMIZED_CONFIG.A12_TIMEOUT_MS}ms`

Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  operation,

Â  Â  Â  Â  Â  Â  Â  Â  duration_ms: duration,

Â  Â  Â  Â  Â  Â  Â  Â  error: 'Timeout'

Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  }, OPTIMIZED_CONFIG.A12_TIMEOUT_MS);

  

Â  Â  Â  Â  Promise.resolve(controllerFn(req, res))

Â  Â  Â  Â  Â  Â  .then(() => clearTimeout(timeoutId))

Â  Â  Â  Â  Â  Â  .catch((error) => {

Â  Â  Â  Â  Â  Â  Â  Â  clearTimeout(timeoutId);

Â  Â  Â  Â  Â  Â  Â  Â  const duration = Date.now() - startTime;

Â  Â  Â  Â  Â  Â  Â  Â  console.error(`ğŸ’¥ [${operation}] Exception in ${duration}ms:`, error.message);

Â  Â  Â  Â  Â  Â  Â  Â  resolve({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  success: false,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  httpStatus: 500,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  payload: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status: false,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: error?.message || 'Unknown A12 error'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  operation,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  duration_ms: duration,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: error.message

Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  });

Â  Â  });

};

  

/**

Â * Extract and flatten all situations from A12 output for parallel processing

Â *

Â * @param {Object} a12Output - A12 generation result

Â * @param {string} job_role - Original job role for context

Â * @returns {Array} Flattened array of situations with metadata

Â */

function extractSituationsForParallelProcessing(a12Output, job_role) {

Â  Â  console.log('ğŸ” [EXTRACT] Extracting situations for parallel processing...');

Â  Â  try {

Â  Â  Â  Â  const { step_a2 } = a12Output;

Â  Â  Â  Â  if (!step_a2?.data?.output || !Array.isArray(step_a2.data.output)) {

Â  Â  Â  Â  Â  Â  throw new Error('Invalid A12 output: missing step_a2.data.output array');

Â  Â  Â  Â  }

  

Â  Â  Â  Â  const topics = step_a2.data.output;

Â  Â  Â  Â  console.log(`ğŸ“Š [EXTRACT] Processing ${topics.length} topics from A12 output`);

  

Â  Â  Â  Â  const allSituations = [];

Â  Â  Â  Â  let globalSituationIndex = 0;

  

Â  Â  Â  Â  topics.forEach((topic, topicIndex) => {

Â  Â  Â  Â  Â  Â  const { topic_name, situations } = topic;

Â  Â  Â  Â  Â  Â  if (!situations || !Array.isArray(situations) || situations.length === 0) {

Â  Â  Â  Â  Â  Â  Â  Â  console.warn(`âš ï¸ [EXTRACT] Topic ${topicIndex + 1} "${topic_name}" has no valid situations`);

Â  Â  Â  Â  Â  Â  Â  Â  return;

Â  Â  Â  Â  Â  Â  }

  

Â  Â  Â  Â  Â  Â  console.log(`ğŸ“ [EXTRACT] Topic ${topicIndex + 1}: "${topic_name}" has ${situations.length} situations`);

  

Â  Â  Â  Â  Â  Â  situations.forEach((situation, situationIndex) => {

Â  Â  Â  Â  Â  Â  Â  Â  const {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stakeholder,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  scenario_description,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_details

Â  Â  Â  Â  Â  Â  Â  Â  } = situation;

  

Â  Â  Â  Â  Â  Â  Â  Â  // Generate unique identifiers

Â  Â  Â  Â  Â  Â  Â  Â  const collection_id = `${OPTIMIZED_CONFIG.COLLECTION_PREFIX}${String(globalSituationIndex + 1).padStart(3, '0')}`;

Â  Â  Â  Â  Â  Â  Â  Â  const topic_id = `${OPTIMIZED_CONFIG.TOPIC_PREFIX}${String(topicIndex + 1).padStart(3, '0')}`;

Â  Â  Â  Â  Â  Â  Â  Â  const situation_id = `${OPTIMIZED_CONFIG.SITUATION_PREFIX}${String(situationIndex + 1).padStart(3, '0')}`;

  

Â  Â  Â  Â  Â  Â  Â  Â  // Create A3 input format

Â  Â  Â  Â  Â  Â  Â  Â  const a3Input = {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  collection_id,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_id,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_id,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  generateQuestions4Inputs: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  job_role,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  output: [{

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situations: [situation] // Single situation for individual A3 processing

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  };

  

Â  Â  Â  Â  Â  Â  Â  Â  allSituations.push({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Identifiers

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  global_index: globalSituationIndex,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_index: topicIndex,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_index: situationIndex,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  collection_id,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_id,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_id,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Content

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stakeholder,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  scenario_description,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  questions: situation_details?.stakeholder_ask || [],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // A3 processing input

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3_input: a3Input,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Metadata for tracking

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  metadata: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  created_at: new Date().toISOString(),

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  extraction_order: globalSituationIndex + 1,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  belongs_to_topic: topic_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  question_count: situation_details?.stakeholder_ask?.length || 0

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  });

  

Â  Â  Â  Â  Â  Â  Â  Â  globalSituationIndex++;

Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  });

  

Â  Â  Â  Â  console.log(`âœ… [EXTRACT] Successfully extracted ${allSituations.length} situations for parallel processing`);

Â  Â  Â  Â  return allSituations;

  

Â  Â  } catch (error) {

Â  Â  Â  Â  console.error('âŒ [EXTRACT] Extraction failed:', error.message);

Â  Â  Â  Â  throw new Error(`Situation extraction failed: ${error.message}`);

Â  Â  }

}

  

/**

Â * Execute parallel A3 processing with intelligent batch management

Â *

Â * @param {Array} situations - Array of situations to process

Â * @returns {Promise<Array>} Array of A3 processing results

Â */

async function executeParallelA3Processing(situations) {

Â  Â  const startTime = Date.now();

Â  Â  console.log(`âš¡ [A3_PARALLEL] Starting parallel processing for ${situations.length} situations...`);

Â  Â  // Configuration

Â  Â  const {

Â  Â  Â  Â  MAX_CONCURRENT_A3,

Â  Â  Â  Â  MAX_WORKERS,

Â  Â  Â  Â  A3_TIMEOUT_MS,

Â  Â  Â  Â  ENABLE_DETAILED_LOGGING

Â  Â  } = OPTIMIZED_CONFIG;

  

Â  Â  console.log(`âš™ï¸ [A3_PARALLEL] Configuration: max_concurrent=${MAX_CONCURRENT_A3}, max_workers=${MAX_WORKERS}`);

  

Â  Â  // Create batches for controlled parallel processing

Â  Â  const batches = [];

Â  Â  for (let i = 0; i < situations.length; i += MAX_WORKERS) {

Â  Â  Â  Â  batches.push(situations.slice(i, i + MAX_WORKERS));

Â  Â  }

  

Â  Â  console.log(`ğŸ“¦ [A3_PARALLEL] Processing ${batches.length} batches...`);

Â  Â  const allResults = [];

Â  Â  let totalSuccessful = 0;

Â  Â  let totalFailed = 0;

Â  Â  // Process batches with controlled concurrency

Â  Â  for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {

Â  Â  Â  Â  const batch = batches[batchIndex];

Â  Â  Â  Â  const batchStartTime = Date.now();

Â  Â  Â  Â  console.log(`ğŸ”„ [A3_PARALLEL] Processing batch ${batchIndex + 1}/${batches.length} (${batch.length} situations)...`);

Â  Â  Â  Â  // Create promises for parallel execution within batch

Â  Â  Â  Â  const batchPromises = batch.map(async (situation) => {

Â  Â  Â  Â  Â  Â  const { global_index, collection_id, topic_id, situation_id, a3_input } = situation;

Â  Â  Â  Â  Â  Â  const operationName = `A3_${global_index + 1}_${collection_id}`;

Â  Â  Â  Â  Â  Â  try {

Â  Â  Â  Â  Â  Â  Â  Â  if (ENABLE_DETAILED_LOGGING) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.log(`ğŸ¯ [A3_PARALLEL] Starting ${operationName}: ${situation.situation_name}`);

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  // Create A3 execution promise with timeout

Â  Â  Â  Â  Â  Â  Â  Â  const a3Promise = new Promise((resolve, reject) => {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let a3Result = null;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const mockRes = {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  json: (data) => {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3Result = data;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  resolve(a3Result);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  status: (code) => ({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  json: (data) => {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reject(new Error(`A3 failed with status ${code}: ${JSON.stringify(data)}`));

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const mockReq = { body: a3_input };

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  generateFullLearningPipeline(mockReq, mockRes)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .then(() => {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (!a3Result) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reject(new Error('A3 completed without returning data'));

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .catch(reject);

Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  Â  Â  // Execute with timeout

Â  Â  Â  Â  Â  Â  Â  Â  const result = await Promise.race([

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3Promise,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new Promise((_, reject) =>

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  setTimeout(() => reject(new Error(`A3 timeout after ${A3_TIMEOUT_MS}ms`)), A3_TIMEOUT_MS)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  ]);

Â  Â  Â  Â  Â  Â  Â  Â  if (ENABLE_DETAILED_LOGGING) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.log(`âœ… [A3_PARALLEL] ${operationName} completed successfully`);

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  success: true,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_metadata: situation.metadata,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  identifiers: { global_index, collection_id, topic_id, situation_id },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_name: situation.topic_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_name: situation.situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3_result: result,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_time_ms: Date.now() - batchStartTime,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: null

Â  Â  Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  } catch (error) {

Â  Â  Â  Â  Â  Â  Â  Â  console.error(`âŒ [A3_PARALLEL] ${operationName} failed:`, error.message);

Â  Â  Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  success: false,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_metadata: situation.metadata,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  identifiers: { global_index, collection_id, topic_id, situation_id },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_name: situation.topic_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_name: situation.situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3_result: null,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_time_ms: Date.now() - batchStartTime,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: error.message

Â  Â  Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  });

Â  Â  Â  Â  // Wait for current batch to complete

Â  Â  Â  Â  const batchResults = await Promise.all(batchPromises);

Â  Â  Â  Â  allResults.push(...batchResults);

Â  Â  Â  Â  // Update statistics

Â  Â  Â  Â  const batchSuccessful = batchResults.filter(r => r.success).length;

Â  Â  Â  Â  const batchFailed = batchResults.length - batchSuccessful;

Â  Â  Â  Â  totalSuccessful += batchSuccessful;

Â  Â  Â  Â  totalFailed += batchFailed;

Â  Â  Â  Â  const batchDuration = Date.now() - batchStartTime;

Â  Â  Â  Â  console.log(`âœ… [A3_PARALLEL] Batch ${batchIndex + 1} completed in ${batchDuration}ms: ${batchSuccessful}/${batchResults.length} successful`);

Â  Â  Â  Â  // Optional: Brief pause between batches to manage system load

Â  Â  Â  Â  if (batchIndex < batches.length - 1) {

Â  Â  Â  Â  Â  Â  await new Promise(resolve => setTimeout(resolve, 100));

Â  Â  Â  Â  }

Â  Â  }

  

Â  Â  const totalDuration = Date.now() - startTime;

Â  Â  console.log(`ğŸ‰ [A3_PARALLEL] All parallel processing completed in ${totalDuration}ms`);

Â  Â  console.log(`ğŸ“Š [A3_PARALLEL] Final results: ${totalSuccessful}/${allResults.length} successful, ${totalFailed} failed`);

  

Â  Â  // Calculate performance metrics

Â  Â  const avgProcessingTime = allResults

Â  Â  Â  Â  .filter(r => r.success && r.processing_time_ms > 0)

Â  Â  Â  Â  .reduce((sum, r) => sum + r.processing_time_ms, 0) / totalSuccessful || 0;

  

Â  Â  if (OPTIMIZED_CONFIG.ENABLE_PERFORMANCE_TRACKING) {

Â  Â  Â  Â  console.log(`ğŸ“ˆ [A3_PARALLEL] Performance metrics:`, {

Â  Â  Â  Â  Â  Â  total_duration_ms: totalDuration,

Â  Â  Â  Â  Â  Â  avg_processing_time_ms: avgProcessingTime.toFixed(2),

Â  Â  Â  Â  Â  Â  success_rate: `${((totalSuccessful / allResults.length) * 100).toFixed(1)}%`,

Â  Â  Â  Â  Â  Â  situations_per_second: (allResults.length / (totalDuration / 1000)).toFixed(2)

Â  Â  Â  Â  });

Â  Â  }

  

Â  Â  return allResults;

}

  

/**

Â * Create simplified learning structure with only important keys

Â * Structure: JOB_ROLE â†’ TOPICS â†’ SITUATIONS â†’ pipeline_results

Â *

Â * @param {string} job_role - Job role name

Â * @param {Array} processedTopics - Processed topics with situations

Â * @returns {Object} Simplified structure

Â */

function createSimplifiedLearningStructure(job_role, processedTopics) {

Â  Â  console.log('ğŸ”§ [SIMPLIFY] Creating simplified learning structure...');

Â  Â  const simplifiedStructure = {

Â  Â  Â  Â  job_role: job_role,

Â  Â  Â  Â  topics: []

Â  Â  };

  

Â  Â  processedTopics.forEach((topic) => {

Â  Â  Â  Â  const simplifiedTopic = {

Â  Â  Â  Â  Â  Â  topic_name: topic.topic_name,

Â  Â  Â  Â  Â  Â  situations: []

Â  Â  Â  Â  };

  

Â  Â  Â  Â  topic.situations.forEach((situation) => {

Â  Â  Â  Â  Â  Â  const simplifiedSituation = {

Â  Â  Â  Â  Â  Â  Â  Â  situation_name: situation.situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  stakeholder: situation.stakeholder,

Â  Â  Â  Â  Â  Â  Â  Â  scenario_description: situation.scenario_description,

Â  Â  Â  Â  Â  Â  Â  Â  image_url_situation: situation.image_url_situation || null, // Thumbnail link from A12

Â  Â  Â  Â  Â  Â  Â  Â  pipeline_results: null

Â  Â  Â  Â  Â  Â  };

  

Â  Â  Â  Â  Â  Â  // Extract pipeline_results from learning_content if available

Â  Â  Â  Â  Â  Â  if (situation.learning_content && situation.learning_content.pipeline_results) {

Â  Â  Â  Â  Â  Â  Â  Â  simplifiedSituation.pipeline_results = situation.learning_content.pipeline_results;

Â  Â  Â  Â  Â  Â  } else if (situation.learning_content && situation.learning_content.pipeline_results === null) {

Â  Â  Â  Â  Â  Â  Â  Â  // Handle case where pipeline_results is explicitly null

Â  Â  Â  Â  Â  Â  Â  Â  simplifiedSituation.pipeline_results = null;

Â  Â  Â  Â  Â  Â  } else {

Â  Â  Â  Â  Â  Â  Â  Â  // Handle missing or failed learning_content

Â  Â  Â  Â  Â  Â  Â  Â  simplifiedSituation.pipeline_results = null;

Â  Â  Â  Â  Â  Â  }

  

Â  Â  Â  Â  Â  Â  simplifiedTopic.situations.push(simplifiedSituation);

Â  Â  Â  Â  });

  

Â  Â  Â  Â  simplifiedStructure.topics.push(simplifiedTopic);

Â  Â  });

  

Â  Â  console.log(`âœ… [SIMPLIFY] Created simplified structure: ${simplifiedStructure.topics.length} topics`);

Â  Â  return simplifiedStructure;

}

  

/**

Â * UPDATED MERGE FUNCTION - OUTSIDE-IN APPROACH

Â * Replaces the old inside-out merge logic with natural hierarchy traversal

Â */

  

/**

Â * Intelligent result merging and organization using Outside-In approach

Â *

Â * @param {Object} a12Output - Original A12 output

Â * @param {Array} a3Results - Parallel A3 processing results

Â * @param {string} job_role - Original job role

Â * @returns {Object} Organized final response

Â */

function mergeAndOrganizeResults(a12Output, a3Results, job_role) {

Â  Â  console.log('ğŸ”§ [MERGE_OUTSIDE_IN] Starting outside-in intelligent result merging...');

Â  Â  const mergeStartTime = Date.now();

Â  Â  try {

Â  Â  Â  Â  // ========== INPUT VALIDATION ==========

Â  Â  Â  Â  if (!a12Output?.step_a2?.data?.output) {

Â  Â  Â  Â  Â  Â  throw new Error('Invalid A12 output: missing step_a2.data.output array');

Â  Â  Â  Â  }

Â  Â  Â  Â  if (!Array.isArray(a3Results)) {

Â  Â  Â  Â  Â  Â  throw new Error('A3 results must be an array');

Â  Â  Â  Â  }

  

Â  Â  Â  Â  // ========== CREATE A3 RESULTS LOOKUP MAP ==========

Â  Â  Â  Â  console.log(`ğŸ“Š [MERGE_OUTSIDE_IN] Creating lookup map for ${a3Results.length} A3 results...`);

Â  Â  Â  Â  const a3ResultsMap = new Map();

Â  Â  Â  Â  const a3ProcessingStats = {

Â  Â  Â  Â  Â  Â  total_a3_results: a3Results.length,

Â  Â  Â  Â  Â  Â  indexed_results: 0,

Â  Â  Â  Â  Â  Â  indexing_errors: []

Â  Â  Â  Â  };

  

Â  Â  Â  Â  a3Results.forEach((a3Result, index) => {

Â  Â  Â  Â  Â  Â  try {

Â  Â  Â  Â  Â  Â  Â  Â  if (!a3Result?.identifiers) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  throw new Error(`A3 result ${index} missing identifiers`);

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  const { topic_id, situation_id } = a3Result.identifiers;

Â  Â  Â  Â  Â  Â  Â  Â  if (!topic_id || !situation_id) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  throw new Error(`A3 result ${index} missing topic_id or situation_id`);

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  const lookupKey = `${topic_id}_${situation_id}`;

Â  Â  Â  Â  Â  Â  Â  Â  a3ResultsMap.set(lookupKey, a3Result);

Â  Â  Â  Â  Â  Â  Â  Â  a3ProcessingStats.indexed_results++;

Â  Â  Â  Â  Â  Â  } catch (error) {

Â  Â  Â  Â  Â  Â  Â  Â  console.error(`âŒ [MERGE_OUTSIDE_IN] Failed to index A3 result ${index}:`, error.message);

Â  Â  Â  Â  Â  Â  Â  Â  a3ProcessingStats.indexing_errors.push({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  index,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: error.message,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result_preview: JSON.stringify(a3Result).substring(0, 100) + '...'

Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  });

  

Â  Â  Â  Â  console.log(`âœ… [MERGE_OUTSIDE_IN] Lookup map created: ${a3ResultsMap.size} indexed successfully`);

  

Â  Â  Â  Â  // ========== START FROM A12 STRUCTURE (OUTSIDE-IN) ==========

Â  Â  Â  Â  const originalTopics = a12Output.step_a2.data.output;

Â  Â  Â  Â  console.log(`ğŸ—ï¸ [MERGE_OUTSIDE_IN] Processing ${originalTopics.length} topics from A12 structure...`);

  

Â  Â  Â  Â  // Statistics tracking

Â  Â  Â  Â  const mergeStats = {

Â  Â  Â  Â  Â  Â  total_topics: originalTopics.length,

Â  Â  Â  Â  Â  Â  total_original_situations: 0,

Â  Â  Â  Â  Â  Â  completed_situations: 0,

Â  Â  Â  Â  Â  Â  failed_situations: 0,

Â  Â  Â  Â  Â  Â  not_processed_situations: 0,

Â  Â  Â  Â  Â  Â  processing_errors: []

Â  Â  Â  Â  };

  

Â  Â  Â  Â  // ========== ITERATE THROUGH TOPICS (PRESERVE A12 ORDER) ==========

Â  Â  Â  Â  const organizedByTopic = {};

Â  Â  Â  Â  const processedTopics = originalTopics.map((originalTopic, topicIndex) => {

Â  Â  Â  Â  Â  Â  const topicId = `TOP${String(topicIndex + 1).padStart(3, '0')}`;

Â  Â  Â  Â  Â  Â  console.log(`ğŸ“ [MERGE_OUTSIDE_IN] Processing topic ${topicIndex + 1}: "${originalTopic.topic_name}"`);

  

Â  Â  Â  Â  Â  Â  // Preserve ALL original topic metadata from A12

Â  Â  Â  Â  Â  Â  const topicResult = {

Â  Â  Â  Â  Â  Â  Â  Â  topic_name: originalTopic.topic_name,

Â  Â  Â  Â  Â  Â  Â  Â  topic_description: originalTopic.topic_description || null,

Â  Â  Â  Â  Â  Â  Â  Â  learning_objectives: originalTopic.learning_objectives || [],

Â  Â  Â  Â  Â  Â  Â  Â  topic_metadata: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_id: topicId,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_index: topicIndex,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_order: topicIndex + 1,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source: 'A12_generation'

Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  situations: [],

Â  Â  Â  Â  Â  Â  Â  Â  topic_stats: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_situations: 0,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  successful_situations: 0,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  failed_situations: 0,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  not_processed_situations: 0

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  };

  

Â  Â  Â  Â  Â  Â  // ========== ITERATE THROUGH SITUATIONS IN EACH TOPIC ==========

Â  Â  Â  Â  Â  Â  if (originalTopic.situations && Array.isArray(originalTopic.situations)) {

Â  Â  Â  Â  Â  Â  Â  Â  topicResult.situations = originalTopic.situations.map((originalSituation, situationIndex) => {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const situationId = `SIT${String(situationIndex + 1).padStart(3, '0')}`;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const lookupKey = `${topicId}_${situationId}`;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.log(` Â  ğŸ¯ [MERGE_OUTSIDE_IN] Processing situation ${situationIndex + 1}: "${originalSituation.situation_name}"`);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mergeStats.total_original_situations++;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topicResult.topic_stats.total_situations++;

  

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // ========== FIND CORRESPONDING A3 RESULT ==========

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const a3Result = a3ResultsMap.get(lookupKey);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (a3Result) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.log(` Â  âœ… [MERGE_OUTSIDE_IN] Found A3 result for ${lookupKey}`);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (a3Result.success) {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mergeStats.completed_situations++;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topicResult.topic_stats.successful_situations++;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // ========== ORIGINAL SITUATION METADATA (PRESERVED) ==========

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_name: originalSituation.situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stakeholder: originalSituation.stakeholder,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  scenario_description: originalSituation.scenario_description,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_details: originalSituation.situation_details,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  image_url_situation: originalSituation.image_url_situation, // Thumbnail from A12

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // ========== SITUATION METADATA ==========

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_metadata: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_id: situationId,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_index: situationIndex,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_order: situationIndex + 1,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_reference: topicId,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source: 'A12_generation'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // ========== A3 PROCESSING RESULT ==========

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  learning_content: a3Result.a3_result,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_status: 'completed',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_error: null,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_time_ms: a3Result.processing_time_ms || 0,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // ========== LINKING METADATA ==========

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  collection_id: a3Result.identifiers?.collection_id || null,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  linked_successfully: true,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3_metadata: a3Result.situation_metadata || null

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  } else {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // A3 processing failed

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mergeStats.failed_situations++;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topicResult.topic_stats.failed_situations++;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mergeStats.processing_errors.push({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic: originalTopic.topic_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation: originalSituation.situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lookup_key: lookupKey,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: a3Result.error

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Original situation metadata preserved

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_name: originalSituation.situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stakeholder: originalSituation.stakeholder,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  scenario_description: originalSituation.scenario_description,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_details: originalSituation.situation_details,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  image_url_situation: originalSituation.image_url_situation, // Thumbnail from A12

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_metadata: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_id: situationId,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_index: situationIndex,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_order: situationIndex + 1,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_reference: topicId,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source: 'A12_generation'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Failed A3 processing

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  learning_content: null,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_status: 'failed',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_error: a3Result.error,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_time_ms: a3Result.processing_time_ms || 0,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  collection_id: a3Result.identifiers?.collection_id || null,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  linked_successfully: true,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3_metadata: a3Result.situation_metadata || null

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  } else {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Missing A3 result

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.log(` Â  âŒ [MERGE_OUTSIDE_IN] Missing A3 result for ${lookupKey}`);

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mergeStats.not_processed_situations++;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topicResult.topic_stats.not_processed_situations++;

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mergeStats.processing_errors.push({

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic: originalTopic.topic_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation: originalSituation.situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lookup_key: lookupKey,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error: 'A3 result not found in parallel processing results'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Original situation metadata preserved

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_name: originalSituation.situation_name,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stakeholder: originalSituation.stakeholder,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  scenario_description: originalSituation.scenario_description,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_details: originalSituation.situation_details,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  image_url_situation: originalSituation.image_url_situation, // Thumbnail from A12

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_metadata: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_id: situationId,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  situation_index: situationIndex,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  original_order: situationIndex + 1,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  topic_reference: topicId,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  source: 'A12_generation'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Not processed

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  learning_content: null,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_status: 'not_processed',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_error: 'A3 result not found in parallel processing results',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processing_time_ms: 0,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  collection_id: null,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  linked_successfully: false,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3_metadata: null

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  Â  Â  }

  

Â  Â  Â  Â  Â  Â  // Calculate topic success rate

Â  Â  Â  Â  Â  Â  topicResult.topic_stats.success_rate = topicResult.topic_stats.total_situations > 0

Â  Â  Â  Â  Â  Â  Â  Â  ? `${((topicResult.topic_stats.successful_situations / topicResult.topic_stats.total_situations) * 100).toFixed(1)}%`

Â  Â  Â  Â  Â  Â  Â  Â  : '0%';

  

Â  Â  Â  Â  Â  Â  console.log(`âœ… [MERGE_OUTSIDE_IN] Topic "${originalTopic.topic_name}" completed: ${topicResult.topic_stats.successful_situations}/${topicResult.topic_stats.total_situations} successful`);

  

Â  Â  Â  Â  Â  Â  // Add to organized structure (for backwards compatibility)

Â  Â  Â  Â  Â  Â  organizedByTopic[originalTopic.topic_name] = topicResult;

  

Â  Â  Â  Â  Â  Â  return topicResult;

Â  Â  Â  Â  });

  

Â  Â  Â  Â  // ========== CALCULATE OVERALL STATISTICS ==========

Â  Â  Â  Â  const overallSuccessRate = mergeStats.total_original_situations > 0

Â  Â  Â  Â  Â  Â  ? (mergeStats.completed_situations / mergeStats.total_original_situations) * 100

Â  Â  Â  Â  Â  Â  : 0;

  

Â  Â  Â  Â  const meetsSuccessThreshold = overallSuccessRate >= (ERROR_CONFIG.MIN_SUCCESS_RATE * 100);

Â  Â  Â  Â  const mergeDuration = Date.now() - mergeStartTime;

  

Â  Â  Â  Â  console.log(`âœ… [MERGE_OUTSIDE_IN] Merging completed in ${mergeDuration}ms`);

Â  Â  Â  Â  console.log(`ğŸ“Š [MERGE_OUTSIDE_IN] Overall success rate: ${overallSuccessRate.toFixed(1)}% (${mergeStats.completed_situations}/${mergeStats.total_original_situations})`);

Â  Â  Â  Â  if (!meetsSuccessThreshold) {

Â  Â  Â  Â  Â  Â  console.warn(`âš ï¸ [MERGE_OUTSIDE_IN] Success rate ${overallSuccessRate.toFixed(1)}% below threshold ${(ERROR_CONFIG.MIN_SUCCESS_RATE * 100)}%`);

Â  Â  Â  Â  }

  

Â  Â  Â  Â  // ========== CREATE SIMPLIFIED STRUCTURE ==========

Â  Â  Â  Â  const simplifiedStructure = createSimplifiedLearningStructure(job_role, processedTopics);

Â  Â  Â  Â  // ========== RETURN STRUCTURED RESULT ==========

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  success: meetsSuccessThreshold,

Â  Â  Â  Â  Â  Â  execution_type: 'OPTIMIZED_A12_PARALLEL_A3_OUTSIDE_IN',

Â  Â  Â  Â  Â  Â  // Input context

Â  Â  Â  Â  Â  Â  input: {

Â  Â  Â  Â  Â  Â  Â  Â  job_role,

Â  Â  Â  Â  Â  Â  Â  Â  optimization_approach: 'A12_combined + A3_parallel + outside_in_merge'

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  // A12 results (preserved)

Â  Â  Â  Â  Â  Â  a12_generation: {

Â  Â  Â  Â  Â  Â  Â  Â  step_a1: a12Output.step_a1,

Â  Â  Â  Â  Â  Â  Â  Â  step_a2: a12Output.step_a2,

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  // NEW: Simplified hierarchical structure (JOB_ROLE â†’ TOPICS â†’ SITUATIONS â†’ pipeline_results)

Â  Â  Â  Â  Â  Â  learning_content: simplifiedStructure,

Â  Â  Â  Â  Â  Â  // Comprehensive statistics

Â  Â  Â  Â  Â  Â  execution_summary: {

Â  Â  Â  Â  Â  Â  Â  Â  total_topics: mergeStats.total_topics,

Â  Â  Â  Â  Â  Â  Â  Â  total_situations: mergeStats.total_original_situations,

Â  Â  Â  Â  Â  Â  Â  Â  successful_generations: mergeStats.completed_situations,

Â  Â  Â  Â  Â  Â  Â  Â  failed_generations: mergeStats.failed_situations,

Â  Â  Â  Â  Â  Â  Â  Â  not_processed_generations: mergeStats.not_processed_situations,

Â  Â  Â  Â  Â  Â  Â  Â  overall_success_rate: `${overallSuccessRate.toFixed(1)}%`,

Â  Â  Â  Â  Â  Â  Â  Â  meets_quality_threshold: meetsSuccessThreshold,

Â  Â  Â  Â  Â  Â  Â  Â  processing_approach: 'parallel_optimized_outside_in'

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  // Performance metrics

Â  Â  Â  Â  Â  Â  performance_metrics: {

Â  Â  Â  Â  Â  Â  Â  Â  merge_duration_ms: mergeDuration,

Â  Â  Â  Â  Â  Â  Â  Â  merge_strategy: 'outside_in_natural_hierarchy',

Â  Â  Â  Â  Â  Â  Â  Â  structure_preservation: 'complete',

Â  Â  Â  Â  Â  Â  Â  Â  metadata_retention: 'full',

Â  Â  Â  Â  Â  Â  Â  Â  a3_indexing_success_rate: a3ProcessingStats.indexed_results > 0

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ? `${((a3ProcessingStats.indexed_results / a3ProcessingStats.total_a3_results) * 100).toFixed(1)}%`

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  : '0%'

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  // Error details (if any)

Â  Â  Â  Â  Â  Â  error_details: mergeStats.processing_errors.length > 0 ? {

Â  Â  Â  Â  Â  Â  Â  Â  failed_count: mergeStats.processing_errors.length,

Â  Â  Â  Â  Â  Â  Â  Â  errors: mergeStats.processing_errors,

Â  Â  Â  Â  Â  Â  Â  Â  a3_indexing_errors: a3ProcessingStats.indexing_errors

Â  Â  Â  Â  Â  Â  } : null

Â  Â  Â  Â  };

  

Â  Â  } catch (error) {

Â  Â  Â  Â  const mergeDuration = Date.now() - mergeStartTime;

Â  Â  Â  Â  console.error('âŒ [MERGE_OUTSIDE_IN] Critical merge failure:', error.message);

Â  Â  Â  Â  throw new Error(`Outside-in merge failed after ${mergeDuration}ms: ${error.message}`);

Â  Â  }

}

  

// ===================================================================================

// COMPARISON: OLD vs NEW MERGE APPROACH

// ===================================================================================

  

/*

BEFORE (Problematic Inside-Out):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ 1. Start from A3 results array Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

â”‚ 2. Group by topic_name (lose A12 structure) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

â”‚ 3. Push situations into topics (lose order) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

â”‚ 4. Missing situations = invisible Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

â”‚ 5. A12 metadata = lost Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  

AFTER (Improved Outside-In):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ 1. Start from A12 original structure Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

â”‚ 2. Iterate topics maintaining order & metadata Â  Â  Â  Â  Â  Â  Â  Â  â”‚

â”‚ 3. For each situation, lookup A3 result by ID Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

â”‚ 4. Handle missing A3 results gracefully Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

â”‚ 5. Preserve complete hierarchy and metadata Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚

â”‚ 6. Generate comprehensive statistics Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  

Benefits of Updated Approach:

âœ… Maintains A12 original structure completely

âœ… Preserves topic and situation order

âœ… Retains all A12 metadata (descriptions, objectives, etc.)

âœ… Easy tracking of missing/failed A3 results

âœ… Better error handling and reporting

âœ… More intuitive debugging

âœ… Backwards compatible API response format

âœ… Enhanced hierarchical data structure for future use

âœ… Comprehensive merge statistics and performance metrics

*/

// ===================================================================================

// MAIN OPTIMIZED CONTROLLER

// ===================================================================================

  

/**

Â * MAIN OPTIMIZED ENTRY POINT: A12 + Parallel A3 Pipeline

Â *

Â * API Endpoint: POST /api/generate-learning-path-user-input-to-done-full

Â *

Â * @param {Object} req - Express request object

Â * @param {Object} req.body - Request body

Â * @param {string} req.body.job_role - Job role for learning content generation

Â * @param {Object} req.body.options - Optional configuration overrides

Â * @param {Object} res - Express response object

Â *

Â * Optimized Pipeline Flow:

Â * 1. Input validation and configuration setup

Â * 2. A12: Combined A1+A2 generation (job_role â†’ topics/situations)

Â * 3. EXTRACT: Flatten situations for parallel processing

Â * 4. A3_PARALLEL: Concurrent full pipeline execution

Â * 5. MERGE: Intelligent result organization

Â * 6. Return comprehensive results with performance metrics

Â *

Â * Performance Benefits:

Â * - 60-80% faster than sequential approach

Â * - Better resource utilization

Â * - Individual error isolation

Â * - Scalable architecture

Â */

exports.generateOptimizedLearningPipeline = async (req, res) => {

Â  Â  const pipelineStartTime = Date.now();

Â  Â  const executionId = `OPT_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

Â  Â  console.log(`ğŸš€ [OPTIMIZED] Starting optimized A12â†’Parallel A3 pipeline [${executionId}]`);

Â  Â  try {

Â  Â  Â  Â  const { data = {}, options = {} } = req.body;

Â  Â  Â  Â  const { job_role } = data;

Â  Â  Â  Â  // ========== INPUT VALIDATION ==========

Â  Â  Â  Â  if (!job_role || typeof job_role !== 'string') {

Â  Â  Â  Â  Â  Â  console.error('âŒ [OPTIMIZED] Missing or invalid job_role');

Â  Â  Â  Â  Â  Â  return res.status(400).json({

Â  Â  Â  Â  Â  Â  Â  Â  success: false,

Â  Â  Â  Â  Â  Â  Â  Â  execution_id: executionId,

Â  Â  Â  Â  Â  Â  Â  Â  error: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  code: 'INVALID_INPUT',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message: 'data.job_role is required and must be a string'

Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  received: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data: typeof data,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  job_role: typeof job_role,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  body_structure: Object.keys(req.body)

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  }

  

Â  Â  Â  Â  console.log(`ğŸ“‹ [OPTIMIZED] Processing job_role: "${job_role}"`);

  

Â  Â  Â  Â  // ========== STEP A12: COMBINED A1+A2 GENERATION ==========

Â  Â  Â  Â  console.log('ğŸ”„ [STEP A12] Starting combined A1+A2 generation...');

Â  Â  Â  Â  const a12StartTime = Date.now();

Â  Â  Â  Â  const a12Input = { data: { job_role } };

Â  Â  Â  Â  const a12Result = await callA12Controller(

Â  Â  Â  Â  Â  Â  generateA1andA2_UserInput2JTBD2TopicAndSituation,

Â  Â  Â  Â  Â  Â  a12Input,

Â  Â  Â  Â  Â  Â  'A12_COMBINED'

Â  Â  Â  Â  );

Â  Â  Â  Â  const a12Duration = Date.now() - a12StartTime;

Â  Â  Â  Â  if (!a12Result.success || a12Result.payload?.status === false) {

Â  Â  Â  Â  Â  Â  console.error(`âŒ [STEP A12] Failed after ${a12Duration}ms:`, a12Result.error);

Â  Â  Â  Â  Â  Â  return res.status(a12Result.httpStatus || 500).json({

Â  Â  Â  Â  Â  Â  Â  Â  success: false,

Â  Â  Â  Â  Â  Â  Â  Â  execution_id: executionId,

Â  Â  Â  Â  Â  Â  Â  Â  step: 'A12',

Â  Â  Â  Â  Â  Â  Â  Â  error: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  code: 'A12_GENERATION_FAILED',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message: a12Result.error || 'A12 generation failed',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  details: a12Result.payload?.error

Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  performance: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a12_duration_ms: a12Duration,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_duration_ms: Date.now() - pipelineStartTime

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  }

  

Â  Â  Â  Â  const a12Output = a12Result.payload;

Â  Â  Â  Â  console.log(`âœ… [STEP A12] Completed successfully in ${a12Duration}ms`);

  

Â  Â  Â  Â  // ========== STEP EXTRACT: SITUATION EXTRACTION ==========

Â  Â  Â  Â  console.log('ğŸ” [STEP EXTRACT] Extracting situations for parallel processing...');

Â  Â  Â  Â  const extractStartTime = Date.now();

Â  Â  Â  Â  let allSituations;

Â  Â  Â  Â  try {

Â  Â  Â  Â  Â  Â  allSituations = extractSituationsForParallelProcessing(a12Output, job_role);

Â  Â  Â  Â  } catch (extractError) {

Â  Â  Â  Â  Â  Â  console.error(`âŒ [STEP EXTRACT] Failed:`, extractError.message);

Â  Â  Â  Â  Â  Â  return res.status(500).json({

Â  Â  Â  Â  Â  Â  Â  Â  success: false,

Â  Â  Â  Â  Â  Â  Â  Â  execution_id: executionId,

Â  Â  Â  Â  Â  Â  Â  Â  step: 'EXTRACT',

Â  Â  Â  Â  Â  Â  Â  Â  a12_output: a12Output,

Â  Â  Â  Â  Â  Â  Â  Â  error: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  code: 'EXTRACTION_FAILED',

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  message: extractError.message

Â  Â  Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  Â  Â  performance: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a12_duration_ms: a12Duration,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_duration_ms: Date.now() - pipelineStartTime

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  }

  

Â  Â  Â  Â  const extractDuration = Date.now() - extractStartTime;

Â  Â  Â  Â  console.log(`âœ… [STEP EXTRACT] Completed in ${extractDuration}ms: Prepared ${allSituations.length} situations`);

  

Â  Â  Â  Â  if (allSituations.length === 0) {

Â  Â  Â  Â  Â  Â  return res.json({

Â  Â  Â  Â  Â  Â  Â  Â  success: true,

Â  Â  Â  Â  Â  Â  Â  Â  execution_id: executionId,

Â  Â  Â  Â  Â  Â  Â  Â  message: 'No situations found to process',

Â  Â  Â  Â  Â  Â  Â  Â  a12_output: a12Output,

Â  Â  Â  Â  Â  Â  Â  Â  performance: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a12_duration_ms: a12Duration,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  extract_duration_ms: extractDuration,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  total_duration_ms: Date.now() - pipelineStartTime

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  });

Â  Â  Â  Â  }

  

Â  Â  Â  Â  // ========== STEP A3_PARALLEL: CONCURRENT EXECUTION ==========

Â  Â  Â  Â  console.log('âš¡ [STEP A3_PARALLEL] Starting parallel A3 processing...');

Â  Â  Â  Â  const a3ParallelStartTime = Date.now();

Â  Â  Â  Â  const a3Results = await executeParallelA3Processing(allSituations);

Â  Â  Â  Â  const a3ParallelDuration = Date.now() - a3ParallelStartTime;

Â  Â  Â  Â  console.log(`âœ… [STEP A3_PARALLEL] Completed in ${a3ParallelDuration}ms`);

  

Â  Â  Â  Â  // ========== STEP MERGE: RESULT ORGANIZATION ==========

Â  Â  Â  Â  console.log('ğŸ”§ [STEP MERGE] Organizing results...');

Â  Â  Â  Â  const mergeStartTime = Date.now();

Â  Â  Â  Â  const mergedResults = mergeAndOrganizeResults(a12Output, a3Results, job_role);

Â  Â  Â  Â  const mergeDuration = Date.now() - mergeStartTime;

Â  Â  Â  Â  console.log(`âœ… [STEP MERGE] Completed in ${mergeDuration}ms`);

  

Â  Â  Â  Â  // ========== FINAL RESPONSE GENERATION ==========

Â  Â  Â  Â  const totalDuration = Date.now() - pipelineStartTime;

Â  Â  Â  Â  // Calculate estimated time savings vs sequential approach

Â  Â  Â  Â  const estimatedSequentialTime = a12Duration + (allSituations.length * 15000); // Assume 15s per situation

Â  Â  Â  Â  const timeSaved = estimatedSequentialTime - totalDuration;

Â  Â  Â  Â  const speedupPercentage = ((timeSaved / estimatedSequentialTime) * 100);

  

Â  Â  Â  Â  // Add performance metrics to merged results

Â  Â  Â  Â  mergedResults.performance_metrics = {

Â  Â  Â  Â  Â  Â  ...mergedResults.performance_metrics,

Â  Â  Â  Â  Â  Â  a12_duration_ms: a12Duration,

Â  Â  Â  Â  Â  Â  extract_duration_ms: extractDuration,

Â  Â  Â  Â  Â  Â  a3_parallel_duration_ms: a3ParallelDuration,

Â  Â  Â  Â  Â  Â  merge_duration_ms: mergeDuration,

Â  Â  Â  Â  Â  Â  total_duration_ms: totalDuration,

Â  Â  Â  Â  Â  Â  // Optimization metrics

Â  Â  Â  Â  Â  Â  estimated_sequential_time_ms: estimatedSequentialTime,

Â  Â  Â  Â  Â  Â  time_saved_ms: timeSaved,

Â  Â  Â  Â  Â  Â  speedup_percentage: `${speedupPercentage.toFixed(1)}%`,

Â  Â  Â  Â  Â  Â  avg_situation_processing_ms: a3ParallelDuration / allSituations.length,

Â  Â  Â  Â  Â  Â  // Resource efficiency

Â  Â  Â  Â  Â  Â  parallel_efficiency: `${((allSituations.length * 15000) / a3ParallelDuration).toFixed(2)}x`,

Â  Â  Â  Â  Â  Â  memory_optimization: 'batch_processing_enabled',

Â  Â  Â  Â  Â  Â  concurrency_level: OPTIMIZED_CONFIG.MAX_CONCURRENT_A3

Â  Â  Â  Â  };

  

Â  Â  Â  Â  const finalSuccess = mergedResults.success &&

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mergedResults.execution_summary.successful_generations > 0;

  

Â  Â  Â  Â  console.log(`ğŸ‰ [OPTIMIZED] Pipeline completed in ${totalDuration}ms [${executionId}]`);

Â  Â  Â  Â  console.log(`ğŸ“Š [OPTIMIZED] Performance: ${speedupPercentage.toFixed(1)}% faster than sequential (saved ${(timeSaved/1000).toFixed(1)}s)`);

  

Â  Â  Â  Â  return res.status(finalSuccess ? 200 : 206).json({

Â  Â  Â  Â  Â  Â  ...mergedResults,

Â  Â  Â  Â  Â  Â  execution_id: executionId,

Â  Â  Â  Â  Â  Â  generated_at: new Date().toISOString()

Â  Â  Â  Â  });

  

Â  Â  } catch (error) {

Â  Â  Â  Â  const totalDuration = Date.now() - pipelineStartTime;

Â  Â  Â  Â  console.error(`ğŸ’¥ [OPTIMIZED] Critical failure after ${totalDuration}ms [${executionId}]:`, error.message);

Â  Â  Â  Â  return res.status(500).json({

Â  Â  Â  Â  Â  Â  success: false,

Â  Â  Â  Â  Â  Â  execution_id: executionId,

Â  Â  Â  Â  Â  Â  error: {

Â  Â  Â  Â  Â  Â  Â  Â  code: 'PIPELINE_CRITICAL_ERROR',

Â  Â  Â  Â  Â  Â  Â  Â  message: error?.message || 'Critical pipeline failure',

Â  Â  Â  Â  Â  Â  Â  Â  stack: process.env.NODE_ENV === 'development' ? error.stack : undefined

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  performance: {

Â  Â  Â  Â  Â  Â  Â  Â  total_duration_ms: totalDuration,

Â  Â  Â  Â  Â  Â  Â  Â  failed_at: new Date().toISOString()

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  });

Â  Â  }

};

  

// ===================================================================================

// HEALTH CHECK AND MONITORING ENDPOINTS

// ===================================================================================

  

/**

Â * Pipeline health check endpoint

Â */

exports.getOptimizedPipelineHealth = async (req, res) => {

Â  Â  try {

Â  Â  Â  Â  const healthData = {

Â  Â  Â  Â  Â  Â  status: 'healthy',

Â  Â  Â  Â  Â  Â  pipeline_type: 'optimized_a12_parallel_a3',

Â  Â  Â  Â  Â  Â  configuration: {

Â  Â  Â  Â  Â  Â  Â  Â  max_concurrent_a3: OPTIMIZED_CONFIG.MAX_CONCURRENT_A3,

Â  Â  Â  Â  Â  Â  Â  Â  max_workers: OPTIMIZED_CONFIG.MAX_WORKERS,

Â  Â  Â  Â  Â  Â  Â  Â  timeouts: {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a12_timeout_ms: OPTIMIZED_CONFIG.A12_TIMEOUT_MS,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a3_timeout_ms: OPTIMIZED_CONFIG.A3_TIMEOUT_MS

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  features: {

Â  Â  Â  Â  Â  Â  Â  Â  parallel_processing: true,

Â  Â  Â  Â  Â  Â  Â  Â  error_isolation: true,

Â  Â  Â  Â  Â  Â  Â  Â  performance_optimization: true,

Â  Â  Â  Â  Â  Â  Â  Â  graceful_degradation: ERROR_CONFIG.ENABLE_GRACEFUL_DEGRADATION

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  timestamp: new Date().toISOString()

Â  Â  Â  Â  };

  

Â  Â  Â  Â  res.json(healthData);

Â  Â  } catch (error) {

Â  Â  Â  Â  res.status(500).json({

Â  Â  Â  Â  Â  Â  status: 'unhealthy',

Â  Â  Â  Â  Â  Â  error: error.message,

Â  Â  Â  Â  Â  Â  timestamp: new Date().toISOString()

Â  Â  Â  Â  });

Â  Â  }

};

  

/**

Â * Performance metrics endpoint

Â */

exports.getOptimizedPipelineMetrics = async (req, res) => {

Â  Â  try {

Â  Â  Â  Â  // In a real implementation, these would come from a metrics store

Â  Â  Â  Â  const metrics = {

Â  Â  Â  Â  Â  Â  pipeline_performance: {

Â  Â  Â  Â  Â  Â  Â  Â  avg_execution_time_ms: 0, // Would be calculated from historical data

Â  Â  Â  Â  Â  Â  Â  Â  success_rate: 0, Â  Â  Â  Â  Â // Would be calculated from historical data

Â  Â  Â  Â  Â  Â  Â  Â  error_rate: 0, Â  Â  Â  Â  Â  Â // Would be calculated from historical data

Â  Â  Â  Â  Â  Â  Â  Â  throughput_per_hour: 0 Â  Â // Would be calculated from historical data

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  optimization_metrics: {

Â  Â  Â  Â  Â  Â  Â  Â  avg_speedup_percentage: 0,

Â  Â  Â  Â  Â  Â  Â  Â  avg_time_saved_ms: 0,

Â  Â  Â  Â  Â  Â  Â  Â  parallel_efficiency: 0,

Â  Â  Â  Â  Â  Â  Â  Â  resource_utilization: 0

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  system_health: {

Â  Â  Â  Â  Â  Â  Â  Â  memory_usage: process.memoryUsage(),

Â  Â  Â  Â  Â  Â  Â  Â  uptime_seconds: process.uptime(),

Â  Â  Â  Â  Â  Â  Â  Â  cpu_usage: process.cpuUsage()

Â  Â  Â  Â  Â  Â  },

Â  Â  Â  Â  Â  Â  configuration: OPTIMIZED_CONFIG,

Â  Â  Â  Â  Â  Â  timestamp: new Date().toISOString()

Â  Â  Â  Â  };

  

Â  Â  Â  Â  res.json(metrics);

Â  Â  } catch (error) {

Â  Â  Â  Â  res.status(500).json({

Â  Â  Â  Â  Â  Â  error: error.message,

Â  Â  Â  Â  Â  Â  timestamp: new Date().toISOString()

Â  Â  Â  Â  });

Â  Â  }

};

  

// ===================================================================================

// EXPORTS

// ===================================================================================

  

module.exports = {

Â  Â  // Main optimized controller

Â  Â  generateOptimizedLearningPipeline: exports.generateOptimizedLearningPipeline,

Â  Â  // Health and monitoring

Â  Â  getOptimizedPipelineHealth: exports.getOptimizedPipelineHealth,

Â  Â  getOptimizedPipelineMetrics: exports.getOptimizedPipelineMetrics,

Â  Â  // Utility functions (for testing and integration)

Â  Â  extractSituationsForParallelProcessing,

Â  Â  executeParallelA3Processing,

Â  Â  mergeAndOrganizeResults,

Â  Â  callA12Controller,

Â  Â  // Configuration exports

Â  Â  OPTIMIZED_CONFIG,

Â  Â  ERROR_CONFIG

};

  

/**

Â * ===================================================================================

Â * USAGE EXAMPLES

Â * ===================================================================================

Â *

Â * Example 1: Basic optimized pipeline execution

Â * curl --location 'http://localhost:3000/api/generate-learning-path-user-input-to-done-full' \

Â * --header 'Content-Type: application/json' \

Â * --data '{"job_role": "AI Engineer"}'

Â *

Â * Example 2: Pipeline with custom options

Â * curl --location 'http://localhost:3000/api/generate-learning-path-user-input-to-done-full' \

Â * --header 'Content-Type: application/json' \

Â * --data '{

Â * Â  "job_role": "Product Manager",

Â * Â  "options": {

Â * Â  Â  "max_concurrent": 3,

Â * Â  Â  "batch_size": 2,

Â * Â  Â  "enable_detailed_logging": true

Â * Â  }

Â * }'

Â *

Â * Example 3: Health check

Â * curl --location 'http://localhost:3000/api/health/optimized-pipeline-status'

Â *

Â * Example 4: Performance metrics

Â * curl --location 'http://localhost:3000/api/metrics/pipeline-performance'

Â *

Â * ===================================================================================

Â * INTEGRATION NOTES

Â * ===================================================================================

Â *

Â * 1. Route Setup:

Â * app.post('/api/generate-learning-path-user-input-to-done-full', generateOptimizedLearningPipeline);

Â * app.get('/api/health/optimized-pipeline-status', getOptimizedPipelineHealth);

Â * app.get('/api/metrics/pipeline-performance', getOptimizedPipelineMetrics);

Â *

Â * 2. Performance Monitoring:

Â * - Monitor execution_id for request tracing

Â * - Track speedup_percentage for optimization effectiveness

Â * - Monitor success_rate for quality assurance

Â * - Watch memory usage during parallel processing

Â *

Â * 3. Error Handling:

Â * - Partial failures return 206 (Partial Content) status

Â * - Complete failures return 500 (Internal Server Error)

Â * - Input validation errors return 400 (Bad Request)

Â *

Â * 4. Scaling Considerations:

Â * - Adjust MAX_CONCURRENT_A3 based on system resources

Â * - Monitor API rate limits for external dependencies

Â * - Consider horizontal scaling for high loads

Â * - Implement result caching for repeated requests

Â */

  
  
  

/*

{

Â  "success": true,

Â  "execution_type": "OPTIMIZED_A12_PARALLEL_A3_OUTSIDE_IN",

Â  "input": { ... },

Â  "a12_generation": {

Â  Â  "job_role": "DÆ°á»£c sÄ©"

Â  },

Â  "learning_content": {

Â  Â  "topics": [

Â  Â  Â  {

Â  Â  Â  Â  "topic_name": "TÆ° váº¥n thuá»‘c kÃª Ä‘Æ¡n cho bá»‡nh nhÃ¢n",

Â  Â  Â  Â  "situations": [

Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "situation_name": "Giáº£i thÃ­ch thuá»‘c má»›i cho bá»‡nh nhÃ¢n lá»›n tuá»•i",

Â  Â  Â  Â  Â  Â  "stakeHolder": "Bá»‡nh nhÃ¢n",

Â  Â  Â  Â  Â  Â  "scenario_description": "Má»™t bá»‡nh nhÃ¢n lá»›n tuá»•i nháº­n Ä‘Æ¡n thuá»‘c tim máº¡ch má»›i vÃ  ráº¥t lo láº¯ng. Há» khÃ´ng hiá»ƒu rÃµ vá» thuá»‘c. HÃ£y giáº£i Ä‘Ã¡p cÃ¡c tháº¯c máº¯c cá»§a há».",

Â  Â  Â  Â  Â  Â  "image_url_situation": "https://smedia.stepup.edu.vn/thecoach/100nghe/upload_media/image/gna/duoc-si-tu-van-thuoc-ke-don-cho-benh-nhan_giai-thich-thuoc-moi-cho-benh-nhan-lon-tuoi_benh-nhan_thumbnail_20250916_142724_00001.webp",

Â  Â  Â  Â  Â  Â  "pipeline_results": {

Â  Â  Â  Â  Â  Â  Â  "SCENARIO_DETAIL_1": { ... },

Â  Â  Â  Â  Â  Â  Â  "SCENARIO_DETAIL_2": { ... },

Â  Â  Â  Â  Â  Â  Â  "SCENARIO_DETAIL_3": { ... },

Â  Â  Â  Â  Â  Â  Â  "ONION_MINI": { ... },

Â  Â  Â  Â  Â  Â  Â  "PTY1": { ... },

Â  Â  Â  Â  Â  Â  Â  "SCENARIO_DETAIL_4": { ... },

Â  Â  Â  Â  Â  Â  Â  "SCENARIO_DETAIL_5": { ... },

Â  Â  Â  Â  Â  Â  Â  "ONION_FULL": { ... },

Â  Â  Â  Â  Â  Â  Â  "PTY_AFTER_ONION_MINI": { ... }

Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  }

Â  Â  Â  Â  ]

Â  Â  Â  }

Â  Â  ]

Â  },

Â  "execution_summary": { ... },

Â  "performance_metrics": { ... },

Â  "error_details": null,

Â  "execution_id": "OPT_1758087509295_rp9ebzt6",

Â  "generated_at": "2025-09-16T03:54:54.726Z"

}

  

*/
```