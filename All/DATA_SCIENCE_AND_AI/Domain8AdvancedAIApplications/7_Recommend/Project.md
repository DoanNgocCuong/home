

1. Frequently 
2. ....
3. Matrix Factorition 
4. Session based


5. Why we need recommender system? 
- Google, Amazon
- Youtube
- News ???
- Facebook : user - user ~ LINK ANALYSIS ~ Link Prediction

2. Recommender vs Information retrieval
- Information Retrieval: for user that user want
- Recommender System: suggest for user thing that user don't know they want

3. Applications: 
- Amazon: increase 30% vanue
- Netflix: 1B $ each year
- ....
<chÃº tÃ¢m liÃªn tá»¥c cáº£m giÃ¡c ná»•i trá»™i toÃ n thÃ¢n>



## 1. Content-based Approach
- User info : gender, age  + History purchase
- Product info: categpory, prices, ...

item - 

**Content-based Approach (CÃ¡ch tiáº¿p cáº­n dá»±a trÃªn ná»™i dung)**

Content-based approach lÃ  má»™t cÃ¡ch mÃ  há»‡ thá»‘ng khuyáº¿n nghá»‹ (recommendation systems) sá»­ dá»¥ng Ä‘á»ƒ Ä‘á» xuáº¥t sáº£n pháº©m cho ngÆ°á»i dÃ¹ng. NÃ³ dá»±a vÃ o thÃ´ng tin mÃ  ngÆ°á»i dÃ¹ng Ä‘Ã£ cung cáº¥p vÃ  cÃ¡c sáº£n pháº©m há» Ä‘Ã£ tá»«ng mua hoáº·c yÃªu thÃ­ch.

- **ThÃ´ng tin ngÆ°á»i dÃ¹ng (User info):**
    
    - **Giá»›i tÃ­nh (Gender):** Há»‡ thá»‘ng biáº¿t báº¡n lÃ  nam hay ná»¯ Ä‘á»ƒ Ä‘á» xuáº¥t sáº£n pháº©m phÃ¹ há»£p hÆ¡n.
    - **Äá»™ tuá»•i (Age):** Äá»™ tuá»•i giÃºp há»‡ thá»‘ng hiá»ƒu Ä‘Æ°á»£c sá»Ÿ thÃ­ch cá»§a báº¡n. VÃ­ dá»¥: Má»™t ngÆ°á»i tráº» tuá»•i cÃ³ thá»ƒ thÃ­ch thá»i trang nÄƒng Ä‘á»™ng, cÃ²n ngÆ°á»i lá»›n tuá»•i cÃ³ thá»ƒ thÃ­ch phong cÃ¡ch trang nhÃ£.
    - **Lá»‹ch sá»­ mua hÃ ng (History purchase):** Náº¿u báº¡n Ä‘Ã£ tá»«ng mua sÃ¡ch vá» Ã¢m nháº¡c, há»‡ thá»‘ng cÃ³ thá»ƒ gá»£i Ã½ nhá»¯ng sÃ¡ch khÃ¡c cÃ¹ng thá»ƒ loáº¡i.
- **ThÃ´ng tin sáº£n pháº©m (Product info):**
    
    - **Danh má»¥c sáº£n pháº©m (Category):** VÃ­ dá»¥: quáº§n Ã¡o, sÃ¡ch, Ä‘á»“ Ä‘iá»‡n tá»­, hoáº·c má»¹ pháº©m.
    - **GiÃ¡ cáº£ (Prices):** Há»‡ thá»‘ng sáº½ xem báº¡n thÆ°á»ng mua nhá»¯ng mÃ³n Ä‘áº¯t tiá»n hay ráº» hÆ¡n Ä‘á»ƒ gá»£i Ã½ sáº£n pháº©m phÃ¹ há»£p vá»›i ngÃ¢n sÃ¡ch cá»§a báº¡n.

**VÃ­ dá»¥ thá»±c táº¿:**  
Báº¡n lÃ  má»™t há»c sinh ná»¯, 16 tuá»•i, vÃ  Ä‘Ã£ mua sÃ¡ch vá» tiáº¿ng Anh. Há»‡ thá»‘ng cÃ³ thá»ƒ gá»£i Ã½ thÃªm sÃ¡ch há»c ngá»¯ phÃ¡p, sÃ¡ch luyá»‡n IELTS, hoáº·c nhá»¯ng sáº£n pháº©m liÃªn quan Ä‘áº¿n há»c táº­p.


## 2. 
**Giáº£i thÃ­ch Collaborative Filtering cho há»c sinh cáº¥p 2:**

HÃ£y tÆ°á»Ÿng tÆ°á»£ng em vÃ  cÃ¡c báº¡n cÃ¹ng lá»›p Ä‘ang tháº£o luáº­n xem nÃªn xem phim gÃ¬. Em thÃ­ch phim hoáº¡t hÃ¬nh, cÃ²n báº¡n An thÃ­ch phim hÃ nh Ä‘á»™ng. Má»™t ngÃ y, cÃ³ má»™t báº¡n má»›i vÃ o lá»›p tÃªn lÃ  Minh, vÃ  Minh cÅ©ng thÃ­ch phim hÃ nh Ä‘á»™ng nhÆ° báº¡n An.

Dá»±a trÃªn sá»Ÿ thÃ­ch giá»‘ng nhau, em cÃ³ thá»ƒ Ä‘oÃ¡n ráº±ng Minh sáº½ thÃ­ch nhá»¯ng bá»™ phim mÃ  báº¡n An tá»«ng giá»›i thiá»‡u, Ä‘Ãºng khÃ´ng? ÄÃ¢y chÃ­nh lÃ  cÃ¡ch mÃ  **Collaborative Filtering** hoáº¡t Ä‘á»™ng.

Cá»¥ thá»ƒ hÆ¡n:

- Náº¿u hai ngÆ°á»i cÃ³ sá»Ÿ thÃ­ch giá»‘ng nhau (nhÆ° Minh vÃ  An), há»‡ thá»‘ng sáº½ gá»£i Ã½ nhá»¯ng gÃ¬ má»™t ngÆ°á»i thÃ­ch cho ngÆ°á»i cÃ²n láº¡i.
- Thay vÃ¬ chá»‰ nhÃ¬n vÃ o tá»«ng mÃ³n Ä‘á»“, há»‡ thá»‘ng dá»±a vÃ o dá»¯ liá»‡u tá»« nhiá»u ngÆ°á»i giá»‘ng nhau Ä‘á»ƒ Ä‘Æ°a ra gá»£i Ã½.

VÃ­ dá»¥ thá»±c táº¿: TrÃªn Netflix, náº¿u em vÃ  báº¡n cá»§a em Ä‘á»u xem nhiá»u phim hoáº¡t hÃ¬nh, Netflix sáº½ gá»£i Ã½ thÃªm cÃ¡c phim hoáº¡t hÃ¬nh mÃ  báº¡n em Ä‘Ã£ xem nhÆ°ng em chÆ°a xem. NÃ³ giá»‘ng nhÆ° báº¡n bÃ¨ chia sáº» sá»Ÿ thÃ­ch váº­y! ğŸ˜Š


## Challenges: 
- The number of transactions << actual 
- Not enough information 
- Real-time ? 
- Change of behavior buy 

## 3. Evaluation Method?


Give: user U , item I 

5 stage scale: 1, 2, 3, 4, 5 
Binary scale : 0, 1 

Dataset seperated into 


train/test




## 4. A/B Testing

![[Pasted image 20241126113005.png]]

![[Pasted image 20241126113106.png]]

==========

Exercise: 


```
Giáº£ sá»­ John dá»± Ä‘oÃ¡n cho cÃ¡c phim: Aliens(5), Terminator(5), Nero(1), Gladiator (6)
GiÃ¡ trá»‹ cao hÆ¡n thÃ¬ tá»‘t hÆ¡n 

giáº£ sá»± dá»± Ä‘oÃ¡n cá»§a há»‡ gá»£i Ã½ lÃ : Aliens(4, 3), Terminator(5, 4), Nero(1, 3) vÃ  Gldiator(5)

a, Cal : MSE
b, Calucatlate: MAE
c, RMSE
d, Calculate NMAE and NRMSE, given that rating is in the range of (1, 2, ...6)


```
**Giáº£i thÃ­ch tá»«ng bÆ°á»›c má»™t cÃ¡ch dá»… hiá»ƒu:**

ChÃºng ta cÃ³ 4 bá»™ phim vÃ  Ä‘iá»ƒm sá»‘ mÃ  John thá»±c sá»± Ä‘Ã¡nh giÃ¡ (Ä‘iá»ƒm tháº­t) vÃ  Ä‘iá»ƒm mÃ  há»‡ thá»‘ng dá»± Ä‘oÃ¡n (Ä‘iá»ƒm dá»± Ä‘oÃ¡n):

|**Phim**|**Äiá»ƒm tháº­t (T)**|**Äiá»ƒm dá»± Ä‘oÃ¡n (P)**|
|---|---|---|
|Aliens|5|4.3|
|Terminator|5|5.4|
|Nero|1|1.3|
|Gladiator|6|5|

**BÆ°á»›c 1: TÃ­nh lá»—i cho má»—i phim**

Lá»—i (E) cho má»—i phim lÃ  sá»± chÃªnh lá»‡ch giá»¯a Ä‘iá»ƒm dá»± Ä‘oÃ¡n vÃ  Ä‘iá»ƒm tháº­t:

E = P - T

ChÃºng ta cÅ©ng tÃ­nh giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a lá»—i (|E|) vÃ  bÃ¬nh phÆ°Æ¡ng cá»§a lá»—i (EÂ²):
```
| **Phim** | **T** | **P** | **E = P - T** | **|E|** | **EÂ²** | |----------------|-------|-------|---------------|--------|---------| | Aliens | 5 | 4.3 | -0.7 | 0.7 | 0.49 | | Terminator | 5 | 5.4 | 0.4 | 0.4 | 0.16 | | Nero | 1 | 1.3 | 0.3 | 0.3 | 0.09 | | Gladiator | 6 | 5 | -1.0 | 1.0 | 1.00 |
```


**a) TÃ­nh MSE (Mean Squared Error - Trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng lá»—i)**

MSE lÃ  trung bÃ¬nh cá»§a cÃ¡c bÃ¬nh phÆ°Æ¡ng lá»—i:

1. Cá»™ng táº¥t cáº£ cÃ¡c EÂ² láº¡i:
    
    - Tá»•ng EÂ² = 0.49 + 0.16 + 0.09 + 1.00 = **1.74**
2. Chia tá»•ng nÃ y cho sá»‘ lÆ°á»£ng phim (n = 4):
    
    - MSE = Tá»•ng EÂ² / n = 1.74 / 4 = **0.435**

**b) TÃ­nh MAE (Mean Absolute Error - Trung bÃ¬nh giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a lá»—i)**

MAE lÃ  trung bÃ¬nh cá»§a cÃ¡c giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a lá»—i:

1. Cá»™ng táº¥t cáº£ cÃ¡c |E| láº¡i:
    
    - Tá»•ng |E| = 0.7 + 0.4 + 0.3 + 1.0 = **2.4**
2. Chia tá»•ng nÃ y cho sá»‘ lÆ°á»£ng phim:
    
    - MAE = Tá»•ng |E| / n = 2.4 / 4 = **0.6**

**c) TÃ­nh RMSE (Root Mean Squared Error - CÄƒn báº­c hai cá»§a MSE)**

RMSE lÃ  cÄƒn báº­c hai cá»§a MSE:

- RMSE = âˆšMSE = âˆš0.435 â‰ˆ **0.6595**

**d) TÃ­nh NMAE vÃ  NRMSE (Chuáº©n hÃ³a MAE vÃ  RMSE)**

VÃ¬ Ä‘iá»ƒm sá»‘ náº±m trong khoáº£ng tá»« 1 Ä‘áº¿n 6, pháº¡m vi Ä‘iá»ƒm sá»‘ lÃ :

- Pháº¡m vi = Äiá»ƒm cao nháº¥t - Äiá»ƒm tháº¥p nháº¥t = 6 - 1 = **5**

**TÃ­nh NMAE (Normalized MAE):**

- NMAE = MAE / Pháº¡m vi = 0.6 / 5 = **0.12**

**TÃ­nh NRMSE (Normalized RMSE):**

- NRMSE = RMSE / Pháº¡m vi = 0.6595 / 5 â‰ˆ **0.1319**

---

**TÃ³m láº¡i:**

a) **MSE = 0.435**

b) **MAE = 0.6**

c) **RMSE â‰ˆ 0.6595**

d) **NMAE = 0.12** vÃ  **NRMSE â‰ˆ 0.1319**

**Giáº£i thÃ­ch Ä‘Æ¡n giáº£n:**

- **MSE** Ä‘o lÆ°á»ng Ä‘á»™ chÃªnh lá»‡ch bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿.
- **MAE** Ä‘o lÆ°á»ng Ä‘á»™ chÃªnh lá»‡ch trung bÃ¬nh theo giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i.
- **RMSE** lÃ  cÄƒn báº­c hai cá»§a MSE, giÃºp Ä‘Æ°a Ä‘Æ¡n vá»‹ vá» cÃ¹ng má»©c vá»›i dá»¯ liá»‡u gá»‘c.
- **NMAE** vÃ  **NRMSE** chuáº©n hÃ³a cÃ¡c lá»—i nÃ y dá»±a trÃªn pháº¡m vi Ä‘iá»ƒm sá»‘, giÃºp so sÃ¡nh dá»… dÃ ng hÆ¡n.

**NhÆ° váº­y, chÃºng ta Ä‘Ã£ tÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ lá»—i má»™t cÃ¡ch dá»… hiá»ƒu cho há»c sinh cáº¥p 2!**




--------------
### **TÃ³m táº¯t cÃ´ng thá»©c cá»§a cÃ¡c chá»‰ sá»‘: MAE, MSE, RMSE, NMAE, NMSE, NRMSE**

#### **CÃ´ng thá»©c:**

1. **MAE (Mean Absolute Error):**
    
    MAE=1nâˆ‘(u,i)âˆ£puiâˆ’ruiâˆ£MAE = \frac{1}{n} \sum_{(u,i)} |p_{ui} - r_{ui}|
2. **MSE (Mean Square Error):**
    
    MSE=1nâˆ‘(u,i)(puiâˆ’rui)2MSE = \frac{1}{n} \sum_{(u,i)} (p_{ui} - r_{ui})^2
3. **RMSE (Root Mean Square Error):**
    
    RMSE=1nâˆ‘(u,i)(puiâˆ’rui)2RMSE = \sqrt{\frac{1}{n} \sum_{(u,i)} (p_{ui} - r_{ui})^2}
4. **NMAE (Normalized Mean Absolute Error):**
    
    NMAE=MAErmaxâˆ’rminNMAE = \frac{MAE}{r_{max} - r_{min}}
5. **NMSE (Normalized Mean Square Error):**
    
    NMSE=MSE(rmaxâˆ’rmin)2NMSE = \frac{MSE}{(r_{max} - r_{min})^2}
6. **NRMSE (Normalized Root Mean Square Error):**
    
    NRMSE=RMSErmaxâˆ’rminNRMSE = \frac{RMSE}{r_{max} - r_{min}}

---

### **Báº£ng so sÃ¡nh chi tiáº¿t: MAE, MSE, RMSE --- NMAE, NMSE, NRMSE**

|**Metric**|**Dáº¡ng Sai Sá»‘**|**Nháº¥n Máº¡nh Lá»—i Lá»›n?**|**Chuáº©n HÃ³a?**|**á»¨ng Dá»¥ng PhÃ¹ Há»£p**|
|---|---|---|---|---|
|**MAE**|Tuyá»‡t Ä‘á»‘i|KhÃ´ng|KhÃ´ng|Dá»¯ liá»‡u á»•n Ä‘á»‹nh, khÃ´ng cáº§n nháº¥n máº¡nh lá»—i lá»›n.|
|**MSE**|BÃ¬nh phÆ°Æ¡ng|CÃ³|KhÃ´ng|PhÃ¡t hiá»‡n lá»—i lá»›n, phÃ¹ há»£p khi dá»¯ liá»‡u Ã­t outliers.|
|**RMSE**|CÄƒn bÃ¬nh phÆ°Æ¡ng|CÃ³|KhÃ´ng|Táº­p trung vÃ o cÃ¡c lá»—i lá»›n, phÃ¹ há»£p vá»›i dá»¯ liá»‡u nháº¡y cáº£m.|
|**NMAE**|Tuyá»‡t Ä‘á»‘i (chuáº©n hÃ³a)|KhÃ´ng|CÃ³|So sÃ¡nh trÃªn nhiá»u thang Ä‘o, khÃ´ng cáº§n quan tÃ¢m lá»—i lá»›n.|
|**NMSE**|BÃ¬nh phÆ°Æ¡ng (chuáº©n hÃ³a)|CÃ³|CÃ³|Tá»•ng quÃ¡t, so sÃ¡nh trÃªn thang Ä‘o khÃ¡c nhau, Ã­t nháº¥n lá»—i lá»›n.|
|**NRMSE**|CÄƒn bÃ¬nh phÆ°Æ¡ng (chuáº©n hÃ³a)|CÃ³|CÃ³|So sÃ¡nh trÃªn nhiá»u thang Ä‘o, nháº¥n máº¡nh lá»—i lá»›n.|

---

### **1. Format cÃ´ng thá»©c vÃ  chÃº thÃ­ch rÃµ rÃ ng hÆ¡n:**

- **Cáº§n Ä‘á»“ng nháº¥t vá» cÃ¡ch hiá»ƒn thá»‹ cÃ´ng thá»©c:** VÃ­ dá»¥:

MAE=1nâˆ‘(u,i)âˆ£puiâˆ’ruiâˆ£MAE = \frac{1}{n} \sum_{(u,i)} |p_{ui} - r_{ui}|

=> ThÃªm chÃº thÃ­ch dÆ°á»›i cÃ´ng thá»©c:

- puip_{ui}: GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n.
- ruir_{ui}: GiÃ¡ trá»‹ thá»±c táº¿.
- nn: Sá»‘ lÆ°á»£ng vÃ­ dá»¥ trong táº­p kiá»ƒm tra.

---

### **2. Pháº§n Ranking (Precision, Recall, F-Score):**

- **Precision:**
    
    Precision=SoË†ËŠÂ lÆ°á»£ngÂ má»¥cÂ Ä‘Æ°á»£cÂ gá»£iÂ yËŠÂ Ä‘uËŠngTá»•ngÂ soË†ËŠÂ má»¥cÂ Ä‘Æ°á»£cÂ gá»£iÂ yËŠPrecision = \frac{\text{Sá»‘ lÆ°á»£ng má»¥c Ä‘Æ°á»£c gá»£i Ã½ Ä‘Ãºng}}{\text{Tá»•ng sá»‘ má»¥c Ä‘Æ°á»£c gá»£i Ã½}}
- **Recall:**
    
    Recall=SoË†ËŠÂ lÆ°á»£ngÂ má»¥cÂ Ä‘Æ°á»£cÂ gá»£iÂ yËŠÂ Ä‘uËŠngTá»•ngÂ soË†ËŠÂ má»¥cÂ Ä‘uËŠngÂ trongÂ thá»±cÂ teË†ËŠRecall = \frac{\text{Sá»‘ lÆ°á»£ng má»¥c Ä‘Æ°á»£c gá»£i Ã½ Ä‘Ãºng}}{\text{Tá»•ng sá»‘ má»¥c Ä‘Ãºng trong thá»±c táº¿}}
- **F-Score:**
    
    F=2â‹…Precisionâ‹…RecallPrecision+RecallF = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}

**LÆ°u Ã½:**

- **Precision:** Há»‡ thá»‘ng gá»£i Ã½ bao nhiÃªu má»¥c Ä‘Ãºng so vá»›i tá»•ng sá»‘ má»¥c Ä‘Ã£ gá»£i Ã½.
- **Recall:** Há»‡ thá»‘ng gá»£i Ã½ Ä‘Æ°á»£c bao nhiÃªu má»¥c Ä‘Ãºng trong táº¥t cáº£ cÃ¡c má»¥c mÃ  ngÆ°á»i dÃ¹ng thá»±c sá»± thÃ­ch.
- **F-Score:** CÃ¢n báº±ng giá»¯a Precision vÃ  Recall.

---

### **3. NMSE vÃ  NRMSE:**

- **KhÃ¡c biá»‡t:**
    
    - **NMSE** dá»±a trÃªn MSE (khÃ´ng cÄƒn báº­c hai), Ã­t nháº¡y cáº£m vá»›i lá»—i lá»›n.
    - **NRMSE** dá»±a trÃªn RMSE (cÃ³ cÄƒn báº­c hai), nháº¡y cáº£m hÆ¡n vá»›i lá»—i lá»›n.
- **á»¨ng dá»¥ng thá»±c táº¿:**
    
    - **NMSE:** ThÆ°á»ng Ä‘Æ°á»£c dÃ¹ng trong bÃ i toÃ¡n Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ khi khÃ´ng cáº§n nháº¥n máº¡nh lá»—i lá»›n.
    - **NRMSE:** PhÃ¹ há»£p vá»›i bÃ i toÃ¡n nháº¡y cáº£m vá»›i sai sá»‘ lá»›n, nhÆ° há»‡ thá»‘ng gá»£i Ã½ phim hoáº·c quáº£ng cÃ¡o.

---

### **4. Khi nÃ o sá»­ dá»¥ng tá»«ng chá»‰ sá»‘?**

|**Chá»‰ sá»‘**|**á»¨ng dá»¥ng**|
|---|---|
|**MAE**|Dá»… hiá»ƒu, phÃ¹ há»£p khi dá»¯ liá»‡u khÃ´ng cÃ³ outliers vÃ  cÃ¡c lá»—i lá»›n khÃ´ng quan trá»ng.|
|**MSE**|Nháº¥n máº¡nh lá»—i lá»›n, phÃ¹ há»£p khi cáº§n phÃ¡t hiá»‡n cÃ¡c dá»± Ä‘oÃ¡n sai nghiÃªm trá»ng.|
|**RMSE**|TÆ°Æ¡ng tá»± MSE, nhÆ°ng nháº¡y cáº£m hÆ¡n vá»›i lá»—i lá»›n, phÃ¹ há»£p vá»›i dá»¯ liá»‡u cÃ³ outliers.|
|**NMAE**|Chuáº©n hÃ³a MAE Ä‘á»ƒ so sÃ¡nh giá»¯a cÃ¡c há»‡ thá»‘ng hoáº·c mÃ´ hÃ¬nh vá»›i thang Ä‘o khÃ¡c nhau.|
|**NMSE**|Chuáº©n hÃ³a MSE, phÃ¹ há»£p khi cáº§n so sÃ¡nh trÃªn nhiá»u thang Ä‘o vÃ  khÃ´ng nháº¥n máº¡nh cÃ¡c lá»—i lá»›n.|
|**NRMSE**|Chuáº©n hÃ³a RMSE, nháº¥n máº¡nh cÃ¡c lá»—i lá»›n, phÃ¹ há»£p khi cáº§n Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c trÃªn cÃ¡c thang Ä‘o khÃ¡c nhau.|

---

### **5. Káº¿t luáº­n:**

- **MAE vÃ  RMSE:** ThÃ­ch há»£p Ä‘á»ƒ Ä‘o sai sá»‘ trá»±c tiáº¿p. RMSE nháº¥n máº¡nh lá»—i lá»›n hÆ¡n MAE.
- **NMAE, NMSE, NRMSE:** Há»¯u Ã­ch khi cáº§n chuáº©n hÃ³a sai sá»‘, giÃºp dá»… dÃ ng so sÃ¡nh trÃªn cÃ¡c thang Ä‘iá»ƒm khÃ¡c nhau.
- **Ranking metrics (Precision, Recall, F-Score):** Quan trá»ng trong cÃ¡c há»‡ thá»‘ng gá»£i Ã½, táº­p trung vÃ o viá»‡c Ä‘o lÆ°á»ng hiá»‡u quáº£ gá»£i Ã½.



#### CÃ¢u há»i : 

1. Giáº£i thÃ­ch dá»… hiá»ƒu hÆ¡n Ä‘oáº¡n "á»©ng dá»¥ng phÃ¹ há»£p cá»§a MSE vÃ  RMSE" - PhÃ¢n biá»‡t?? --- cho há»c sinh cáº¥p 2
```
**Giáº£i thÃ­ch MSE vÃ  RMSE dá»… hiá»ƒu hÆ¡n cho há»c sinh cáº¥p 2:**

1. **MSE (Mean Square Error - Sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh):**

- **HÃ¬nh dung Ä‘Æ¡n giáº£n:**
    
    - HÃ£y tÆ°á»Ÿng tÆ°á»£ng em Ä‘ang cá»‘ gáº¯ng Ä‘oÃ¡n sá»‘ káº¹o trong má»™t lá». Náº¿u em Ä‘oÃ¡n sai, ngÆ°á»i ta sáº½ tÃ­nh xem em sai bao nhiÃªu káº¹o vÃ  nhÃ¢n Ä‘Ã´i lá»—i Ä‘Ã³ (bÃ¬nh phÆ°Æ¡ng) Ä‘á»ƒ "pháº¡t náº·ng" lá»—i lá»›n hÆ¡n.
    - VÃ­ dá»¥: Em Ä‘oÃ¡n sai 3 káº¹o, thÃ¬ má»©c sai sá»‘ lÃ  32=93^2 = 9. Náº¿u sai 1 káº¹o, má»©c sai chá»‰ lÃ  12=11^2 = 1.
- **á»¨ng dá»¥ng:**
    
    - MSE nháº¥n máº¡nh vÃ o **nhá»¯ng lá»—i Ä‘oÃ¡n sai lá»›n**, giÃºp phÃ¡t hiá»‡n xem mÃ´ hÃ¬nh cÃ³ dá»± Ä‘oÃ¡n quÃ¡ tá»‡ á»Ÿ Ä‘Ã¢u khÃ´ng.
    - DÃ¹ng khi em muá»‘n kiá»ƒm tra xem mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n cÃ³ váº¥n Ä‘á» nghiÃªm trá»ng á»Ÿ cÃ¡c lá»—i lá»›n khÃ´ng, vÃ­ dá»¥ nhÆ° trong há»‡ thá»‘ng gá»£i Ã½ mÃ  káº¿t quáº£ dá»± Ä‘oÃ¡n bá»‹ lá»‡ch quÃ¡ nhiá»u.

---

2. **RMSE (Root Mean Square Error - Sai sá»‘ cÄƒn bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh):**

- **HÃ¬nh dung Ä‘Æ¡n giáº£n:**
    
    - RMSE cÅ©ng giá»‘ng nhÆ° MSE nhÆ°ng sau khi tÃ­nh má»©c pháº¡t báº±ng bÃ¬nh phÆ°Æ¡ng, ngÆ°á»i ta láº¥y cÄƒn báº­c hai Ä‘á»ƒ Ä‘Æ°a sai sá»‘ vá» **Ä‘Æ¡n vá»‹ giá»‘ng vá»›i dá»± Ä‘oÃ¡n ban Ä‘áº§u**.
    - VÃ­ dá»¥: Thay vÃ¬ má»©c pháº¡t lÃ  99 (nhÆ° trong MSE), RMSE sáº½ láº¥y cÄƒn báº­c hai cá»§a 99, tá»©c lÃ  9=3 \sqrt{9} = 3.
- **á»¨ng dá»¥ng:**
    
    - RMSE dá»… hiá»ƒu hÆ¡n MSE vÃ¬ sai sá»‘ cuá»‘i cÃ¹ng (sau khi láº¥y cÄƒn báº­c hai) cÃ³ cÃ¹ng Ä‘Æ¡n vá»‹ vá»›i dá»¯ liá»‡u ban Ä‘áº§u.
    - ThÃ­ch há»£p khi muá»‘n **hiá»ƒu rÃµ má»©c sai trung bÃ¬nh thá»±c táº¿** cá»§a há»‡ thá»‘ng.

---

### **PhÃ¢n biá»‡t MSE vÃ  RMSE:**

|**MSE**|**RMSE**|
|---|---|
|Sai sá»‘ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch **bÃ¬nh phÆ°Æ¡ng sai lá»‡ch**.|Sai sá»‘ Ä‘Æ°á»£c tÃ­nh báº±ng **bÃ¬nh phÆ°Æ¡ng** rá»“i láº¥y **cÄƒn báº­c hai**.|
|Káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  má»™t **giÃ¡ trá»‹ lá»›n** hÆ¡n má»©c sai thá»±c táº¿, vÃ¬ sai sá»‘ bá»‹ phÃ³ng Ä‘áº¡i.|Káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  má»™t **giÃ¡ trá»‹ dá»… hiá»ƒu hÆ¡n**, gáº§n vá»›i sai thá»±c táº¿.|
|Nháº¥n máº¡nh máº¡nh hÆ¡n vÃ o **nhá»¯ng lá»—i lá»›n**.|Nháº¥n máº¡nh lá»—i lá»›n nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c Ã½ nghÄ©a vá»›i Ä‘Æ¡n vá»‹ dá»¯ liá»‡u.|
|ThÃ­ch há»£p Ä‘á»ƒ phÃ¡t hiá»‡n lá»—i cá»±c lá»›n hoáº·c phÃ¢n tÃ­ch má»©c Ä‘á»™ chá»‡ch lá»›n.|ThÃ­ch há»£p khi cáº§n Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ vÃ  hiá»ƒu má»©c sai sá»‘ thá»±c táº¿.|

---

### **VÃ­ dá»¥ cá»¥ thá»ƒ:**

- **Em Ä‘oÃ¡n Ä‘iá»ƒm bÃ i kiá»ƒm tra cá»§a báº¡n mÃ¬nh (thang 1-10):**
    
    - Báº¡n Ä‘Æ°á»£c 9 Ä‘iá»ƒm, nhÆ°ng em Ä‘oÃ¡n lÃ  6 (sai 3 Ä‘iá»ƒm).
    - Náº¿u dÃ¹ng **MSE**:
        - Sai sá»‘ lÃ  32=93^2 = 9.
        - GiÃ¡ trá»‹ lá»›n hÆ¡n lá»—i thá»±c táº¿, nháº¥n máº¡nh ráº±ng em Ä‘oÃ¡n quÃ¡ sai.
    - Náº¿u dÃ¹ng **RMSE**:
        - Sai sá»‘ lÃ  9=3 \sqrt{9} = 3.
        - Káº¿t quáº£ gáº§n vá»›i má»©c sai thá»±c táº¿ (sai 3 Ä‘iá»ƒm), giÃºp dá»… hiá»ƒu hÆ¡n.
- **Khi nÃ o dÃ¹ng MSE hoáº·c RMSE?**
    
    - DÃ¹ng **MSE** náº¿u em muá»‘n phÃ¡t hiá»‡n nhá»¯ng dá»± Ä‘oÃ¡n sai nghiÃªm trá»ng.
    - DÃ¹ng **RMSE** náº¿u em muá»‘n biáº¿t sai sá»‘ trung bÃ¬nh dá»… hiá»ƒu vÃ  sÃ¡t thá»±c táº¿.
Æ°





- NhÃ³m 2: ... 
- NhÃ³m 3: Book Recommneder system : 
		- Dataset: User Data: 
		- Aim to provide user 2 types of ouput : 
			- Top-K Movie Recommendtions 
			- Rating 
	- RMSE: ...
	- Methods: BPR, LighGCN, 
	- mETRICS: RMSE, Precision @10, Recall @10 
		- LightGCN, 
	- Cold Start Problem: 
		- with LightGCN, EmerG - Item Specific Feature Inteactions for Cold-Start CTR Prediction KDD 2024. 
		- GNN process and predict the likelihood ? 
		- EmerG designed to predict Click-through Rate CTR. 
		- Which estimate the probability that a user .... 
- Result : 
				-  Cold, Warm-A, Warm-B, Warm-C 
Model: LightGCN,   P@10, R@10, P@10, R@10, ... 
	EmerG 
Results: inclusing : MF (baseline), BPR, NCF, DeepFM, FM, BiVAE, LightGCN, EmerG. 

- Increase about result: Model: MF -> NCF -> 

---
LEARN FROM BEST PRACTICES AND MANY SUMMARY&PRACTICES OF PEOPLE 

- movie: LightGCN, BPR, DeepFM, BiVAE, NCF, MF baseline 
- RMSE, Pricision@10, Recall@10, NDCG@10

3. Book: 
- BasÃ© on Singular Value Decomposition SVD, 
- Books - User 10.3M - Item 4.4M - Rating 29.5M (Amazon Reviews Dataset). 
- 2 methods: Conllaoborative (SA SREC, SVD, Sli_Rec) + Content-base (NAML, NPA)
NPA (Neural Recommendation with Personalized Attiion), 
NAML (NeÃ‚Ml News Recommendation , ... )

Transfomer based method: SASREC, SSEPT (Collaborative filtering), ... 
Single Value SV: 


4. RAG: 
- model and identify the topic distribution for each research paper. 
- Inputt Abstracts list of artivel 
- b, Co-Author Network: Nodes each node represents an author , edge weight = average topic distributions of co-authored pper 

Apporch1: Topic Model-Based Search 
	- Topic Filtering: 500 most relevant pape based on
	- Embedding and Reranking: 
		- embedding represent absstract 
		- rerank 500
Approach2 : Author Network-Based Search 
1. Topic Filerting: Similar approach 1 => 50 most 
2. Expand Dataset : top 100 author related to above papper 
3. Embedding Reranks 
4. Final: Choose top 10 for the relationship 

Dataset: Kaggle: metadataa: id, submitter, authors, ... 



-------------------------------------

Methods Semantic Analysis 
- máº¹thod: Há»‰eachical Graph Convolutions network HIER - GCN (bert encode the review sentence and gen re)
- Domain:Hotel, Resuatl, 
- 4 sentiment: none, pos, neg, neutral
- VNCorenlp, ... 
- Method 3; 

-----------------

Group 7: Film: 
Dataset: Netlfix 2006, 5600 ratings/users, 200 rating/1 films . 
User ID, Fil,, Date, Rading 

Trying to predict the ratings in the test set when given User ID, filmID, and dataset. 
(In other words: Given)

Method ; 

SVD : soimplest solutoion 

- A is the rating matrics, all missing values 
- A = U * XÃ­ch ma V_T 
- Either learn a function R'_ij directly 
Method: Time-aware rating predction from assumption that therÃ© tesceits a shift in á»§e preferrence s
Grading Boosting decision tree: 


--------------------------------
Group 8: Extract opinions about specific aspects, obtaining deeper and more details. 
3 Main subtassk: 
- MATE Multimodel Aspect Sentiment Classification MASC : MABSSC, MCBSC. 
	- mcbsc : USE , ....
- MASC Multimode 
- ASPE PEpect S Pairs

Method: Text : RNN< LSTM, Bert 
Image : Cnn, Resnet 
Cross model attention mechainis ,; Focus on inÃ´, extract content form text or image, 

Fusion Techiniques : 
	-  Earrly fusion (Feature layer fusion) - Late fusion - Immedate ... 
Dataset;ViMACSA, ... 

Textual Data: Tokenize, Stemming Normalize remove stipss , Image : Normalize, Augmentation, .. 


3.3 Models: FCMF (main model, show in the papper, ) 
MIMN: Feature Extraction, Multimodel Memory Network, Multi, ... 
tomBERT?? 

-------------------

GROUP 8-
Machine learning: TF-IDF, Word Frequency, N-gram models, word couting 

- SVM for multi class finication : One-Rest  (1 class vs others classes)---- One vs One (classifier for every pair of classes : 20*19/2
)- Decision Tree, LSTM, Bi-LSTM
- Finetunging bÃ¡es methofs: Finetuning the pretrained RoBERTa model (a new traing strategfy for the BERT MODEL). 

dATASET: 510.000 twáº¿t, each contain 1 emoji that servers as the classifi cation 

Metrics : Macro-averate F1 Score ??



-----------------

IGROUP 10

userID, ProductID of a related product 
ouput: top N products 

Dataset: Amazon Reviews 18

Methods: content-basÃ© recommend TD-IDF, 
Collaborative Filtering: 
	- Item baed recommend with KNN
	- Single Value Decomposition
	- Náº»ual 

User_item Interaction Graph, High-order Connectity 

Hybrid Approach: Combine, lessen the impact of item Cold Start Project


Evaluation 

HR Hir Ratio: HR = U_hit/U_all



======
group 11 : 
- GNN to capture  food recommendation 
Data: Inputs: User data, restaurant data, graph structure (user - item releations hÃ­p : review,s , purchased)
-> output: recommend list: ranks food 

- Matrix factoriaxz e 
- Neighborhood based collaborative filtering

GNNs: implemment deep learning, GNNS capture complex

CKAN: 
KGNN-LS : 

Dataset : Dianping-Food, - 10 milliojjn interractins *click, purchasesm add ing* (user, item, interactions, inter-avg, entities, relationship, ..)

Metrics : RMSE< F1 score 




# 6. OPINION WEBMING: 
Opinion mining is about understanding what people think about products, services, or anything else. Hereâ€™s a simple explanation for each problem:

1. **Sentiment Analysis**: This is figuring out if a comment or review is saying something good, bad, or just neutral (neither good nor bad). For example, â€œBPhone 3, cháº¥t Ä‘áº¿n tá»«ng chi tiáº¿tâ€ is a positive comment.

2. **Opinion Summarization**: This is about summarizing reviews by identifying what people are talking about (the â€œaspectâ€) and whether they feel positive, negative, or neutral about each part of the product.

3. **Comparative Opinions**: Sometimes people compare two or more things, like saying â€œObject A is better than Object Bâ€ or â€œI like object A more than others.â€

4. **Opinions Searching**: This is about finding what people are saying about a certain object or product when we search online (group or forum, ...).

5. **Opinions Filtering**: This is sorting out fake or misleading comments, like some people writing fake good reviews (hype spam) or fake bad reviews (defaming spam).

## 6.1 Methods of Sentiment Analysis 
![[Pasted image 20241022112012.png]]

### IMPORTANCE UNSUPERVISED:  PhÃ¢n tÃ­ch Cáº£m xÃºc KhÃ´ng GiÃ¡m sÃ¡t - Giáº£i thÃ­ch cho Há»c sinh Lá»›p 3

BÃ i toÃ¡n nÃ y giá»‘ng nhÆ° viá»‡c chÃºng ta muá»‘n biáº¿t má»™t bÃ i viáº¿t nÃ³i vá» má»™t mÃ³n Ä‘á»“ chÆ¡i lÃ  tá»‘t hay xáº¥u mÃ  khÃ´ng cáº§n ai nÃ³i trÆ°á»›c cho chÃºng ta biáº¿t.

- BÆ°á»›c 1: TÃ¬m Cá»¥m Tá»« Biá»ƒu Lá»™ Ã Kiáº¿n
	- Use : Identify language patterns with potential for opinion expression) 
	- VÃ­ dá»¥: "xe Ä‘áº¹p", "Ä‘á»“ chÆ¡i bá»n", "ráº¥t vui".
- BÆ°á»›c 2: XÃ¡c Äá»‹nh HÆ°á»›ng Cáº£m XÃºc
	-  Compare whether that phrase often appears with the word "good" or "bad" in many other articles. (For example: "nice car" is more likely to go with "good" than "bad"). 
	- Sso sÃ¡nh xem cá»¥m tá»« Ä‘Ã³ thÆ°á»ng xuáº¥t hiá»‡n cÃ¹ng vá»›i tá»« "tá»‘t" hay "xáº¥u" hÆ¡n trong ráº¥t nhiá»u bÃ i viáº¿t khÃ¡c. (VÃ­ dá»¥: "xe Ä‘áº¹p" thÆ°á»ng Ä‘i vá»›i "tá»‘t" hÆ¡n lÃ  "xáº¥u", nÃªn "xe Ä‘áº¹p" lÃ  Ã½ kiáº¿n â€‹â€‹tá»‘t).
	- Step by Step: 
		1. **Äáº¿m:** Äáº¿m xem cÃ¢u nÃ³i Ä‘Ã³ xuáº¥t hiá»‡n bao nhiÃªu láº§n trÃªn máº¡ng Internet.
		2. **Äáº¿m cáº·p:** Äáº¿m xem cÃ¢u nÃ³i Ä‘Ã³ vÃ  tá»« "tá»‘t" (hoáº·c "kÃ©m") xuáº¥t hiá»‡n **cÃ¹ng nhau** bao nhiÃªu láº§n.
		3. **So sÃ¡nh:** Náº¿u cÃ¢u nÃ³i Ä‘Ã³ xuáº¥t hiá»‡n cÃ¹ng "tá»‘t" nhiá»u hÆ¡n "kÃ©m", thÃ¬ cÃ¢u nÃ³i Ä‘Ã³ lÃ  lá»i khen. NgÆ°á»£c láº¡i, náº¿u cÃ¢u nÃ³i Ä‘Ã³ xuáº¥t hiá»‡n cÃ¹ng "kÃ©m" nhiá»u hÆ¡n "tá»‘t", thÃ¬ cÃ¢u nÃ³i Ä‘Ã³ lÃ  lá»i chÃª.
- BÆ°á»›c 3: Tá»« Ä‘Ã³ Ä‘Æ°a ra káº¿t luáº­n. 
	- **VÃ­ dá»¥:** CÃ¢u nÃ³i: "Phim nÃ y hay quÃ¡!"
	- PMI("Phim nÃ y hay quÃ¡!"; "tá»‘t") cao.
	- PMI("Phim nÃ y hay quÃ¡!"; "kÃ©m") tháº¥p.

Vá»€ TOÃN: 
- Step by Step 'HOW TO CALCULATE PMI':

	1. Determine the similarity of two phrases based on the likelihood of co-occurrence on a large corpus: 
		- Large Text Set: Web Text
		- Possibility co-occurrence: Pointwise Mutual Information (PMI)
		- SO(t) = PMI(t; â€˜tá»‘tâ€™) - PMI(t; â€˜kÃ©mâ€™)
	
	2. Then, Váº¬Y THÃŒ CÃ”NG THá»¨C PMI() tÃ­nh nhÆ° nÃ o? 
		CÃ¡ch tÃ­nh **PMI (Pointwise Mutual Information)** cho má»™t tá»« t vÃ  má»™t tá»« khÃ¡c (vÃ­ dá»¥: tá»« "tá»‘t" hoáº·c "kÃ©m") dá»±a trÃªn xÃ¡c suáº¥t cá»§a chÃºng trong táº­p vÄƒn báº£n lá»›n (corpus).

		CÃ´ng thá»©c PMI Ä‘Æ°á»£c tÃ­nh nhÆ° sau:
		$PMI(t, \text{"?"}) = \log \frac{P(t, \text{"?"})}{P(t) \cdot P(\text{"?"})}$

			Trong Ä‘Ã³:
			- **P(t, "?"**) lÃ  xÃ¡c suáº¥t cá»§a tá»« t vÃ  tá»« "?" cÃ¹ng xuáº¥t hiá»‡n trong vÄƒn báº£n (xÃ¡c suáº¥t Ä‘á»“ng xuáº¥t hiá»‡n).
			- **P(t)** lÃ  xÃ¡c suáº¥t xuáº¥t hiá»‡n cá»§a tá»« t.
			- **P("?")** lÃ  xÃ¡c suáº¥t xuáº¥t hiá»‡n cá»§a tá»« "?" (vÃ­ dá»¥: tá»« "tá»‘t" hoáº·c "kÃ©m").
		
		### Chi tiáº¿t tÃ­nh toÃ¡n:
		
		1. **XÃ¡c suáº¥t P(t)**:
		   $P(t) = \frac{\text{count}(t) + 1}{\sum_{tâ€™} \text{count}(tâ€™) + V}$
		   - **count(t)**: Sá»‘ láº§n xuáº¥t hiá»‡n cá»§a tá»« t trong táº­p vÄƒn báº£n.
		   - **V**: KÃ­ch thÆ°á»›c tá»« vá»±ng (sá»‘ tá»« khÃ¡c nhau trong táº­p vÄƒn báº£n).
		
		2. **XÃ¡c suáº¥t Ä‘á»“ng xuáº¥t hiá»‡n P(t, "?")**:
		  P(t, \text{"?"}) = $\frac{\text{count}(t, \text{"?"}) + 1}{\text{Tá»•ng sá»‘ láº§n xuáº¥t hiá»‡n cá»§a táº¥t cáº£ cÃ¡c cáº·p tá»« trong vÄƒn báº£n} + V}$
		   - **count(t, "?"**): Sá»‘ láº§n t vÃ  "?" xuáº¥t hiá»‡n cÃ¹ng nhau trong má»™t cÃ¢u hoáº·c Ä‘oáº¡n vÄƒn.
	


- **Táº¡i sao tÃ­nh PMI láº¡i lÃ  cÃ´ng thá»©c kia, táº¡i sao láº¡i lÃ  P(t) * P(?) vÃ  táº¡i sao lÃºc tÃ­nh P láº¡i pháº£i count + 1/ tá»•ng sá»‘ láº§n xuáº¥t hiá»‡n + V, V lÃ  gÃ¬ ? **   => GPT for detail. 
	- **P(t)â‹…P(?)P(t) \cdot P(?)P(t)â‹…P(?)** chÃ­nh lÃ  cÃ¡ch Ä‘o **má»©c Ä‘á»™ ngáº«u nhiÃªn**: Náº¿u tá»« ttt vÃ  tá»« ??? xuáº¥t hiá»‡n ngáº«u nhiÃªn, khÃ´ng liÃªn quan Ä‘áº¿n nhau, thÃ¬ xÃ¡c suáº¥t chÃºng xuáº¥t hiá»‡n cÃ¹ng nhau chá»‰ Ä‘Æ¡n giáº£n lÃ  tÃ­ch cá»§a xÃ¡c suáº¥t tá»«ng tá»« xuáº¥t hiá»‡n riÃªng láº».
	- P(t,?), tá»©c lÃ  xÃ¡c suáº¥t Ä‘á»“ng xuáº¥t hiá»‡n thá»±c táº¿,

## 6.2 SOT - Sentinment Ontology Tree

1. explain for child: 
- HÃ£y nghÄ© vá» SOT nhÆ° má»™t **cÃ¢y gia Ä‘Ã¬nh** cho Ã½ kiáº¿n vá» sáº£n pháº©m.
- **Gá»‘c cÃ¢y** lÃ  sáº£n pháº©m báº¡n quan tÃ¢m, vÃ­ dá»¥ nhÆ° Ä‘iá»‡n thoáº¡i.
- **CÃ¡c nhÃ¡nh cÃ¢y** lÃ  cÃ¡c Ä‘áº·c Ä‘iá»ƒm cá»§a sáº£n pháº©m, vÃ­ dá»¥ nhÆ° mÃ n hÃ¬nh, pin, camera.
- **LÃ¡ cÃ¢y** lÃ  Ã½ kiáº¿n â€‹â€‹cá»§a má»i ngÆ°á»i vá» nhá»¯ng Ä‘áº·c Ä‘iá»ƒm Ä‘Ã³.
    - **LÃ¡ xanh:** Ã kiáº¿n â€‹â€‹tÃ­ch cá»±c, vÃ­ dá»¥ nhÆ° "mÃ n hÃ¬nh Ä‘áº¹p", "pin lÃ¢u".
    - **LÃ¡ Ä‘á»:** Ã kiáº¿n â€‹â€‹tiÃªu cá»±c, vÃ­ dá»¥ nhÆ° "camera má»", "giÃ¡ cao".
2. While SOT offers a robust solution for fine-grained sentiment analysis and is well-regarded for certain research purposes, its broader adoption is limited by its complexity and lack of widespread tool support.



```
IE architecture

IE problems

NER: CRF, LSTM

Relation extraction - Snowball --- Distant supervision

Coreperence resolution
```

Snowball: 

### Giáº£i ThÃ­ch PhÆ°Æ¡ng PhÃ¡p Snowball ÄÆ¡n Giáº£n cho Há»c Sinh Cáº¥p 2

**PhÆ°Æ¡ng phÃ¡p Snowball** lÃ  má»™t cÃ¡ch mÃ  mÃ¡y tÃ­nh sá»­ dá»¥ng Ä‘á»ƒ tÃ¬m hiá»ƒu vÃ  nháº­n diá»‡n cÃ¡c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« hoáº·c cá»¥m tá»« trong vÄƒn báº£n. HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n Ä‘ang xÃ¢y dá»±ng má»™t **cÃ¡i vÃ²ng tuyáº¿t** (snowball) tá»« nhá»¯ng háº¡t tuyáº¿t nhá». Má»—i háº¡t tuyáº¿t thÃªm vÃ o sáº½ lÃ m cho vÃ²ng tuyáº¿t trá»Ÿ nÃªn lá»›n hÆ¡n vÃ  máº¡nh máº½ hÆ¡n. TÆ°Æ¡ng tá»±, phÆ°Æ¡ng phÃ¡p Snowball báº¯t Ä‘áº§u tá»« má»™t vÃ i vÃ­ dá»¥ nhá» vÃ  dáº§n dáº§n má»Ÿ rá»™ng Ä‘á»ƒ nháº­n diá»‡n nhiá»u má»‘i quan há»‡ hÆ¡n.

#### **VÃ­ dá»¥ Minh Há»a:**

Giáº£ sá»­ báº¡n muá»‘n dáº¡y mÃ¡y tÃ­nh nháº­n biáº¿t má»‘i quan há»‡ "tÃ¡c giáº£ viáº¿t sÃ¡ch".

1. **BÆ°á»›c 1: Báº¯t Äáº§u vá»›i VÃ­ Dá»¥ Ban Äáº§u (Seed Words)**
    
    - Báº¡n cung cáº¥p cho mÃ¡y tÃ­nh má»™t vÃ i cÃ¢u máº«u chá»©a má»‘i quan há»‡ nÃ y, cháº³ng háº¡n:
        - "J.K. Rowling viáº¿t Harry Potter."
        - "George Orwell viáº¿t 1984."
2. **BÆ°á»›c 2: TÃ¬m CÃ¡c Máº«u CÃ¢u (Pattern)**
    
    - MÃ¡y tÃ­nh sáº½ nháº­n ra ráº±ng trong cÃ¡c cÃ¢u trÃªn, tá»« "viáº¿t" thÆ°á»ng xuáº¥t hiá»‡n giá»¯a tÃªn tÃ¡c giáº£ vÃ  tÃªn sÃ¡ch.
    - NhÆ° váº­y, máº«u cÃ¢u cÃ³ thá»ƒ lÃ : "**[TÃªn TÃ¡c Giáº£] viáº¿t [TÃªn SÃ¡ch]**".
3. **BÆ°á»›c 3: Má»Ÿ Rá»™ng TÃ¬m Kiáº¿m**
    
    - Dá»±a vÃ o máº«u cÃ¢u nÃ y, mÃ¡y tÃ­nh sáº½ quÃ©t toÃ n bá»™ vÄƒn báº£n Ä‘á»ƒ tÃ¬m thÃªm cÃ¡c cÃ¢u cÃ³ cáº¥u trÃºc tÆ°Æ¡ng tá»±.
    - VÃ­ dá»¥:
        - "Agatha Christie viáº¿t Máº­t MÃ£ Da Vinci."
        - "Haruki Murakami viáº¿t Kafka bÃªn bá» biá»ƒn."
4. **BÆ°á»›c 4: XÃ¡c Nháº­n vÃ  Má»Ÿ Rá»™ng Má»‘i Quan Há»‡**
    
    - Khi mÃ¡y tÃ­nh tÃ¬m tháº¥y nhiá»u vÃ­ dá»¥ hÆ¡n, nÃ³ sáº½ hiá»ƒu rÃµ hÆ¡n vá» má»‘i quan há»‡ "tÃ¡c giáº£ viáº¿t sÃ¡ch" vÃ  cÃ³ thá»ƒ nháº­n diá»‡n Ä‘Æ°á»£c nhiá»u cÃ¢u hÆ¡n mÃ  khÃ´ng chá»‰ giá»›i háº¡n trong nhá»¯ng vÃ­ dá»¥ ban Ä‘áº§u.

#### **Æ¯u Äiá»ƒm cá»§a PhÆ°Æ¡ng PhÃ¡p Snowball:**

- **ÄÆ¡n Giáº£n:** Dá»… hiá»ƒu vÃ  dá»… triá»ƒn khai.
- **Tá»± Äá»™ng Má»Ÿ Rá»™ng:** CÃ³ kháº£ nÄƒng tÃ¬m kiáº¿m vÃ  nháº­n diá»‡n nhiá»u má»‘i quan há»‡ má»›i dá»±a trÃªn nhá»¯ng gÃ¬ Ä‘Ã£ há»c.

#### **Háº¡n Cháº¿:**

- **Phá»¥ Thuá»™c VÃ o VÃ­ Dá»¥ Ban Äáº§u:** Náº¿u cÃ¡c vÃ­ dá»¥ ban Ä‘áº§u khÃ´ng Ä‘á»§ Ä‘a dáº¡ng, mÃ¡y tÃ­nh cÃ³ thá»ƒ bá» sÃ³t nhiá»u má»‘i quan há»‡ quan trá»ng.
- **Dá»… Bá»‹ Láº¡c ÄÆ°á»ng:** CÃ³ thá»ƒ tÃ¬m tháº¥y cÃ¡c má»‘i quan há»‡ khÃ´ng chÃ­nh xÃ¡c náº¿u khÃ´ng kiá»ƒm soÃ¡t cháº·t cháº½.

#### **TÃ³m Láº¡i:**

PhÆ°Æ¡ng phÃ¡p Snowball giá»‘ng nhÆ° viá»‡c báº¡n báº¯t Ä‘áº§u tá»« má»™t háº¡t tuyáº¿t nhá» vÃ  dáº§n dáº§n xÃ¢y dá»±ng thÃ nh má»™t vÃ²ng tuyáº¿t lá»›n hÆ¡n. Báº±ng cÃ¡ch báº¯t Ä‘áº§u vá»›i má»™t vÃ i vÃ­ dá»¥, mÃ¡y tÃ­nh cÃ³ thá»ƒ tá»± Ä‘á»™ng tÃ¬m kiáº¿m vÃ  nháº­n diá»‡n nhiá»u má»‘i quan há»‡ hÆ¡n trong vÄƒn báº£n. ÄÃ¢y lÃ  má»™t cÃ´ng cá»¥ há»¯u Ã­ch giÃºp mÃ¡y tÃ­nh hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  trÃ­ch xuáº¥t thÃ´ng tin quan trá»ng tá»« cÃ¡c Ä‘oáº¡n vÄƒn báº£n lá»›n.


### Understanding the Snowball Method in Information Extraction

The **Snowball method** is a technique used in **Relation Extraction**, a subfield of **Information Extraction (IE)** in Natural Language Processing (NLP). Its primary goal is to identify and classify relationships between entities (like people, organizations, locations) within large volumes of text. Here's a breakdown of what the Snowball method does and how it operates:

#### **1. Purpose of the Snowball Method**

- **Identify Relationships:** The Snowball method is designed to discover and extract specific types of relationships between entities in text. For example, identifying that "Albert Einstein **developed** the theory of relativity" establishes a "developer-of" relationship between "Albert Einstein" and "the theory of relativity."
    
- **Expand Knowledge Bases:** By extracting relationships from vast amounts of text, the Snowball method helps in building and enriching structured databases or knowledge graphs, which can be used for various applications like search engines, recommendation systems, and question-answering systems.
    

#### **2. How the Snowball Method Works**

The Snowball method is primarily **rule-based** and operates through iterative pattern matching and expansion. Here's a step-by-step overview:

##### **a. Start with Seed Patterns and Examples**

- **Seed Patterns:** Begin with a few predefined patterns that represent the relationship you're interested in extracting. For instance, to extract "author-of" relationships, seed patterns might include:
    - "[Author] **wrote** [Book]"
    - "[Author] **authored** [Novel]"
- **Seed Examples:** Provide initial examples that fit these patterns:
    - "J.K. Rowling **wrote** Harry Potter."
    - "George Orwell **authored** 1984."

##### **b. Pattern Matching and Extraction**

- **Scan Text:** Use the seed patterns to scan large corpora (collections of text) and identify sentences that match these patterns.
    
- **Extract Relationships:** When a sentence matches a pattern, extract the entities and their relationship. From "J.K. Rowling wrote Harry Potter," extract ("J.K. Rowling", "wrote", "Harry Potter").
    

##### **c. Expand Patterns and Examples**

- **Iterative Expansion:** Use the newly extracted examples to discover new patterns or refine existing ones. For example, if the method frequently encounters variations like "J.K. Rowling **authored** Harry Potter," it can add "**authored**" as another pattern synonym for "wrote."
    
- **Generalization:** Broaden the patterns to capture more diverse expressions of the same relationship. This helps in identifying relationships expressed in different linguistic forms.
    

##### **d. Repeat the Process**

- **Snowball Effect:** As more patterns and examples are discovered, the method can uncover increasingly diverse and numerous instances of the target relationship, much like a snowball growing larger as it rolls.

##### **e. Filter and Validate**

- **Remove Noise:** Not all extracted relationships will be correct. Implement filtering mechanisms to eliminate false positives, ensuring the accuracy of the extracted data.
    
- **Refinement:** Continuously refine patterns based on feedback and validation to improve precision and recall.
    

#### **3. Advantages of the Snowball Method**

- **Scalability:** Capable of processing large volumes of text to extract a wide range of relationships without requiring extensive manual annotation.
    
- **Flexibility:** Can be adapted to extract different types of relationships by modifying seed patterns and examples.
    
- **Efficiency:** Automates the discovery of relationships, reducing the need for manual intervention.
    

#### **4. Limitations of the Snowball Method**

- **Dependency on Seed Patterns:** The quality and diversity of seed patterns significantly influence the method's effectiveness. Poorly chosen seeds can lead to limited or biased extraction.
    
- **Pattern Maintenance:** As language evolves or varies across different domains, maintaining and updating patterns can become challenging.
    
- **Noise and Precision:** Rule-based methods can generate false positives, especially in complex or ambiguous linguistic contexts. Ensuring high precision often requires additional filtering steps.
    

#### **5. Practical Example**

Imagine you're using the Snowball method to extract "parent-of" relationships from a collection of biographies:

1. **Seed Patterns:**
    
    - "[Parent] **is the parent of** [Child]"
    - "[Parent] **has a child named** [Child]"
2. **Initial Extraction:**
    
    - "Maria **is the parent of** Sophia."
    - "John **has a child named** Michael."
3. **Pattern Expansion:**
    
    - Discover sentences like "Maria **raised** Sophia," leading to adding "**raised**" as a new pattern.
4. **Further Extraction:**
    
    - "Carlos **brought up** Elena," extracting ("Carlos", "brought up", "Elena").
5. **Final Output:**
    
    - A comprehensive list of parent-child relationships extracted from the text.

#### **6. Comparison with Other Methods**

- **Rule-Based vs. Machine Learning:**
    
    - **Snowball (Rule-Based):** Relies on predefined linguistic patterns and iterative expansion. It's transparent and easier to control but may lack flexibility and require significant manual effort for pattern creation.
    - **Machine Learning-Based Methods:** Utilize statistical models or deep learning to learn patterns from data automatically. They can capture more nuanced relationships but often require large annotated datasets and can be less interpretable.
- **Snowball vs. Distant Supervision:**
    
    - **Snowball:** Begins with seed patterns and expands through pattern matching, focusing on rule-based extraction.
    - **Distant Supervision:** Uses existing databases to automatically label training data, assuming that sentences mentioning entity pairs express their relationship, which can introduce noise but leverages large datasets for training machine learning models.

#### **7. Modern Enhancements**

While the traditional Snowball method is purely rule-based, modern approaches often integrate it with machine learning to improve accuracy and scalability:

- **Hybrid Models:** Combine rule-based pattern matching with statistical models to validate and refine extracted relationships.
    
- **Natural Language Processing Tools:** Utilize advanced NLP techniques like dependency parsing to enhance pattern matching accuracy.
    
- **Feedback Loops:** Implement mechanisms where extracted data is reviewed and used to update and improve patterns continuously.
    

### **Conclusion**

The Snowball method is a foundational technique in Relation Extraction, leveraging iterative pattern matching to discover and extract meaningful relationships between entities in text. Its simplicity and scalability make it a valuable tool, especially when combined with other methods to enhance precision and adaptability. Understanding the Snowball method provides a stepping stone towards more advanced information extraction techniques used in modern NLP applications.



### Giáº£i ThÃ­ch Äoáº¡n CÃ´ng Thá»©c Snowball ÄÆ¡n Giáº£n Cho Há»c Sinh Cáº¥p 2

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n Ä‘ang chÆ¡i trÃ² tÃ¬m kiáº¿m kho bÃ¡u vá»›i nhá»¯ng manh má»‘i xung quanh. Äoáº¡n cÃ´ng thá»©c dÆ°á»›i Ä‘Ã¢y giá»‘ng nhÆ° hÆ°á»›ng dáº«n cÃ¡ch báº¡n tÃ¬m kho bÃ¡u Ä‘Ã³ má»™t cÃ¡ch thÃ´ng minh vÃ  hiá»‡u quáº£. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch giáº£i thÃ­ch tá»«ng bÆ°á»›c má»™t cÃ¡ch dá»… hiá»ƒu:

#### **Äoáº¡n CÃ´ng Thá»©c:**

```plaintext
1) {< o, ` >,< ls, t1, ms, t2, rs >} = = CreateOccurrence(text segment);
   TC = < o, ` >; SimBest = 0;
   foreach p in Patterns
2)    sim = Match(< ls, t1, ms, t2, rs >, p);
       if ( sim â‰¥ Ï„sim )
3)          UpdatePatternSelectivity( p, TC );
           if( sim â‰¥ SimBest )
               SimBest = sim;
               PBest = p;
   if( SimBest â‰¥ Ï„sim )
       CandidateTuples [TC].Patterns [PBest] = SimBest;
   return CandidateTuples;
```

#### **Giáº£i ThÃ­ch BÆ°á»›c Qua BÆ°á»›c:**

##### **BÆ°á»›c 1: Táº¡o Xuáº¥t Hiá»‡n (CreateOccurrence)**

- **Äiá»u GÃ¬ Xáº£y Ra:**
    
    - MÃ¡y tÃ­nh Ä‘á»c má»™t Ä‘oáº¡n vÄƒn báº£n vÃ  tÃ¬m tháº¥y hai thá»±c thá»ƒ (vÃ­ dá»¥: má»™t cÃ´ng ty vÃ  má»™t Ä‘á»‹a Ä‘iá»ƒm).
    - NÃ³ táº¡o ra má»™t **5-tuple** gá»“m cÃ¡c pháº§n thÃ´ng tin xung quanh hai thá»±c thá»ƒ nÃ y: bÃªn trÃ¡i, giá»¯a vÃ  bÃªn pháº£i.
- **VÃ­ Dá»¥:**
    
    - Giáº£ sá»­ trong cÃ¢u "Microsoft, Redmond, Ä‘Ã£ ra máº¯t sáº£n pháº©m má»›i," "Microsoft" lÃ  ORGANIZATION (cÃ´ng ty) vÃ  "Redmond" lÃ  LOCATION (Ä‘á»‹a Ä‘iá»ƒm).
    - MÃ¡y tÃ­nh sáº½ lÆ°u láº¡i thÃ´ng tin xung quanh hai tá»« nÃ y Ä‘á»ƒ so sÃ¡nh vá»›i cÃ¡c máº«u Ä‘Ã£ biáº¿t.

##### **BÆ°á»›c 2: So SÃ¡nh Vá»›i CÃ¡c Máº«u (Match)**

- **Äiá»u GÃ¬ Xáº£y Ra:**
    
    - MÃ¡y tÃ­nh sáº½ kiá»ƒm tra xem Ä‘oáº¡n vÄƒn báº£n vá»«a tÃ¬m tháº¥y cÃ³ phÃ¹ há»£p vá»›i báº¥t ká»³ máº«u nÃ o trong danh sÃ¡ch cÃ¡c **Patterns** (máº«u trÃ­ch xuáº¥t) hay khÃ´ng.
    - **sim** lÃ  má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a Ä‘oáº¡n vÄƒn báº£n vÃ  máº«u.
- **Äiá»u Kiá»‡n Kiá»ƒm Tra:**
    
    - Náº¿u má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng **sim** lá»›n hÆ¡n hoáº·c báº±ng má»™t ngÆ°á»¡ng Ä‘Ã£ Ä‘á»‹nh **Ï„sim**, mÃ¡y tÃ­nh sáº½ xem xÃ©t máº«u Ä‘Ã³ lÃ  phÃ¹ há»£p.

##### **BÆ°á»›c 3: Cáº­p Nháº­t Äá»™ Chá»n Lá»c vÃ  Chá»n Máº«u Tá»‘t Nháº¥t**

- **Cáº­p Nháº­t Äá»™ Chá»n Lá»c (UpdatePatternSelectivity):**
    
    - MÃ¡y tÃ­nh cáº­p nháº­t thÃ´ng tin vá» má»©c Ä‘á»™ tin cáº­y cá»§a máº«u **p** dá»±a trÃªn viá»‡c nÃ³ vá»«a trÃ¹ng khá»›p vá»›i Ä‘oáº¡n vÄƒn báº£n nÃ y hay khÃ´ng.
- **Chá»n Máº«u Tá»‘t Nháº¥t:**
    
    - Náº¿u **sim** hiá»‡n táº¡i lá»›n hÆ¡n hoáº·c báº±ng **SimBest** (má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng tá»‘t nháº¥t hiá»‡n táº¡i), mÃ¡y tÃ­nh sáº½ cáº­p nháº­t **SimBest** vÃ  lÆ°u láº¡i máº«u **PBest** lÃ  máº«u cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t.
- **LÆ°u Káº¿t Quáº£:**
    
    - Náº¿u **SimBest** váº«n lá»›n hÆ¡n hoáº·c báº±ng **Ï„sim**, mÃ¡y tÃ­nh sáº½ lÆ°u láº¡i ráº±ng cáº·p thá»±c thá»ƒ **TC** (vÃ­ dá»¥: Microsoft vÃ  Redmond) Ä‘Æ°á»£c há»— trá»£ bá»Ÿi máº«u **PBest** vá»›i má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng **SimBest**.

##### **Tráº£ Vá» Káº¿t Quáº£ (Return CandidateTuples):**

- **Äiá»u GÃ¬ Xáº£y Ra:**
    - MÃ¡y tÃ­nh tráº£ vá» danh sÃ¡ch cÃ¡c cáº·p thá»±c thá»ƒ cÃ¹ng vá»›i cÃ¡c máº«u Ä‘Ã£ há»— trá»£ chÃºng. ÄÃ¢y chÃ­nh lÃ  nhá»¯ng má»‘i quan há»‡ mÃ  mÃ¡y tÃ­nh Ä‘Ã£ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c tá»« Ä‘oáº¡n vÄƒn báº£n.

#### **TÃ³m Táº¯t Báº±ng VÃ­ Dá»¥:**

1. **TÃ¬m Xuáº¥t Hiá»‡n:**
    
    - MÃ¡y tÃ­nh Ä‘á»c cÃ¢u "Microsoft, Redmond, Ä‘Ã£ ra máº¯t sáº£n pháº©m má»›i" vÃ  xÃ¡c Ä‘á»‹nh "Microsoft" lÃ  cÃ´ng ty vÃ  "Redmond" lÃ  Ä‘á»‹a Ä‘iá»ƒm. NÃ³ cÅ©ng ghi nhá»› cÃ¡c tá»« xung quanh hai tá»« nÃ y nhÆ° "Microsoft, Redmond, Ä‘Ã£ ra máº¯t".
2. **So SÃ¡nh Vá»›i Máº«u:**
    
    - MÃ¡y tÃ­nh so sÃ¡nh Ä‘oáº¡n "Microsoft, Redmond, Ä‘Ã£ ra máº¯t" vá»›i cÃ¡c máº«u Ä‘Ã£ biáº¿t nhÆ° "<tá»« bÃªn trÃ¡i>, LOCATION, <tá»« giá»¯a>, ORGANIZATION, <tá»« bÃªn pháº£i>".
    - Náº¿u má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao (vÃ­ dá»¥: **sim = 0.8** vÃ  **Ï„sim = 0.7**), nÃ³ sáº½ tiáº¿p tá»¥c xá»­ lÃ½.
3. **Cáº­p Nháº­t vÃ  LÆ°u Káº¿t Quáº£:**
    
    - MÃ¡y tÃ­nh cáº­p nháº­t Ä‘á»™ tin cáº­y cá»§a máº«u vÃ  lÆ°u láº¡i ráº±ng cáº·p "Microsoft - Redmond" phÃ¹ há»£p vá»›i máº«u nÃ y vá»›i má»©c Ä‘á»™ tin cáº­y lÃ  0.8.
4. **HoÃ n ThÃ nh:**
    
    - Cuá»‘i cÃ¹ng, mÃ¡y tÃ­nh tráº£ vá» danh sÃ¡ch cÃ¡c cáº·p thá»±c thá»ƒ vÃ  máº«u Ä‘Ã£ há»— trá»£ chÃºng, vÃ­ dá»¥: ("Microsoft", "Redmond") Ä‘Æ°á»£c há»— trá»£ bá»Ÿi máº«u vá»›i **sim = 0.8**.

#### **Káº¿t Luáº­n:**

Äoáº¡n cÃ´ng thá»©c nÃ y giÃºp mÃ¡y tÃ­nh tá»± Ä‘á»™ng tÃ¬m kiáº¿m vÃ  xÃ¡c Ä‘á»‹nh cÃ¡c má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ trong vÄƒn báº£n báº±ng cÃ¡ch so sÃ¡nh cÃ¡c Ä‘oáº¡n vÄƒn báº£n vá»›i cÃ¡c máº«u Ä‘Ã£ biáº¿t. QuÃ¡ trÃ¬nh nÃ y giá»‘ng nhÆ° viá»‡c báº¡n sá»­ dá»¥ng cÃ¡c manh má»‘i Ä‘á»ƒ tÃ¬m kho bÃ¡u, nÆ¡i mÃ¡y tÃ­nh sá»­ dá»¥ng cÃ¡c máº«u vÃ  má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c má»‘i quan há»‡ má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£.