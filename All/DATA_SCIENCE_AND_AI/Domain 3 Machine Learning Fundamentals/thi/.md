# Bagging 


Mình giải thích ngắn – dễ nhớ nhé.

# Bagging là gì (trong Random Forest)?

 **Bagging = Bootstrap Aggregating** . Ý tưởng: tạo **nhiều bộ dữ liệu con** bằng cách **lấy mẫu có hoàn lại (bootstrap)** từ dữ liệu gốc, huấn luyện **nhiều cây** độc lập trên các mẫu này, rồi  **gộp dự đoán** :

* Phân loại: lấy  **đa số phiếu** .
* Hồi quy: lấy  **trung bình** .

# Vì sao làm vậy?

* Các cây học từ những mẫu khác nhau → **ít tương quan** với nhau.
* Khi gộp lại,  **phương sai (variance) giảm mạnh** , dự đoán ổn định hơn → mô hình ít “quá khớp” hơn so với một cây đơn lẻ.

# Các bước bagging trong Random Forest

1. Lặp B lần (ví dụ 100 cây):
   * Lấy 1 mẫu bootstrap (cùng kích thước,  **có hoàn lại** ).
   * Huấn luyện một cây quyết định trên mẫu này.
   * (Đặc trưng RF còn **random feature** ở mỗi nút: chỉ xét k đặc trưng ngẫu nhiên khi tách – đôi khi gọi là  *feature bagging* .)
2. Khi dự đoán: cho điểm mới đi qua **tất cả** cây → gộp kết quả.

# Bonus: OOB (Out-of-Bag)

Vì lấy mẫu có hoàn lại, khoảng **~36%** điểm gốc **không xuất hiện** trong mỗi bootstrap. Ta dùng chính các điểm “bị bỏ quên” này để **đánh giá sai số OOB** – gần như một phép cross-validation “miễn phí”.

# So với boosting thì sao?

* **Bagging** : các cây  **độc lập** , gộp để  **giảm variance** .
* **Boosting** : các cây  **tuần tự** , cây sau sửa lỗi cây trước →  **giảm bias** , nhưng dễ quá khớp nếu không regularize.

---

Một câu check nhanh: theo bạn,  **bagging chủ yếu giảm bias hay giảm variance** ? (chọn một)

Nếu thích, mình làm luôn một ví dụ số nhỏ để bạn thấy rõ hiệu ứng gộp.


---



B. Đúng — Random Forest dùng bootstrap (mỗi cây huấn luyện trên một mẫu dữ liệu khác nhau) để tăng đa dạng.

C. Đúng — Ở mỗi nút chỉ xét một tập con đặc trưng ngẫu nhiên (feature bagging), tiếp tục tăng đa dạng giữa các cây.

Sao C lại đúng ?

---

C đúng vì đây chính là “feature bagging” – cơ chế cốt lõi thứ hai của Random Forest.

* Ở  **mỗi nút tách** , thay vì xét tất cả (p) đặc trưng, rừng chỉ **lấy ngẫu nhiên một tập con** (kích thước (m_\text{try})) để tìm split tốt nhất.
* Việc **giới hạn & ngẫu nhiên hoá đặc trưng** làm các cây ít dùng cùng một đặc trưng/split ⇒ **giảm tương quan giữa cây** ⇒ tăng **đa dạng** của rừng, giúp tổng hợp (bỏ phiếu/trung bình) giảm phương sai.

Một vài điểm cụ thể:

* Nếu có một đặc trưng “quá mạnh”, xét toàn bộ (p) đặc trưng sẽ khiến **mọi cây giống nhau** (đều chọn đặc trưng đó ở gần gốc). Giảm về (m_\text{try}) khiến đặc trưng mạnh  **đôi khi không được đưa vào danh sách xét** , buộc cây chọn hướng khác → đa dạng hơn.
* **Điều chỉnh (m_\text{try})** là nút chỉnh “đa dạng ↔ sức mạnh cây”:
  * (m_\text{try}) **nhỏ** →  **đa dạng↑** , tương quan↓ (nhưng mỗi cây yếu hơn chút).
  * (m_\text{try}) **lớn** → cây mạnh hơn, nhưng tương quan↑, đa dạng↓.
* Thực tế hay dùng:  **classification** : (m_\text{try}\approx \sqrt{p});  **regression** : (m_\text{try}\approx p/3) (chỉ là kinh nghiệm, có thể tune).

Lưu ý về câu chữ: “ **thay đổi số lượng đặc trưng tại mỗi nút** ” ở đây nên hiểu là **không xét toàn bộ đặc trưng** mà **chỉ xét một số lượng (mtry) nhỏ, được chọn ngẫu nhiên** ở  **mỗi nút** . Số lượng (m_\text{try}) có thể giữ **cố định** cho cả cây/rừng (chuẩn RF), nhưng  **tập đặc trưng được rút thăm thì thay đổi theo từng nút** , và chính điều đó tạo đa dạng.


---



Ok, nói “dễ nuốt” nè 😄

## Vì sao C đúng?

**C: “Thay vì xét tất cả đặc trưng, mỗi nút chỉ xét MỘT NHÓM ĐẶC TRƯNG NGẪU NHIÊN (mtry).”**

→ Đây gọi là **feature bagging** – mẹo chính của Random Forest để  **làm các cây khác nhau** .

### Hình dung đơn giản

* Nếu **xét tất cả đặc trưng** ở mọi nút, “đặc trưng mạnh nhất” sẽ **luôn** được chọn ở gần gốc → **các cây giống nhau** → dự đoán bỏ phiếu cũng… giống nhau ⇒  **ít đa dạng** .
* Khi **mỗi nút chỉ được bốc thăm vài đặc trưng** (ví dụ 2 trong tổng 10):
  * Nhiều nút **không thấy** đặc trưng mạnh nhất trong danh sách → buộc phải chọn đặc trưng khác.
  * Mỗi cây đi theo **nhánh khác nhau** → **đa dạng** ↑, **tương quan giữa cây** ↓ → gộp lại ( **đa số/trung bình** ) sẽ **ổn định hơn** (giảm variance).

### Ví dụ 10s

Giả sử có 5 đặc trưng: A (rất mạnh), B,C,D,E.

* Nếu xét **tất cả 5** ở gốc: cây nào cũng chọn **A** → cây na ná nhau.
* Nếu mỗi nút chỉ xét  **mtry = 2** : đôi khi gốc **không có A** trong danh sách → cây đó chọn B hoặc C… → rừng đa dạng.

### Ghi nhớ nhanh

* **mtry nhỏ** → đa dạng ↑ (mỗi cây yếu hơn chút, cả rừng ổn định hơn).
* **mtry lớn** → mỗi cây mạnh hơn nhưng giống nhau hơn (đa dạng ↓).
* Kinh nghiệm: phân loại dùng  **mtry ≈ √p** , hồi quy dùng **mtry ≈ p/3** (tune thêm tùy bài).

=> Nên C đúng, vì **giới hạn & rút thăm đặc trưng ở mỗi nút** chính là cách Random Forest tạo  **đa dạng giữa các cây** .



---

![1759071136852](image/.md/1759071136852.png)


**B. 4 → 6 → 5 → 1 → 3 → 2.**

* **4** : Tạo N bootstrap từ dữ liệu gốc.
* **6** : Với mỗi bootstrap, xây một cây quyết định.
* **5** : Ở mỗi nút, chọn ngẫu nhiên k (ở đề là 2) đặc trưng ( *feature bagging* ).
* **1** : Tính Gini/Entropy để chọn split tốt nhất.
* **3** : (nếu theo quy trình đề bài) loại đặc trưng đã chọn trong nút đó.
* **2** : Lặp quá trình cho tới khi điều kiện dừng thỏa.

---

![1759071289750](image/.md/1759071289750.png)
