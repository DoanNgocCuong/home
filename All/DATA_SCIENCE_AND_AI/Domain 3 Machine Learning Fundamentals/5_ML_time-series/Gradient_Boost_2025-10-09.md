https://lms.aivietnam.edu.vn/api/files/683184ca519c0e157fb514cd/Documents%2F2025-8%2FM04W02%20-%20Gradient%20Boosting%2FAIO2025_GradientBoosting_v2.pdf

1. Táº¡i sao dÃ¹ng giÃ¡ trá»‹ trung bÃ¬nh?
2. Residual Error?
3. 

![1757513306598](image/Gradient_Boost_2025-10-09/1757513306598.png)

Khá»Ÿi táº¡o Gradient Boost táº¡i giÃ¡ trá»‹ trung bÃ¬nh vÃ¬ táº¡i Ä‘Ã³ lost bÃ© nháº¥t.

dSSR/d theta = - (...) VÃ¬ Ä‘á»ƒ lost giáº£m nÃªn Ä‘áº¡o hfn

![1757511829580](image/Gradient_Boost_2025-10-09/1757511829580.png)

![1757514765568](image/Gradient_Boost_2025-10-09/1757514765568.png)

---

![1757514465545](image/Gradient_Boost_2025-10-09/1757514465545.png)

```python
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor

# XGBoost
cv_split = TimeSeriesSplit(n_splits=4, test_size=100)
model = GradientBoostingRegressor()
parameters = {
    "max_features": [3, 4, 5],
    "learning_rate": [0.01, 0.05],
    "n_estimators": [100, 300]
}

grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)
grid_search.fit(X_train, y_train)

```

---

![1757514975500](image/Gradient_Boost_2025-10-09/1757514975500.png)

---

![1757515897580](image/Gradient_Boost_2025-10-09/1757515897580.png)

---

![1757516512672](image/Gradient_Boost_2025-10-09/1757516512672.png)

![1757516913998](image/Gradient_Boost_2025-10-09/1757516913998.png)

```python

# Import Library
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix
)

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Chuyá»ƒn thÃ nh DataFrame Ä‘á»ƒ dá»… quan sÃ¡t
df = pd.DataFrame(X, columns=data.feature_names)
df['target'] = y
print(df.head())

# Split data to train/test (giá»¯ tá»· lá»‡ lá»›p báº±ng stratify)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("\nSá»‘ lÆ°á»£ng máº«u train:", X_train.shape[0])
print("Sá»‘ lÆ°á»£ng máº«u test:", X_test.shape[0])

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh cÆ¡ báº£n
gb = GradientBoostingClassifier(random_state=42)

# LÆ°á»›i tham sá»‘
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [2, 3, 4],
    'subsample': [0.8, 1.0]
}

# GridSearch vá»›i cross-validation = 5
grid_search = GridSearchCV(
    estimator=gb,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    scoring='accuracy',
    verbose=2
)

# Train GridSearch
grid_search.fit(X_train, y_train)

# Get model with best hyperparameter
print("Best parameters:", grid_search.best_params_)
print("Best cross-validation accuracy:", grid_search.best_score_)

best_gb = grid_search.best_estimator_

# Evaluate on test set
y_pred = best_gb.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

```

---

# 14/09/2025

1. HÃ m loss cá»§a nÃ³?
2. VÃ­ dá»¥:
3. init
   v

```
CÃ¡ch khá»Ÿi táº¡o learning rate?

Äá»ƒ Ã½ learning rate máº·c Ä‘á»‹nh cá»§a cÃ¡c thÆ° viá»‡n

Khá»Ÿi táº¡o learning rate theo cÃ¡c giáº£i thuáº­t Ä‘áº·c biá»‡t?

```

![1757857978330](image/Gradient_Boost_2025-10-09/1757857978330.png)![1757857737995](image/Gradient_Boost_2025-10-09/1757857737995.png)

- **Äá»‹nh nghÄ©a**: `n_estimators` Ä‘áº¡i diá»‡n cho sá»‘ lÆ°á»£ng cÃ¢y quyáº¿t Ä‘á»‹nh (decision trees) mÃ  mÃ´ hÃ¬nh sáº½ táº¡o ra. Má»—i cÃ¢y trong mÃ´ hÃ¬nh sáº½ há»c tá»« má»™t pháº§n cá»§a dá»¯ liá»‡u vÃ  Ä‘Ã³ng gÃ³p vÃ o dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng.
- `verbose=1`  tham sá»‘ verbose nÃ³i chung trong CS thÆ°á»ng mang Ã½ nghÄ©a lÃ  thÃ´ng tin chi tiáº¿t: thÆ°á»ng dÃ¹ng trong gá»¡ lá»—i, giÃ¡m sÃ¡t, chi tiáº¿t thÃªm thÃ´ng tin vá» 1 quÃ¡ trÃ¬nh Ä‘ang diá»…n ra nÃ o Ä‘áº¥y

![1757859178368](image/Gradient_Boost_2025-10-09/1757859178368.png)

![1757860129488](image/Gradient_Boost_2025-10-09/1757860129488.png)

MÃ¬nh giáº£i thÃ­ch chi tiáº¿t hÃ¬nh XGBoost báº¡n gá»­i nhÃ©:

---

## 1. Ngá»¯ cáº£nh vÃ­ dá»¥

- Dá»¯ liá»‡u cÃ³ 4 Ä‘iá»ƒm:

  ```
  X   Y
  23  0
  24  0
  26  1
  27  1
  ```
- Tham sá»‘: Î»=0\lambda = 0, Ä‘á»™ sÃ¢u cÃ¢y = 1 (stump), learning rate (Î·) = 0.3.

Má»¥c tiÃªu: Dá»± Ä‘oÃ¡n cho X=25.8X = 25.8.

---

## 2. BÆ°á»›c khá»Ÿi táº¡o

- TrÆ°á»›c tiÃªn, XGBoost khá»Ÿi táº¡o xÃ¡c suáº¥t ban Ä‘áº§u cho táº¥t cáº£ máº«u báº±ng **táº§n suáº¥t lá»›p dÆ°Æ¡ng**.
- á» Ä‘Ã¢y cÃ³ 2/4 máº«u thuá»™c lá»›p 1 â†’ xÃ¡c suáº¥t khá»Ÿi táº¡o = 0.5.
- Log-odds khá»Ÿi táº¡o:

  logâ¡0.51âˆ’0.5=0\log\frac{0.5}{1-0.5} = 0

  â†’ First prediction = 0 (logit).

---

## 3. XÃ¢y cÃ¢y Ä‘áº§u tiÃªn (depth = 1)

- CÃ¢y chia táº¡i ngÆ°á»¡ng X<25X < 25.
- NÃºt trÃ¡i: Output = -2.0 (cho cÃ¡c Ä‘iá»ƒm 23, 24).
- NÃºt pháº£i: Output = 2.0 (cho cÃ¡c Ä‘iá»ƒm 26, 27).

CÃ¡c output nÃ y thá»±c cháº¥t Ä‘áº¿n tá»« viá»‡c tÃ­nh gradient vÃ  hessian cá»§a loss trÃªn residuals.

---

## 4. Dá»± Ä‘oÃ¡n cho X=25.8X = 25.8

- VÃ¬ 25.8>2525.8 > 25 â†’ Ä‘i nhÃ¡nh pháº£i â†’ giÃ¡ trá»‹ lÃ¡ = +2.0.
- Vá»›i learning rate Î· = 0.3, giÃ¡ trá»‹ cáº­p nháº­t = 0.3Ã—2.0=0.60.3 \times 2.0 = 0.6.
- Log prediction sau cáº­p nháº­t:

  LogPrediction=0+0.6=0.6\text{LogPrediction} = 0 + 0.6 = 0.6

---

## 5. Chuyá»ƒn logit â†’ xÃ¡c suáº¥t

- XÃ¡c suáº¥t dá»± Ä‘oÃ¡n:

  P=e0.61+e0.6â‰ˆ0.6457P = \frac{e^{0.6}}{1 + e^{0.6}} \approx 0.6457

---

## 6. Ã nghÄ©a

- Quy trÃ¬nh nÃ y minh hoáº¡ cÃ¡ch XGBoost:

  1. Báº¯t Ä‘áº§u báº±ng log-odds tá»« phÃ¢n bá»‘ lá»›p.
  2. Cáº­p nháº­t log-odds báº±ng cÃ¡c cÃ¢y nhá» (weighted by learning rate).
  3. Biáº¿n Ä‘á»•i log-odds thÃ nh xÃ¡c suáº¥t dá»± Ä‘oÃ¡n qua sigmoid.

---

ğŸ‘‰ Báº¡n cÃ³ muá»‘n mÃ¬nh viáº¿t láº¡i vÃ­ dá»¥ nÃ y dÆ°á»›i dáº¡ng **báº£ng tá»«ng bÆ°á»›c (khá»Ÿi táº¡o â†’ chá»n nhÃ¡nh â†’ cáº­p nháº­t logit â†’ xÃ¡c suáº¥t)** Ä‘á»ƒ há»c viÃªn dá»… theo dÃµi hÆ¡n khÃ´ng?
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

- [XGBoost Documentation â€” xgboost 0.4 documentation](https://xgboost-clone.readthedocs.io/en/latest/)
- 


![1757861228498](image/Gradient_Boost_2025-10-09/1757861228498.png)

ÄÃºng rá»“i ğŸ‘Œ, báº¡n hiá»ƒu chÃ­nh xÃ¡c:

Trong  **histogram** , **bin** lÃ  **khoáº£ng giÃ¡ trá»‹ (interval)** mÃ  ta dÃ¹ng Ä‘á»ƒ gom dá»¯ liá»‡u láº¡i trÆ°á»›c khi Ä‘áº¿m táº§n suáº¥t.

---

## 1. VÃ­ dá»¥

Giáº£ sá»­ cÃ³ Ä‘iá»ƒm sá»‘ tá»« 0 Ä‘áº¿n 10:

* Náº¿u ta chá»n bin width = 2 â†’ cÃ¡c bin sáº½ lÃ :

  [0,2),[2,4),[4,6),[6,8),[8,10][0,2), [2,4), [4,6), [6,8), [8,10]
* Má»™t há»c sinh cÃ³ Ä‘iá»ƒm 3.6 sáº½ rÆ¡i vÃ o bin  **[2,4)** .

---

## 2. Ã nghÄ©a

* **Bin nhá» (width háº¹p):** histogram chi tiáº¿t, dá»… tháº¥y biáº¿n Ä‘á»™ng, nhÆ°ng dá»… nhiá»…u.
* **Bin lá»›n (width rá»™ng):** histogram mÆ°á»£t hÆ¡n, dá»… tháº¥y xu hÆ°á»›ng, nhÆ°ng máº¥t chi tiáº¿t.

---

## 3. CÃ´ng thá»©c tá»•ng quÃ¡t

Náº¿u cÃ³ nn bin vÃ  dá»¯ liá»‡u náº±m trong khoáº£ng [xmin,xmax][x_{min}, x_{max}]:

* **Äá»™ rá»™ng bin (bin width):**

  w=xmaxâˆ’xminnw = \frac{x_{max} - x_{min}}{n}
* Má»—i bin lÃ  [xmin+kâ‹…w,â€…â€Šxmin+(k+1)â‹…w)[x_{min} + k \cdot w, \; x_{min} + (k+1) \cdot w) vá»›i k=0,1,â€¦,nâˆ’1k = 0,1,\ldots,n-1.

---

## 4. á»¨ng dá»¥ng

* Trá»±c quan hoÃ¡ dá»¯ liá»‡u (EDA).
* LÃ m sáº¡ch hoáº·c biáº¿n Ä‘á»•i feature (binning trÆ°á»›c khi train).
* Trong boosting (nhÆ° LightGBM), histogram-based learning dÃ¹ng â€œbinâ€ Ä‘á»ƒ tÄƒng tá»‘c tÃ¬m split.

---

ğŸ‘‰ Báº¡n cÃ³ muá»‘n mÃ¬nh viáº¿t thÃªm má»™t **vÃ­ dá»¥ báº±ng Python + matplotlib** Ä‘á»ƒ váº½ histogram vÃ  highlight cÃ¡c bin, Ä‘á»ƒ trá»±c quan hÆ¡n khÃ´ng?
